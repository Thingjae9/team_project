{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기\n",
    "1. SMOTE로 진행한 데이터가 비교적 높은 성능을 보였지만 f1_score, precision, 등 다소 아쉬운 모습을 보임.\n",
    "\n",
    "2. 진행은 Basic 데이터에 class weight만 부여하고 Standard Scaler를 통해 처리후 진행\n",
    " : normalize 진행시 수치가 감소하는 모습을 확인함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 데이터 (EDA)과정 진행 후 데이터\n",
    "X_train = pd.read_csv('csv/Binary_to_share/Binary_to_share/Basic/X_train.csv')\n",
    "X_test = pd.read_csv('csv/Binary_to_share/Binary_to_share/Basic/X_test.csv')\n",
    "X_val = pd.read_csv('csv/Binary_to_share/Binary_to_share/Basic/X_val.csv')\n",
    "y_train = pd.read_csv('csv/Binary_to_share/Binary_to_share/Basic/y_train.csv')\n",
    "y_test = pd.read_csv('csv/Binary_to_share/Binary_to_share/Basic/y_test.csv')\n",
    "y_val = pd.read_csv('csv/Binary_to_share/Binary_to_share/Basic/y_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 표준화\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train['target_class']\n",
    "y_test = y_test['target_class']\n",
    "y_val = y_val['target_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 1.0, 1: 10.428571428571429}\n"
     ]
    }
   ],
   "source": [
    "# 클래스 가중치 계산\n",
    "class_weight = {0: 1., 1: (len(y_train) / sum(y_train))}\n",
    "print(class_weight)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구현\n",
    "\n",
    "- 앞서 처리된 데이터를 모델로 돌려보면서 accuracy를 기준으로 early_stopping을 하면 val_accuracy 기준보다 오래걸리지만 정확도가 더 높아지는 모습을 보임.\n",
    " : 두개 다 확인해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 919us/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 95ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 998us/step\n",
      "85/85 - 0s - loss: 0.7246 - accuracy: 0.0960 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 956us/step\n",
      "85/85 - 0s - loss: 0.2471 - accuracy: 0.9581 - 114ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 856us/step\n",
      "85/85 - 0s - loss: 0.2306 - accuracy: 0.9743 - 94ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 761us/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 94ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.4778 - accuracy: 0.7397 - 118ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 964us/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 101ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 972us/step\n",
      "85/85 - 0s - loss: 0.2802 - accuracy: 0.9467 - 94ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 917us/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 94ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 845us/step\n",
      "85/85 - 0s - loss: 0.5636 - accuracy: 0.9632 - 94ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 917us/step\n",
      "85/85 - 0s - loss: 0.2013 - accuracy: 0.9721 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 908us/step\n",
      "85/85 - 0s - loss: 0.2059 - accuracy: 0.9588 - 88ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 856us/step\n",
      "85/85 - 0s - loss: 0.2146 - accuracy: 0.9658 - 104ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 906us/step\n",
      "85/85 - 0s - loss: 0.1776 - accuracy: 0.9610 - 94ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 898us/step\n",
      "85/85 - 0s - loss: 0.1758 - accuracy: 0.9654 - 95ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1788 - accuracy: 0.9647 - 94ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 946us/step\n",
      "85/85 - 0s - loss: 0.2009 - accuracy: 0.9632 - 91ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 952us/step\n",
      "85/85 - 0s - loss: 0.2325 - accuracy: 0.9603 - 82ms/epoch - 960us/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1744 - accuracy: 0.9695 - 96ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2130 - accuracy: 0.9629 - 94ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 94ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 976us/step\n",
      "85/85 - 0s - loss: 1.3314 - accuracy: 0.9129 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2539 - accuracy: 0.9397 - 105ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 929us/step\n",
      "85/85 - 0s - loss: 0.2832 - accuracy: 0.9368 - 116ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 94ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 988us/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 117ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 969us/step\n",
      "85/85 - 0s - loss: 0.4661 - accuracy: 0.8033 - 102ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 905us/step\n",
      "85/85 - 0s - loss: 0.6990 - accuracy: 0.9364 - 104ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 925us/step\n",
      "85/85 - 0s - loss: 0.2165 - accuracy: 0.9629 - 94ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2440 - accuracy: 0.9706 - 113ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 869us/step\n",
      "85/85 - 0s - loss: 0.3353 - accuracy: 0.9158 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 997us/step\n",
      "85/85 - 0s - loss: 0.1947 - accuracy: 0.9746 - 104ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2369 - accuracy: 0.9691 - 96ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.3119 - accuracy: 0.8559 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2961 - accuracy: 0.9004 - 106ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 903us/step\n",
      "85/85 - 0s - loss: 0.2477 - accuracy: 0.9647 - 105ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2134 - accuracy: 0.9504 - 97ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 929us/step\n",
      "85/85 - 0s - loss: 0.3570 - accuracy: 0.9026 - 100ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2171 - accuracy: 0.9607 - 102ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2581 - accuracy: 0.9390 - 104ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 952us/step\n",
      "85/85 - 0s - loss: 0.2829 - accuracy: 0.9246 - 91ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 962us/step\n",
      "85/85 - 0s - loss: 0.2262 - accuracy: 0.9695 - 94ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 988us/step\n",
      "85/85 - 0s - loss: 0.3396 - accuracy: 0.9121 - 104ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 979us/step\n",
      "85/85 - 0s - loss: 0.2401 - accuracy: 0.9393 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 861us/step\n",
      "85/85 - 0s - loss: 0.1650 - accuracy: 0.9710 - 97ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 976us/step\n",
      "85/85 - 0s - loss: 0.2017 - accuracy: 0.9511 - 110ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 961us/step\n",
      "85/85 - 0s - loss: 0.2657 - accuracy: 0.9158 - 104ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 928us/step\n",
      "85/85 - 0s - loss: 0.2455 - accuracy: 0.9085 - 98ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.3440 - accuracy: 0.7669 - 103ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1664 - accuracy: 0.9640 - 116ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1613 - accuracy: 0.9629 - 113ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1843 - accuracy: 0.9585 - 111ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2039 - accuracy: 0.9596 - 121ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1658 - accuracy: 0.9596 - 94ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 980us/step\n",
      "85/85 - 0s - loss: 0.1525 - accuracy: 0.9577 - 103ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1982 - accuracy: 0.9577 - 108ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 937us/step\n",
      "85/85 - 0s - loss: 0.1859 - accuracy: 0.9518 - 102ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2073 - accuracy: 0.9577 - 119ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2413 - accuracy: 0.9283 - 115ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1943 - accuracy: 0.9511 - 141ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1562 - accuracy: 0.9688 - 97ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1501 - accuracy: 0.9625 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1641 - accuracy: 0.9272 - 100ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1832 - accuracy: 0.9467 - 95ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1625 - accuracy: 0.9504 - 99ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2012 - accuracy: 0.9504 - 111ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2140 - accuracy: 0.9276 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1394 - accuracy: 0.9588 - 107ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1440 - accuracy: 0.9566 - 86ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1418 - accuracy: 0.9724 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1628 - accuracy: 0.9518 - 119ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1619 - accuracy: 0.9599 - 119ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1330 - accuracy: 0.9688 - 104ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1864 - accuracy: 0.9493 - 107ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1802 - accuracy: 0.9540 - 112ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.2043 - accuracy: 0.9603 - 179ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1661 - accuracy: 0.9585 - 101ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1952 - accuracy: 0.9548 - 110ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1577 - accuracy: 0.9680 - 119ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1863 - accuracy: 0.9610 - 110ms/epoch - 1ms/step\n",
      "Best hyperparameters: {'activation': 'elu', 'learning_rate': 0.003, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 256}\n",
      "Best validation accuracy: 0.9746323823928833\n",
      "Best time: 4.31622838973999\n",
      "Best time_hyperparams: {'activation': 'elu', 'learning_rate': 0.001, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 256}\n",
      "Best metric: 0.8656747232064835\n",
      "Best metric_hyperparams: {'activation': 'elu', 'learning_rate': 0.003, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 256}\n"
     ]
    }
   ],
   "source": [
    "# 복습한 내용으로 추가적인 진행 - adam 사용\n",
    "def custom_opt(n):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=n)\n",
    "    return opt\n",
    "\n",
    "\n",
    "# 변수 리스트 생성\n",
    "act_func = ['relu', 'elu', 'tanh', 'sigmoid']\n",
    "batch_lst =[32, 64, 128, 256]\n",
    "best_accuracy = 0.0\n",
    "best_hyperparams = {}\n",
    "lr_lst = [0.009, 0.006, 0.003, 0.001, 0.0005]\n",
    "best_time = 11111.0\n",
    "time_hyper = {}\n",
    "\n",
    "\n",
    "best_metric = 0.0\n",
    "metric_hyper = {}\n",
    "\n",
    "\n",
    "# dropout, 배치 정규화 추가\n",
    "# 모델 구현\n",
    "for func in act_func:\n",
    "    for i in lr_lst:\n",
    "        for batch in batch_lst:\n",
    "            metrics_lst = []\n",
    "\n",
    "            # model구현\n",
    "            model = Sequential()\n",
    "            model.add(Dense(256, activation=func, input_shape=(X_train.shape[1],)))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가\n",
    "            model.add(Dense(128, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가              \n",
    "            model.add(Dense(64, activation=func))\n",
    "            model.add(Dense(32, activation=func))\n",
    "            model.add(Dense(16, activation=func))\n",
    "            model.add(Dense(8, activation=func))\n",
    "            model.add(Dense(8, activation=func))\n",
    "            model.add(Dense(1, activation=func))\n",
    "\n",
    "            # 모델 컴파일\n",
    "            model.compile(optimizer=custom_opt(i), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Early stopping 기능 추가\n",
    "            early_stopping = EarlyStopping(patience=10, monitor='val_accuracy')\n",
    "            start_time = time.time()\n",
    "\n",
    "        \n",
    "            model.fit(X_train, y_train, epochs=1000, batch_size=batch, validation_data=(X_val, y_val), callbacks=[early_stopping], class_weight=class_weight, verbose = 0)\n",
    "            tmp = class_weight\n",
    "         \n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            long_time = end_time - start_time\n",
    "            if long_time < best_time:\n",
    "                best_time = long_time\n",
    "                time_hyper = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "\n",
    "            # 모델 평가\n",
    "            y_pred = model.predict(X_val)\n",
    "            y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "            metrics_lst.append(f1_score(y_val, y_pred_binary))\n",
    "            metrics_lst.append(precision_score(y_val, y_pred_binary))\n",
    "            metrics_lst.append(recall_score(y_val, y_pred_binary))\n",
    "            mean_met = np.mean(metrics_lst)\n",
    "\n",
    "            if mean_met > best_metric:\n",
    "                best_metric = mean_met\n",
    "                metric_hyper = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "            loss, acc = model.evaluate(X_val, y_val, verbose = 2)\n",
    "\n",
    "            if acc > best_accuracy:\n",
    "                best_accuracy = acc\n",
    "                best_hyperparams = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "print('Best hyperparameters:', best_hyperparams)\n",
    "print('Best validation accuracy:', best_accuracy)\n",
    "print('Best time:', best_time)\n",
    "print('Best time_hyperparams:', time_hyper)\n",
    "print('Best metric:', best_metric)\n",
    "print('Best metric_hyperparams:', metric_hyper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1359 - accuracy: 0.9647 - 138ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2235 - accuracy: 0.9371 - 124ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1331 - accuracy: 0.9695 - 142ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1378 - accuracy: 0.9625 - 125ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1409 - accuracy: 0.9629 - 127ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1619 - accuracy: 0.9496 - 176ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1334 - accuracy: 0.9610 - 154ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1419 - accuracy: 0.9636 - 150ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1347 - accuracy: 0.9621 - 155ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1370 - accuracy: 0.9596 - 173ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1880 - accuracy: 0.9312 - 138ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1436 - accuracy: 0.9691 - 140ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1643 - accuracy: 0.9592 - 137ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1725 - accuracy: 0.9551 - 152ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1598 - accuracy: 0.9526 - 150ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1265 - accuracy: 0.9625 - 148ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1595 - accuracy: 0.9629 - 141ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1277 - accuracy: 0.9654 - 141ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1508 - accuracy: 0.9577 - 165ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1477 - accuracy: 0.9599 - 170ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.2276 - accuracy: 0.9574 - 159ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1649 - accuracy: 0.9559 - 167ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1799 - accuracy: 0.9434 - 185ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1380 - accuracy: 0.9581 - 188ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1091 - accuracy: 0.9699 - 143ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1264 - accuracy: 0.9618 - 168ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1794 - accuracy: 0.9632 - 142ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1523 - accuracy: 0.9621 - 168ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1185 - accuracy: 0.9636 - 144ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1331 - accuracy: 0.9647 - 178ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1367 - accuracy: 0.9574 - 151ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1313 - accuracy: 0.9673 - 154ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1401 - accuracy: 0.9621 - 178ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1302 - accuracy: 0.9629 - 146ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1468 - accuracy: 0.9640 - 148ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1205 - accuracy: 0.9662 - 146ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1438 - accuracy: 0.9632 - 139ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1537 - accuracy: 0.9563 - 185ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1416 - accuracy: 0.9614 - 152ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1631 - accuracy: 0.9504 - 193ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1789 - accuracy: 0.9750 - 137ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1377 - accuracy: 0.9680 - 137ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1717 - accuracy: 0.9551 - 136ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1680 - accuracy: 0.9610 - 153ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2101 - accuracy: 0.9485 - 115ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1786 - accuracy: 0.9540 - 142ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1381 - accuracy: 0.9621 - 136ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1689 - accuracy: 0.9540 - 119ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1770 - accuracy: 0.9353 - 93ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1516 - accuracy: 0.9603 - 113ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1589 - accuracy: 0.9574 - 126ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1722 - accuracy: 0.9485 - 136ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1640 - accuracy: 0.9504 - 195ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1337 - accuracy: 0.9581 - 143ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1639 - accuracy: 0.9529 - 148ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1643 - accuracy: 0.9540 - 117ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1636 - accuracy: 0.9537 - 126ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1581 - accuracy: 0.9581 - 113ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1459 - accuracy: 0.9592 - 118ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1460 - accuracy: 0.9603 - 144ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2601 - accuracy: 0.8974 - 138ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1835 - accuracy: 0.9581 - 123ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1478 - accuracy: 0.9614 - 132ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1653 - accuracy: 0.9574 - 138ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1723 - accuracy: 0.9599 - 107ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1336 - accuracy: 0.9676 - 127ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1641 - accuracy: 0.9607 - 139ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1450 - accuracy: 0.9658 - 123ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1876 - accuracy: 0.9438 - 114ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1923 - accuracy: 0.9364 - 123ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1612 - accuracy: 0.9651 - 143ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1921 - accuracy: 0.9511 - 144ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1691 - accuracy: 0.9507 - 105ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1661 - accuracy: 0.9592 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1593 - accuracy: 0.9581 - 145ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1664 - accuracy: 0.9643 - 147ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1577 - accuracy: 0.9585 - 126ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1458 - accuracy: 0.9669 - 123ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1821 - accuracy: 0.9551 - 125ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1935 - accuracy: 0.9632 - 135ms/epoch - 2ms/step\n",
      "Best hyperparameters: {'activation': 'tanh', 'learning_rate': 0.009, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 32}\n",
      "Best validation accuracy: 0.9750000238418579\n",
      "Best time: 5.428201913833618\n",
      "Best time_hyperparams: {'activation': 'tanh', 'learning_rate': 0.003, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 256}\n",
      "Best metric: 0.865815916262915\n",
      "Best metric_hyperparams: {'activation': 'tanh', 'learning_rate': 0.009, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 32}\n"
     ]
    }
   ],
   "source": [
    "# 출력층 함수를 sigmoid로 진행하면 높아질지 확인\n",
    "# 복습한 내용으로 추가적인 진행 - adam 사용\n",
    "def custom_opt(n):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=n)\n",
    "    return opt\n",
    "\n",
    "\n",
    "# 변수 리스트 생성\n",
    "act_func = ['relu', 'elu', 'tanh', 'sigmoid']\n",
    "batch_lst =[32, 64, 128, 256]\n",
    "best_accuracy = 0.0\n",
    "best_hyperparams = {}\n",
    "lr_lst = [0.009, 0.006, 0.003, 0.001, 0.0005]\n",
    "best_time = 11111.0\n",
    "time_hyper = {}\n",
    "\n",
    "\n",
    "best_metric = 0.0\n",
    "metric_hyper = {}\n",
    "\n",
    "\n",
    "# dropout, 배치 정규화 추가\n",
    "# 모델 구현\n",
    "for func in act_func:\n",
    "    for i in lr_lst:\n",
    "        for batch in batch_lst:\n",
    "            metrics_lst = []\n",
    "\n",
    "            # model구현\n",
    "            model = Sequential()\n",
    "            model.add(Dense(256, activation=func, input_shape=(X_train.shape[1],)))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가\n",
    "            model.add(Dense(128, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가              \n",
    "            model.add(Dense(64, activation=func))\n",
    "            model.add(Dense(32, activation=func))\n",
    "            model.add(Dense(16, activation=func))\n",
    "            model.add(Dense(8, activation=func))\n",
    "            model.add(Dense(8, activation=func))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "            # 모델 컴파일\n",
    "            model.compile(optimizer=custom_opt(i), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Early stopping 기능 추가\n",
    "            early_stopping = EarlyStopping(patience=10, monitor='val_accuracy')\n",
    "            start_time = time.time()\n",
    "\n",
    "        \n",
    "            model.fit(X_train, y_train, epochs=1000, batch_size=batch, validation_data=(X_val, y_val), callbacks=[early_stopping], class_weight=class_weight, verbose = 0)\n",
    "            tmp = class_weight\n",
    "         \n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            long_time = end_time - start_time\n",
    "            if long_time < best_time:\n",
    "                best_time = long_time\n",
    "                time_hyper = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "\n",
    "            # 모델 평가\n",
    "            y_pred = model.predict(X_val)\n",
    "            y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "            metrics_lst.append(f1_score(y_val, y_pred_binary))\n",
    "            metrics_lst.append(precision_score(y_val, y_pred_binary))\n",
    "            metrics_lst.append(recall_score(y_val, y_pred_binary))\n",
    "            mean_met = np.mean(metrics_lst)\n",
    "\n",
    "            if mean_met > best_metric:\n",
    "                best_metric = mean_met\n",
    "                metric_hyper = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "            loss, acc = model.evaluate(X_val, y_val, verbose = 2)\n",
    "\n",
    "            if acc > best_accuracy:\n",
    "                best_accuracy = acc\n",
    "                best_hyperparams = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "print('Best hyperparameters:', best_hyperparams)\n",
    "print('Best validation accuracy:', best_accuracy)\n",
    "print('Best time:', best_time)\n",
    "print('Best time_hyperparams:', time_hyper)\n",
    "print('Best metric:', best_metric)\n",
    "print('Best metric_hyperparams:', metric_hyper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 104ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 115ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.3602 - accuracy: 0.9290 - 132ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2255 - accuracy: 0.9684 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 112ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 142ms/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1862 - accuracy: 0.9699 - 133ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 127ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2762 - accuracy: 0.9614 - 117ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 143ms/epoch - 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2229 - accuracy: 0.9728 - 107ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1721 - accuracy: 0.9710 - 117ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 120ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1185 - accuracy: 0.9732 - 115ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2378 - accuracy: 0.9596 - 125ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1850 - accuracy: 0.9632 - 126ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2103 - accuracy: 0.9533 - 105ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1809 - accuracy: 0.9651 - 120ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1795 - accuracy: 0.9654 - 129ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 126ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 111ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 5.7355 - accuracy: 0.6239 - 131ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.5015 - accuracy: 0.9673 - 128ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2694 - accuracy: 0.9614 - 126ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 117ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.4788 - accuracy: 0.9688 - 124ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 1.1366 - accuracy: 0.9250 - 133ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2269 - accuracy: 0.9482 - 122ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 1.4801 - accuracy: 0.9040 - 125ms/epoch - 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mj985\\anaconda3\\envs\\assign\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.4852 - accuracy: 0.9665 - 118ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2772 - accuracy: 0.9357 - 128ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2772 - accuracy: 0.9522 - 118ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2121 - accuracy: 0.9599 - 137ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1999 - accuracy: 0.9566 - 134ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2663 - accuracy: 0.8971 - 140ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.5102 - accuracy: 0.8224 - 132ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2087 - accuracy: 0.9349 - 119ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2394 - accuracy: 0.9577 - 131ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1953 - accuracy: 0.9482 - 114ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2496 - accuracy: 0.9456 - 139ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2593 - accuracy: 0.9673 - 130ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1848 - accuracy: 0.9721 - 129ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2778 - accuracy: 0.9537 - 141ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1790 - accuracy: 0.9691 - 129ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.6901 - accuracy: 0.7029 - 103ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2159 - accuracy: 0.9511 - 111ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1951 - accuracy: 0.9574 - 122ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1700 - accuracy: 0.9533 - 134ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1764 - accuracy: 0.9596 - 129ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.3587 - accuracy: 0.7761 - 118ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.3563 - accuracy: 0.7493 - 130ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2188 - accuracy: 0.9640 - 129ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2803 - accuracy: 0.9467 - 119ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2732 - accuracy: 0.9463 - 149ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2043 - accuracy: 0.9548 - 133ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1867 - accuracy: 0.9515 - 135ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1671 - accuracy: 0.9526 - 106ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1902 - accuracy: 0.9603 - 131ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1922 - accuracy: 0.9518 - 114ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2249 - accuracy: 0.9574 - 113ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1931 - accuracy: 0.9393 - 103ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1658 - accuracy: 0.9629 - 119ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1717 - accuracy: 0.9500 - 119ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1541 - accuracy: 0.9625 - 117ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.2075 - accuracy: 0.9456 - 108ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1690 - accuracy: 0.9335 - 110ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1633 - accuracy: 0.9452 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1534 - accuracy: 0.9651 - 114ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1358 - accuracy: 0.9669 - 111ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1268 - accuracy: 0.9691 - 124ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1748 - accuracy: 0.9493 - 151ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1578 - accuracy: 0.9647 - 99ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1606 - accuracy: 0.9574 - 97ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1778 - accuracy: 0.9518 - 112ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 955us/step\n",
      "85/85 - 0s - loss: 0.1598 - accuracy: 0.9592 - 120ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1600 - accuracy: 0.9581 - 134ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1609 - accuracy: 0.9566 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1641 - accuracy: 0.9603 - 109ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1636 - accuracy: 0.9599 - 120ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1912 - accuracy: 0.9621 - 171ms/epoch - 2ms/step\n",
      "Best hyperparameters: {'activation': 'relu', 'learning_rate': 0.001, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 64}\n",
      "Best validation accuracy: 0.9731617569923401\n",
      "Best time: 5.303998231887817\n",
      "Best time_hyperparams: {'activation': 'relu', 'learning_rate': 0.0005, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 256}\n",
      "Best metric: 0.8582375478927203\n",
      "Best metric_hyperparams: {'activation': 'relu', 'learning_rate': 0.003, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 128}\n"
     ]
    }
   ],
   "source": [
    "# 복습한 내용으로 추가적인 진행 - adam 사용 -accuracy\n",
    "def custom_opt(n):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=n)\n",
    "    return opt\n",
    "\n",
    "\n",
    "# 변수 리스트 생성\n",
    "act_func = ['relu', 'elu', 'tanh', 'sigmoid']\n",
    "batch_lst =[32, 64, 128, 256]\n",
    "best_accuracy = 0.0\n",
    "best_hyperparams = {}\n",
    "lr_lst = [0.009, 0.006, 0.003, 0.001, 0.0005]\n",
    "best_time = 11111.0\n",
    "time_hyper = {}\n",
    "\n",
    "\n",
    "best_metric = 0.0\n",
    "metric_hyper = {}\n",
    "\n",
    "\n",
    "# dropout, 배치 정규화 추가\n",
    "# 모델 구현\n",
    "for func in act_func:\n",
    "    for i in lr_lst:\n",
    "        for batch in batch_lst:\n",
    "            metrics_lst = []\n",
    "\n",
    "            # model구현\n",
    "            model = Sequential()\n",
    "            model.add(Dense(256, activation=func, input_shape=(X_train.shape[1],)))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가\n",
    "            model.add(Dense(128, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가              \n",
    "            model.add(Dense(64, activation=func))\n",
    "            model.add(Dense(32, activation=func))\n",
    "            model.add(Dense(16, activation=func))\n",
    "            model.add(Dense(8, activation=func))\n",
    "            model.add(Dense(8, activation=func))\n",
    "            model.add(Dense(1, activation=func))\n",
    "\n",
    "            # 모델 컴파일\n",
    "            model.compile(optimizer=custom_opt(i), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Early stopping 기능 추가 - accuracy 기준으로 바꿔서 확인\n",
    "            early_stopping = EarlyStopping(patience=10, monitor='accuracy')\n",
    "            start_time = time.time()\n",
    "\n",
    "        \n",
    "            model.fit(X_train, y_train, epochs=1000, batch_size=batch, validation_data=(X_val, y_val), callbacks=[early_stopping], class_weight=class_weight, verbose = 0)\n",
    "            tmp = class_weight\n",
    "         \n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            long_time = end_time - start_time\n",
    "            if long_time < best_time:\n",
    "                best_time = long_time\n",
    "                time_hyper = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "\n",
    "            # 모델 평가\n",
    "            y_pred = model.predict(X_val)\n",
    "            y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "            metrics_lst.append(f1_score(y_val, y_pred_binary))\n",
    "            metrics_lst.append(precision_score(y_val, y_pred_binary))\n",
    "            metrics_lst.append(recall_score(y_val, y_pred_binary))\n",
    "            mean_met = np.mean(metrics_lst)\n",
    "\n",
    "            if mean_met > best_metric:\n",
    "                best_metric = mean_met\n",
    "                metric_hyper = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "            loss, acc = model.evaluate(X_val, y_val, verbose = 2)\n",
    "\n",
    "            if acc > best_accuracy:\n",
    "                best_accuracy = acc\n",
    "                best_hyperparams = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "print('Best hyperparameters:', best_hyperparams)\n",
    "print('Best validation accuracy:', best_accuracy)\n",
    "print('Best time:', best_time)\n",
    "print('Best time_hyperparams:', time_hyper)\n",
    "print('Best metric:', best_metric)\n",
    "print('Best metric_hyperparams:', metric_hyper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1684 - accuracy: 0.9551 - 137ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.2027 - accuracy: 0.9735 - 154ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1246 - accuracy: 0.9713 - 138ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1229 - accuracy: 0.9640 - 127ms/epoch - 1ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1502 - accuracy: 0.9563 - 136ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "85/85 - 0s - loss: 0.1206 - accuracy: 0.9691 - 135ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 1s 2ms/step\n",
      "85/85 - 0s - loss: 0.1483 - accuracy: 0.9640 - 152ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1680 - accuracy: 0.9504 - 155ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1753 - accuracy: 0.9419 - 150ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1384 - accuracy: 0.9684 - 136ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1258 - accuracy: 0.9684 - 148ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1373 - accuracy: 0.9632 - 139ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.2338 - accuracy: 0.9368 - 146ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1403 - accuracy: 0.9629 - 148ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1691 - accuracy: 0.9574 - 157ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1797 - accuracy: 0.9592 - 148ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1619 - accuracy: 0.9511 - 154ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1711 - accuracy: 0.9511 - 161ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1350 - accuracy: 0.9618 - 150ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1194 - accuracy: 0.9717 - 155ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1386 - accuracy: 0.9625 - 162ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1358 - accuracy: 0.9629 - 166ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1256 - accuracy: 0.9654 - 158ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1432 - accuracy: 0.9570 - 172ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.2418 - accuracy: 0.9621 - 150ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1422 - accuracy: 0.9658 - 143ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1587 - accuracy: 0.9614 - 139ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.2732 - accuracy: 0.9250 - 159ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1969 - accuracy: 0.9588 - 152ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1686 - accuracy: 0.9621 - 155ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1677 - accuracy: 0.9629 - 139ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1597 - accuracy: 0.9673 - 146ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1653 - accuracy: 0.9577 - 145ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1392 - accuracy: 0.9632 - 132ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1365 - accuracy: 0.9647 - 161ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1386 - accuracy: 0.9585 - 160ms/epoch - 2ms/step\n",
      "Best hyperparameters: {'activation': 'relu', 'learning_rate': 0.009, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 64}\n",
      "Best validation accuracy: 0.9735293984413147\n",
      "Best time: 8.387002944946289\n",
      "Best time_hyperparams: {'activation': 'tanh', 'learning_rate': 0.003, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 256}\n",
      "Best metric: 0.8626037974475675\n",
      "Best metric_hyperparams: {'activation': 'relu', 'learning_rate': 0.009, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 64}\n"
     ]
    }
   ],
   "source": [
    "# 복습한 내용으로 추가적인 진행 - adam 사용\n",
    "\n",
    "def custom_opt(n):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=n)\n",
    "    return opt\n",
    "\n",
    "\n",
    "# 변수 리스트 생성\n",
    "act_func = ['relu', 'elu', 'tanh']\n",
    "batch_lst =[32, 64, 256]\n",
    "best_accuracy = 0.0\n",
    "best_hyperparams = {}\n",
    "lr_lst = [0.009, 0.006, 0.003, 0.001]\n",
    "best_time = 11111.0\n",
    "time_hyper = {}\n",
    "\n",
    "\n",
    "best_metric = 0.0\n",
    "metric_hyper = {}\n",
    "\n",
    "\n",
    "# dropout, 배치 정규화 추가\n",
    "# 모델 구현 - 다이아몬드 형\n",
    "for func in act_func:\n",
    "    for i in lr_lst:\n",
    "        for batch in batch_lst:\n",
    "            metrics_lst = []\n",
    "\n",
    "            # model구현\n",
    "            model = Sequential()\n",
    "            model.add(Dense(256, activation=func, input_shape=(X_train.shape[1],)))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가\n",
    "            model.add(Dense(512, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가              \n",
    "            model.add(Dense(128, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가 \n",
    "            model.add(Dense(32, activation=func))\n",
    "            model.add(Dense(16, activation=func))\n",
    "            model.add(Dense(8, activation=func))\n",
    "            model.add(Dense(8, activation=func))\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "            # 모델 컴파일\n",
    "            model.compile(optimizer=custom_opt(i), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Early stopping 기능 추가 \n",
    "            early_stopping = EarlyStopping(patience=10, monitor='val_accuracy')\n",
    "            start_time = time.time()\n",
    "\n",
    "        \n",
    "            model.fit(X_train, y_train, epochs=1000, batch_size=batch, validation_data=(X_val, y_val), callbacks=[early_stopping], class_weight=class_weight, verbose = 0)\n",
    "            tmp = class_weight\n",
    "         \n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            long_time = end_time - start_time\n",
    "            if long_time < best_time:\n",
    "                best_time = long_time\n",
    "                time_hyper = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "\n",
    "            # 모델 평가\n",
    "            y_pred = model.predict(X_val)\n",
    "            y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "            metrics_lst.append(f1_score(y_val, y_pred_binary))\n",
    "            metrics_lst.append(precision_score(y_val, y_pred_binary))\n",
    "            metrics_lst.append(recall_score(y_val, y_pred_binary))\n",
    "            mean_met = np.mean(metrics_lst)\n",
    "\n",
    "            if mean_met > best_metric:\n",
    "                best_metric = mean_met\n",
    "                metric_hyper = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "            loss, acc = model.evaluate(X_val, y_val, verbose = 2)\n",
    "\n",
    "            if acc > best_accuracy:\n",
    "                best_accuracy = acc\n",
    "                best_hyperparams = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "print('Best hyperparameters:', best_hyperparams)\n",
    "print('Best validation accuracy:', best_accuracy)\n",
    "print('Best time:', best_time)\n",
    "print('Best time_hyperparams:', time_hyper)\n",
    "print('Best metric:', best_metric)\n",
    "print('Best metric_hyperparams:', metric_hyper)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.2221 - accuracy: 0.9268 - 184ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1201 - accuracy: 0.9706 - 159ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1080 - accuracy: 0.9739 - 142ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1463 - accuracy: 0.9724 - 167ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1282 - accuracy: 0.9603 - 165ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1267 - accuracy: 0.9632 - 142ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1527 - accuracy: 0.9691 - 132ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1354 - accuracy: 0.9647 - 195ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1425 - accuracy: 0.9643 - 179ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1860 - accuracy: 0.9592 - 155ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1532 - accuracy: 0.9529 - 151ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1609 - accuracy: 0.9585 - 135ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1630 - accuracy: 0.9710 - 195ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1379 - accuracy: 0.9654 - 191ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1490 - accuracy: 0.9559 - 178ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1644 - accuracy: 0.9566 - 152ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1214 - accuracy: 0.9673 - 157ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1283 - accuracy: 0.9658 - 209ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1660 - accuracy: 0.9555 - 187ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1592 - accuracy: 0.9599 - 182ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1405 - accuracy: 0.9665 - 170ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1366 - accuracy: 0.9658 - 199ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 3ms/step\n",
      "85/85 - 0s - loss: 0.1540 - accuracy: 0.9592 - 253ms/epoch - 3ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1521 - accuracy: 0.9570 - 181ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.2252 - accuracy: 0.9599 - 157ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.2510 - accuracy: 0.9404 - 142ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.4795 - accuracy: 0.8471 - 205ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1619 - accuracy: 0.9702 - 199ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.2312 - accuracy: 0.9518 - 182ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1856 - accuracy: 0.9382 - 186ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1585 - accuracy: 0.9702 - 144ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1556 - accuracy: 0.9673 - 200ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1462 - accuracy: 0.9629 - 183ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.2087 - accuracy: 0.9537 - 162ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1780 - accuracy: 0.9518 - 153ms/epoch - 2ms/step\n",
      "85/85 [==============================] - 0s 2ms/step\n",
      "85/85 - 0s - loss: 0.1661 - accuracy: 0.9577 - 148ms/epoch - 2ms/step\n",
      "Best hyperparameters: {'activation': 'relu', 'learning_rate': 0.009, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 256}\n",
      "Best validation accuracy: 0.9738970398902893\n",
      "Best time: 15.384447813034058\n",
      "Best time_hyperparams: {'activation': 'elu', 'learning_rate': 0.003, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 256}\n",
      "Best metric: 0.8653270726162173\n",
      "Best metric_hyperparams: {'activation': 'relu', 'learning_rate': 0.009, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 256}\n"
     ]
    }
   ],
   "source": [
    "# 복습한 내용으로 추가적인 진행 - adam 사용\n",
    "def custom_opt(n):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=n)\n",
    "    return opt\n",
    "\n",
    "\n",
    "# 변수 리스트 생성\n",
    "act_func = ['relu', 'elu', 'tanh']\n",
    "batch_lst =[32, 64, 256]\n",
    "best_accuracy = 0.0\n",
    "best_hyperparams = {}\n",
    "lr_lst = [0.009, 0.006, 0.003, 0.001]\n",
    "best_time = 11111.0\n",
    "time_hyper = {}\n",
    "\n",
    "\n",
    "best_metric = 0.0\n",
    "metric_hyper = {}\n",
    "\n",
    "\n",
    "# dropout, 배치 정규화 추가\n",
    "# 모델 구현 - 다이아몬드 형\n",
    "for func in act_func:\n",
    "    for i in lr_lst:\n",
    "        for batch in batch_lst:\n",
    "            metrics_lst = []\n",
    "\n",
    "            # model구현\n",
    "            model = Sequential()\n",
    "            model.add(Dense(512, activation=func, input_shape=(X_train.shape[1],)))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가\n",
    "            model.add(Dense(256, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가              \n",
    "            model.add(Dense(128, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가 \n",
    "            model.add(Dense(32, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가 \n",
    "            model.add(Dense(16, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가\n",
    "            model.add(Dense(8, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가\n",
    "            model.add(Dense(8, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가\n",
    "            model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "            # 모델 컴파일\n",
    "            model.compile(optimizer=custom_opt(i), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            # Early stopping 기능 추가 - accuracy 기준으로 바꿔서 확인\n",
    "            early_stopping = EarlyStopping(patience=10, monitor='accuracy')\n",
    "            start_time = time.time()\n",
    "\n",
    "        \n",
    "            model.fit(X_train, y_train, epochs=1000, batch_size=batch, validation_data=(X_val, y_val), callbacks=[early_stopping], class_weight=class_weight, verbose = 0)\n",
    "            tmp = class_weight\n",
    "         \n",
    "\n",
    "\n",
    "            end_time = time.time()\n",
    "            long_time = end_time - start_time\n",
    "            if long_time < best_time:\n",
    "                best_time = long_time\n",
    "                time_hyper = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "\n",
    "            # 모델 평가\n",
    "            y_pred = model.predict(X_val)\n",
    "            y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "            metrics_lst.append(f1_score(y_val, y_pred_binary))\n",
    "            metrics_lst.append(precision_score(y_val, y_pred_binary))\n",
    "            metrics_lst.append(recall_score(y_val, y_pred_binary))\n",
    "            mean_met = np.mean(metrics_lst)\n",
    "\n",
    "            if mean_met > best_metric:\n",
    "                best_metric = mean_met\n",
    "                metric_hyper = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "            loss, acc = model.evaluate(X_val, y_val, verbose = 2)\n",
    "\n",
    "            if acc > best_accuracy:\n",
    "                best_accuracy = acc\n",
    "                best_hyperparams = {'activation': func, 'learning_rate': i, 'X_train': tmp, 'batch': batch}\n",
    "\n",
    "print('Best hyperparameters:', best_hyperparams)\n",
    "print('Best validation accuracy:', best_accuracy)\n",
    "print('Best time:', best_time)\n",
    "print('Best time_hyperparams:', time_hyper)\n",
    "print('Best metric:', best_metric)\n",
    "print('Best metric_hyperparams:', metric_hyper)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 모델\n",
    "\n",
    "- Best hyperparameters: {'activation': 'tanh', 'learning_rate': 0.009, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 32}\n",
    "- Best validation accuracy: 0.9750000238418579\n",
    "- Best time: 5.428201913833618\n",
    "- Best time_hyperparams: {'activation': 'tanh', 'learning_rate': 0.003, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 256}\n",
    "- Best metric: 0.865815916262915\n",
    "- Best metric_hyperparams: {'activation': 'tanh', 'learning_rate': 0.009, 'X_train': {0: 1.0, 1: 10.428571428571429}, 'batch': 32}\n",
    "\n",
    "----------------------------------\n",
    "\n",
    "최종적으로 평가지표 수치들이 감소한 것으로 확인\n",
    "SMOTE 진행한 데이터가 더 신뢰도가 높음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "340/340 [==============================] - 0s 1ms/step\n",
      "train_loss, train_accuracy\n",
      "340/340 - 1s - loss: 0.1790 - accuracy: 0.9496 - 512ms/epoch - 2ms/step\n",
      "val_loss, val_accuracy\n",
      "85/85 - 0s - loss: 0.1920 - accuracy: 0.9393 - 107ms/epoch - 1ms/step\n",
      "걸린시간 : 20.106853723526\n",
      "=== train set ===\n",
      "f1_score: 0.7802726543704891\n",
      "precision_score: 0.6705720192970366\n",
      "recall_score 0.9328859060402684\n",
      "85/85 [==============================] - 0s 1ms/step\n",
      "=== val set ===\n",
      "f1_score: 0.7393364928909952\n",
      "precision_score: 0.6290322580645161\n",
      "recall_score 0.896551724137931\n"
     ]
    }
   ],
   "source": [
    "func = 'tanh'\n",
    "i = 0.009\n",
    "batch = 32\n",
    "\n",
    "# model구현\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation=func, input_shape=(X_train.shape[1],)))\n",
    "model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "model.add(Dropout(0.2))  # Dropout 추가\n",
    "model.add(Dense(128, activation=func))\n",
    "model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "model.add(Dropout(0.2))  # Dropout 추가              \n",
    "model.add(Dense(64, activation=func))\n",
    "model.add(Dense(32, activation=func))\n",
    "model.add(Dense(16, activation=func))\n",
    "model.add(Dense(8, activation=func))\n",
    "model.add(Dense(8, activation=func))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=custom_opt(i), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping 기능 추가\n",
    "early_stopping = EarlyStopping(patience=10, monitor='val_accuracy')\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=batch, validation_data=(X_val, y_val), callbacks=[early_stopping], class_weight=class_weight, verbose = 0)\n",
    "tmp = class_weight\n",
    "\n",
    "end_time = time.time()\n",
    "long_time = end_time - start_time\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model.predict(X_train)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "metrics_lst.append(f1_score(y_train, y_pred_binary))\n",
    "metrics_lst.append(precision_score(y_train, y_pred_binary))\n",
    "metrics_lst.append(recall_score(y_train, y_pred_binary))\n",
    "mean_met = np.mean(metrics_lst)\n",
    "\n",
    "print(\"train_loss, train_accuracy\")\n",
    "train_loss, train_cc = model.evaluate(X_train, y_train, verbose = 2)\n",
    "\n",
    "print(\"val_loss, val_accuracy\")\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, verbose = 2)\n",
    "\n",
    "print(\"걸린시간 :\", long_time)\n",
    "\n",
    "print(\"=== train set ===\")\n",
    "print('f1_score:', f1_score(y_train, y_pred_binary))\n",
    "print('precision_score:', precision_score(y_train, y_pred_binary))\n",
    "print('recall_score', recall_score(y_train, y_pred_binary))\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "print(\"=== val set ===\")\n",
    "print('f1_score:', f1_score(y_val, y_pred_binary))\n",
    "print('precision_score:', precision_score(y_val, y_pred_binary))\n",
    "print('recall_score', recall_score(y_val, y_pred_binary))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 모델 (data set 수정)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 데이터 (EDA)과정 진행 후 데이터\n",
    "X_train_smote = pd.read_csv('csv/Binary_to_share/Binary_to_share/Minmax+Standard/X_train_MS_smote.csv')\n",
    "X_train_adasyn = pd.read_csv('csv/Binary_to_share/Binary_to_share/Minmax+Standard/X_train_MS_adasyn.csv')\n",
    "X_test = pd.read_csv('csv/Binary_to_share/Binary_to_share/Minmax+Standard/X_test_MS.csv')\n",
    "X_val = pd.read_csv('csv/Binary_to_share/Binary_to_share/Minmax+Standard/X_val_MS.csv')\n",
    "y_train_smote = pd.read_csv('csv/Binary_to_share/Binary_to_share/Minmax+Standard/y_train_MS_smote.csv')\n",
    "y_train_adasyn = pd.read_csv('csv/Binary_to_share/Binary_to_share/Minmax+Standard/y_train_MS_adasyn.csv')\n",
    "y_test = pd.read_csv('csv/Binary_to_share/Binary_to_share/Minmax+Standard/y_test.csv')\n",
    "y_val = pd.read_csv('csv/Binary_to_share/Binary_to_share/Minmax+Standard/y_val.csv')\n",
    "\n",
    "y_train_smote = y_train_smote['target_class']\n",
    "y_test = y_test['target_class']\n",
    "y_val = y_val['target_class']\n",
    "# 클래스 가중치 계산\n",
    "class_weight1 = {0: 1., 1: (len(y_train_smote) / sum(y_train_smote))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615/615 [==============================] - 1s 1ms/step\n",
      "train_loss, train_accuracy\n",
      "615/615 - 1s - loss: 0.1074 - accuracy: 0.9596 - 638ms/epoch - 1ms/step\n",
      "val_loss, val_accuracy\n",
      "85/85 - 0s - loss: 0.1588 - accuracy: 0.9482 - 113ms/epoch - 1ms/step\n",
      "걸린시간 : 17.10420846939087\n",
      "=== train set ===\n",
      "f1_score: 0.9598218803764802\n",
      "precision_score: 0.9552780016116036\n",
      "recall_score 0.9644091925971121\n",
      "107/107 [==============================] - 0s 1ms/step\n",
      "=== test set ===\n",
      "f1_score: 0.8058510638297873\n",
      "precision_score: 0.7112676056338029\n",
      "recall_score 0.9294478527607362\n"
     ]
    }
   ],
   "source": [
    "# Best hyperparameters: {'activation': 'relu', 'learning_rate': 0.003, 'X_train': {0: 1.0, 1: 2.0}, 'batch': 64}\n",
    "# Best validation accuracy: 0.9753676652908325\n",
    "# Best time: 8.609997987747192\n",
    "# Best time_hyperparams: {'activation': 'elu', 'learning_rate': 0.009, 'X_train': {0: 1.0, 1: 2.0}, 'batch': 256}\n",
    "# Best metric: 0.8658478183402076\n",
    "# Best metric_hyperparams: {'activation': 'relu', 'learning_rate': 0.003, 'X_train': {0: 1.0, 1: 2.0}, 'batch': 64}\n",
    "\n",
    "# 복습한 내용으로 추가적인 진행 - adam 사용, smote\n",
    "def custom_opt(n):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=n)\n",
    "    return opt\n",
    "\n",
    "# 변수 리스트 생성\n",
    "func = 'relu'\n",
    "batch = 64\n",
    "i = 0.003\n",
    "\n",
    "# model 구현\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation=func, input_shape=(X_train_smote.shape[1],)))\n",
    "model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "model.add(Dropout(0.2))  # Dropout 추가\n",
    "model.add(Dense(128, activation=func))\n",
    "model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "model.add(Dropout(0.2))  # Dropout 추가              \n",
    "model.add(Dense(64, activation=func))\n",
    "model.add(Dense(32, activation=func))\n",
    "model.add(Dense(16, activation=func))\n",
    "model.add(Dense(8, activation=func))\n",
    "model.add(Dense(8, activation=func))\n",
    "model.add(Dense(1, activation='sigmoid')) # 출력층을 sigmoid , 또는 relu사용 가능\n",
    "\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=custom_opt(i), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping 기능 추가\n",
    "early_stopping = EarlyStopping(patience=10, monitor='val_accuracy')\n",
    "start_time = time.time()\n",
    "\n",
    "# 모델 적용\n",
    "model.fit(X_train_smote, y_train_smote, epochs=1000, batch_size=batch, validation_data=(X_val, y_val), callbacks=[early_stopping], class_weight=class_weight1, verbose = 0)\n",
    "\n",
    "end_time = time.time()\n",
    "long_time = end_time - start_time\n",
    "\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model.predict(X_train_smote)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "metrics_lst.append(f1_score(y_train_smote, y_pred_binary))\n",
    "metrics_lst.append(precision_score(y_train_smote, y_pred_binary))\n",
    "metrics_lst.append(recall_score(y_train_smote, y_pred_binary))\n",
    "mean_met = np.mean(metrics_lst)\n",
    "\n",
    "print(\"train_loss, train_accuracy\")\n",
    "train_loss, train_cc = model.evaluate(X_train_smote, y_train_smote, verbose = 2)\n",
    "\n",
    "print(\"val_loss, val_accuracy\")\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, verbose = 2)\n",
    "\n",
    "print(\"걸린시간 :\", long_time)\n",
    "\n",
    "print(\"=== train set ===\")\n",
    "print('f1_score:', f1_score(y_train_smote, y_pred_binary))\n",
    "print('precision_score:', precision_score(y_train_smote, y_pred_binary))\n",
    "print('recall_score', recall_score(y_train_smote, y_pred_binary))\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "print(\"=== test set ===\")\n",
    "print('f1_score:', f1_score(y_test, y_pred_binary))\n",
    "print('precision_score:', precision_score(y_test, y_pred_binary))\n",
    "print('recall_score', recall_score(y_test, y_pred_binary))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
