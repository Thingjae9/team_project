{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from itertools import combinations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer, MinMaxScaler,StandardScaler\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau\n",
    "from sklearn.model_selection import KFold, GroupKFold, StratifiedKFold\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "from tensorflow_addons.optimizers import AdamW\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('csv/Regression_data_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_add_features(df):\n",
    "    \n",
    "    df['water'] = df['Whole weight'] - (df['Shucked weight'] + df['Viscera weight'] + df['Shell weight'])\n",
    "    df['ratio'] = df['Shucked weight'] / df['Whole weight']\n",
    "    \n",
    "    #whole weight의 경우 상당수가 다른 변수들의 값과 많이 겹치므로 categorize 실시\n",
    "    df['Whole weight'] = df['Whole weight'].map(lambda x: 1 if x <= 0.5 else (2 if x <= 1 else (3 if x<=2.5 else 4)))\n",
    "\n",
    "    #임시변수\n",
    "    df['new1'] = df['Length'] + df['Diameter'] + df['Height']\n",
    "    df['new2'] = df['Length'] * df['Diameter'] * df['Height']\n",
    "    df['new3'] = df['Shell weight']+df['Height']\n",
    "    df['new4'] = df['Viscera weight'] + df['Shucked weight']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Rings</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_I</th>\n",
       "      <th>Sex_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.350</td>\n",
       "      <td>0.265</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.2255</td>\n",
       "      <td>0.0995</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>0.0700</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.530</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.6770</td>\n",
       "      <td>0.2565</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.440</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.1140</td>\n",
       "      <td>0.1550</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.330</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.0395</td>\n",
       "      <td>0.0550</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4172</th>\n",
       "      <td>0.565</td>\n",
       "      <td>0.450</td>\n",
       "      <td>0.165</td>\n",
       "      <td>0.8870</td>\n",
       "      <td>0.3700</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.2490</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4173</th>\n",
       "      <td>0.590</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.9660</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.2145</td>\n",
       "      <td>0.2605</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4174</th>\n",
       "      <td>0.600</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.205</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>0.5255</td>\n",
       "      <td>0.2875</td>\n",
       "      <td>0.3080</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>0.625</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.150</td>\n",
       "      <td>1.0945</td>\n",
       "      <td>0.5310</td>\n",
       "      <td>0.2610</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4176</th>\n",
       "      <td>0.710</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.195</td>\n",
       "      <td>1.9485</td>\n",
       "      <td>0.9455</td>\n",
       "      <td>0.3765</td>\n",
       "      <td>0.4950</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4177 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Length  Diameter  Height  Whole weight  Shucked weight  Viscera weight   \n",
       "0      0.455     0.365   0.095        0.5140          0.2245          0.1010  \\\n",
       "1      0.350     0.265   0.090        0.2255          0.0995          0.0485   \n",
       "2      0.530     0.420   0.135        0.6770          0.2565          0.1415   \n",
       "3      0.440     0.365   0.125        0.5160          0.2155          0.1140   \n",
       "4      0.330     0.255   0.080        0.2050          0.0895          0.0395   \n",
       "...      ...       ...     ...           ...             ...             ...   \n",
       "4172   0.565     0.450   0.165        0.8870          0.3700          0.2390   \n",
       "4173   0.590     0.440   0.135        0.9660          0.4390          0.2145   \n",
       "4174   0.600     0.475   0.205        1.1760          0.5255          0.2875   \n",
       "4175   0.625     0.485   0.150        1.0945          0.5310          0.2610   \n",
       "4176   0.710     0.555   0.195        1.9485          0.9455          0.3765   \n",
       "\n",
       "      Shell weight  Rings  Sex_F  Sex_I  Sex_M  \n",
       "0           0.1500     15      0      0      1  \n",
       "1           0.0700      7      0      0      1  \n",
       "2           0.2100      9      1      0      0  \n",
       "3           0.1550     10      0      0      1  \n",
       "4           0.0550      7      0      1      0  \n",
       "...            ...    ...    ...    ...    ...  \n",
       "4172        0.2490     11      1      0      0  \n",
       "4173        0.2605     10      0      0      1  \n",
       "4174        0.3080      9      0      0      1  \n",
       "4175        0.2960     10      1      0      0  \n",
       "4176        0.4950     12      0      0      1  \n",
       "\n",
       "[4177 rows x 11 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocessing_add_features(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'Rings' : 'Target'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('Target').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_I</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>water</th>\n",
       "      <th>ratio</th>\n",
       "      <th>new1</th>\n",
       "      <th>new2</th>\n",
       "      <th>new3</th>\n",
       "      <th>new4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.055000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.997000</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.140000</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.011500</td>\n",
       "      <td>0.001500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.986500</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>0.000375</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>0.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.128667</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.011767</td>\n",
       "      <td>0.006267</td>\n",
       "      <td>0.008933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.973033</td>\n",
       "      <td>0.011767</td>\n",
       "      <td>0.346333</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>0.050600</td>\n",
       "      <td>0.018033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.221491</td>\n",
       "      <td>0.161579</td>\n",
       "      <td>0.053947</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.024719</td>\n",
       "      <td>0.012956</td>\n",
       "      <td>0.018000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.944325</td>\n",
       "      <td>0.024719</td>\n",
       "      <td>0.437018</td>\n",
       "      <td>0.002239</td>\n",
       "      <td>0.071947</td>\n",
       "      <td>0.037675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.285739</td>\n",
       "      <td>0.210696</td>\n",
       "      <td>0.069913</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.027330</td>\n",
       "      <td>0.036770</td>\n",
       "      <td>0.034783</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.095652</td>\n",
       "      <td>0.874204</td>\n",
       "      <td>0.061696</td>\n",
       "      <td>0.566348</td>\n",
       "      <td>0.004771</td>\n",
       "      <td>0.106683</td>\n",
       "      <td>0.089026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.369363</td>\n",
       "      <td>0.278861</td>\n",
       "      <td>0.092065</td>\n",
       "      <td>2.084942</td>\n",
       "      <td>0.123158</td>\n",
       "      <td>0.058371</td>\n",
       "      <td>0.078388</td>\n",
       "      <td>0.061776</td>\n",
       "      <td>0.833977</td>\n",
       "      <td>0.104247</td>\n",
       "      <td>0.825025</td>\n",
       "      <td>0.109508</td>\n",
       "      <td>0.740289</td>\n",
       "      <td>0.010725</td>\n",
       "      <td>0.170453</td>\n",
       "      <td>0.181529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.422033</td>\n",
       "      <td>0.321535</td>\n",
       "      <td>0.105921</td>\n",
       "      <td>2.276215</td>\n",
       "      <td>0.182657</td>\n",
       "      <td>0.085899</td>\n",
       "      <td>0.111648</td>\n",
       "      <td>0.112532</td>\n",
       "      <td>0.682864</td>\n",
       "      <td>0.204604</td>\n",
       "      <td>0.896010</td>\n",
       "      <td>0.138183</td>\n",
       "      <td>0.849488</td>\n",
       "      <td>0.015846</td>\n",
       "      <td>0.217569</td>\n",
       "      <td>0.268556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.498776</td>\n",
       "      <td>0.384798</td>\n",
       "      <td>0.127007</td>\n",
       "      <td>2.765845</td>\n",
       "      <td>0.293773</td>\n",
       "      <td>0.138502</td>\n",
       "      <td>0.178609</td>\n",
       "      <td>0.214789</td>\n",
       "      <td>0.482394</td>\n",
       "      <td>0.302817</td>\n",
       "      <td>1.154961</td>\n",
       "      <td>0.162672</td>\n",
       "      <td>1.010581</td>\n",
       "      <td>0.026233</td>\n",
       "      <td>0.305616</td>\n",
       "      <td>0.432275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.546865</td>\n",
       "      <td>0.425218</td>\n",
       "      <td>0.142721</td>\n",
       "      <td>3.153846</td>\n",
       "      <td>0.387938</td>\n",
       "      <td>0.187803</td>\n",
       "      <td>0.236509</td>\n",
       "      <td>0.345428</td>\n",
       "      <td>0.251089</td>\n",
       "      <td>0.403483</td>\n",
       "      <td>1.341597</td>\n",
       "      <td>0.174605</td>\n",
       "      <td>1.114804</td>\n",
       "      <td>0.035425</td>\n",
       "      <td>0.379231</td>\n",
       "      <td>0.575740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.574629</td>\n",
       "      <td>0.449290</td>\n",
       "      <td>0.153526</td>\n",
       "      <td>3.389590</td>\n",
       "      <td>0.447217</td>\n",
       "      <td>0.223128</td>\n",
       "      <td>0.282976</td>\n",
       "      <td>0.391167</td>\n",
       "      <td>0.145110</td>\n",
       "      <td>0.463722</td>\n",
       "      <td>1.436270</td>\n",
       "      <td>0.180797</td>\n",
       "      <td>1.177445</td>\n",
       "      <td>0.042198</td>\n",
       "      <td>0.436501</td>\n",
       "      <td>0.670345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.599374</td>\n",
       "      <td>0.470595</td>\n",
       "      <td>0.161253</td>\n",
       "      <td>3.519507</td>\n",
       "      <td>0.503977</td>\n",
       "      <td>0.252393</td>\n",
       "      <td>0.319866</td>\n",
       "      <td>0.410678</td>\n",
       "      <td>0.127310</td>\n",
       "      <td>0.462012</td>\n",
       "      <td>1.451485</td>\n",
       "      <td>0.191770</td>\n",
       "      <td>1.231222</td>\n",
       "      <td>0.048357</td>\n",
       "      <td>0.481118</td>\n",
       "      <td>0.756371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.589457</td>\n",
       "      <td>0.462434</td>\n",
       "      <td>0.161292</td>\n",
       "      <td>3.441948</td>\n",
       "      <td>0.472781</td>\n",
       "      <td>0.239575</td>\n",
       "      <td>0.321157</td>\n",
       "      <td>0.479401</td>\n",
       "      <td>0.078652</td>\n",
       "      <td>0.441948</td>\n",
       "      <td>1.415925</td>\n",
       "      <td>0.184972</td>\n",
       "      <td>1.213184</td>\n",
       "      <td>0.046909</td>\n",
       "      <td>0.482449</td>\n",
       "      <td>0.712356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.578892</td>\n",
       "      <td>0.456453</td>\n",
       "      <td>0.160887</td>\n",
       "      <td>3.423645</td>\n",
       "      <td>0.434638</td>\n",
       "      <td>0.235421</td>\n",
       "      <td>0.320828</td>\n",
       "      <td>0.433498</td>\n",
       "      <td>0.118227</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>1.432759</td>\n",
       "      <td>0.173144</td>\n",
       "      <td>1.196232</td>\n",
       "      <td>0.045341</td>\n",
       "      <td>0.481714</td>\n",
       "      <td>0.670059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.580198</td>\n",
       "      <td>0.458294</td>\n",
       "      <td>0.163571</td>\n",
       "      <td>3.460317</td>\n",
       "      <td>0.427190</td>\n",
       "      <td>0.235258</td>\n",
       "      <td>0.329960</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>1.483782</td>\n",
       "      <td>0.167162</td>\n",
       "      <td>1.202063</td>\n",
       "      <td>0.045950</td>\n",
       "      <td>0.493532</td>\n",
       "      <td>0.662448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.575728</td>\n",
       "      <td>0.456262</td>\n",
       "      <td>0.161359</td>\n",
       "      <td>3.514563</td>\n",
       "      <td>0.402471</td>\n",
       "      <td>0.227539</td>\n",
       "      <td>0.321650</td>\n",
       "      <td>0.398058</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>0.504854</td>\n",
       "      <td>1.562903</td>\n",
       "      <td>0.157745</td>\n",
       "      <td>1.193350</td>\n",
       "      <td>0.044150</td>\n",
       "      <td>0.483010</td>\n",
       "      <td>0.630010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.587537</td>\n",
       "      <td>0.468433</td>\n",
       "      <td>0.171866</td>\n",
       "      <td>3.477612</td>\n",
       "      <td>0.421716</td>\n",
       "      <td>0.239769</td>\n",
       "      <td>0.377858</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>0.104478</td>\n",
       "      <td>0.447761</td>\n",
       "      <td>1.438269</td>\n",
       "      <td>0.168047</td>\n",
       "      <td>1.227836</td>\n",
       "      <td>0.049663</td>\n",
       "      <td>0.549724</td>\n",
       "      <td>0.661485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.601034</td>\n",
       "      <td>0.475345</td>\n",
       "      <td>0.173966</td>\n",
       "      <td>3.586207</td>\n",
       "      <td>0.467052</td>\n",
       "      <td>0.251233</td>\n",
       "      <td>0.393414</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.431034</td>\n",
       "      <td>1.491750</td>\n",
       "      <td>0.175992</td>\n",
       "      <td>1.250345</td>\n",
       "      <td>0.051967</td>\n",
       "      <td>0.567379</td>\n",
       "      <td>0.718284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.596071</td>\n",
       "      <td>0.471310</td>\n",
       "      <td>0.171548</td>\n",
       "      <td>3.547619</td>\n",
       "      <td>0.446833</td>\n",
       "      <td>0.242762</td>\n",
       "      <td>0.386762</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.471262</td>\n",
       "      <td>0.172599</td>\n",
       "      <td>1.238929</td>\n",
       "      <td>0.049926</td>\n",
       "      <td>0.558310</td>\n",
       "      <td>0.689595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.595625</td>\n",
       "      <td>0.470781</td>\n",
       "      <td>0.170313</td>\n",
       "      <td>3.531250</td>\n",
       "      <td>0.440625</td>\n",
       "      <td>0.248859</td>\n",
       "      <td>0.365312</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.468750</td>\n",
       "      <td>1.476453</td>\n",
       "      <td>0.170422</td>\n",
       "      <td>1.236719</td>\n",
       "      <td>0.049597</td>\n",
       "      <td>0.535625</td>\n",
       "      <td>0.689484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.603654</td>\n",
       "      <td>0.482308</td>\n",
       "      <td>0.173846</td>\n",
       "      <td>3.769231</td>\n",
       "      <td>0.458115</td>\n",
       "      <td>0.251135</td>\n",
       "      <td>0.425885</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>1.634096</td>\n",
       "      <td>0.163715</td>\n",
       "      <td>1.259808</td>\n",
       "      <td>0.051744</td>\n",
       "      <td>0.599731</td>\n",
       "      <td>0.709250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.618214</td>\n",
       "      <td>0.485357</td>\n",
       "      <td>0.171071</td>\n",
       "      <td>3.714286</td>\n",
       "      <td>0.447464</td>\n",
       "      <td>0.246607</td>\n",
       "      <td>0.444643</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>1.575571</td>\n",
       "      <td>0.162470</td>\n",
       "      <td>1.274643</td>\n",
       "      <td>0.053843</td>\n",
       "      <td>0.615714</td>\n",
       "      <td>0.694071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.595000</td>\n",
       "      <td>0.472500</td>\n",
       "      <td>0.189167</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>0.405000</td>\n",
       "      <td>0.217167</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.487833</td>\n",
       "      <td>0.155500</td>\n",
       "      <td>1.256667</td>\n",
       "      <td>0.054986</td>\n",
       "      <td>0.579167</td>\n",
       "      <td>0.622167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.587222</td>\n",
       "      <td>0.463889</td>\n",
       "      <td>0.172778</td>\n",
       "      <td>3.555556</td>\n",
       "      <td>0.399444</td>\n",
       "      <td>0.254833</td>\n",
       "      <td>0.370556</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.641833</td>\n",
       "      <td>0.143852</td>\n",
       "      <td>1.223889</td>\n",
       "      <td>0.049744</td>\n",
       "      <td>0.543333</td>\n",
       "      <td>0.654278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.695000</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.600250</td>\n",
       "      <td>0.332750</td>\n",
       "      <td>0.632500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.434500</td>\n",
       "      <td>0.200083</td>\n",
       "      <td>1.435000</td>\n",
       "      <td>0.075100</td>\n",
       "      <td>0.832500</td>\n",
       "      <td>0.933000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.645000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.215000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.426500</td>\n",
       "      <td>0.228500</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.835000</td>\n",
       "      <td>0.142167</td>\n",
       "      <td>1.350000</td>\n",
       "      <td>0.067951</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.495000</td>\n",
       "      <td>0.195000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.384000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.051000</td>\n",
       "      <td>0.128000</td>\n",
       "      <td>1.290000</td>\n",
       "      <td>0.057915</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.574000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.607500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.202500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.539000</td>\n",
       "      <td>0.298000</td>\n",
       "      <td>0.705000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.458000</td>\n",
       "      <td>0.179667</td>\n",
       "      <td>1.310000</td>\n",
       "      <td>0.063042</td>\n",
       "      <td>0.907500</td>\n",
       "      <td>0.837000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.585000</td>\n",
       "      <td>0.185000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.705500</td>\n",
       "      <td>0.321500</td>\n",
       "      <td>0.475000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.498000</td>\n",
       "      <td>0.235167</td>\n",
       "      <td>1.470000</td>\n",
       "      <td>0.075757</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>1.027000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Length  Diameter    Height  Whole weight  Shucked weight   \n",
       "Target                                                               \n",
       "1       0.075000  0.055000  0.010000      2.000000        0.001000  \\\n",
       "2       0.150000  0.100000  0.025000      2.000000        0.004500   \n",
       "3       0.176000  0.128667  0.041667      2.000000        0.011767   \n",
       "4       0.221491  0.161579  0.053947      2.000000        0.024719   \n",
       "5       0.285739  0.210696  0.069913      2.000000        0.061696   \n",
       "6       0.369363  0.278861  0.092065      2.084942        0.123158   \n",
       "7       0.422033  0.321535  0.105921      2.276215        0.182657   \n",
       "8       0.498776  0.384798  0.127007      2.765845        0.293773   \n",
       "9       0.546865  0.425218  0.142721      3.153846        0.387938   \n",
       "10      0.574629  0.449290  0.153526      3.389590        0.447217   \n",
       "11      0.599374  0.470595  0.161253      3.519507        0.503977   \n",
       "12      0.589457  0.462434  0.161292      3.441948        0.472781   \n",
       "13      0.578892  0.456453  0.160887      3.423645        0.434638   \n",
       "14      0.580198  0.458294  0.163571      3.460317        0.427190   \n",
       "15      0.575728  0.456262  0.161359      3.514563        0.402471   \n",
       "16      0.587537  0.468433  0.171866      3.477612        0.421716   \n",
       "17      0.601034  0.475345  0.173966      3.586207        0.467052   \n",
       "18      0.596071  0.471310  0.171548      3.547619        0.446833   \n",
       "19      0.595625  0.470781  0.170313      3.531250        0.440625   \n",
       "20      0.603654  0.482308  0.173846      3.769231        0.458115   \n",
       "21      0.618214  0.485357  0.171071      3.714286        0.447464   \n",
       "22      0.595000  0.472500  0.189167      3.500000        0.405000   \n",
       "23      0.587222  0.463889  0.172778      3.555556        0.399444   \n",
       "24      0.695000  0.540000  0.200000      4.000000        0.600250   \n",
       "25      0.645000  0.490000  0.215000      4.000000        0.426500   \n",
       "26      0.600000  0.495000  0.195000      4.000000        0.384000   \n",
       "27      0.607500  0.500000  0.202500      4.000000        0.539000   \n",
       "29      0.700000  0.585000  0.185000      4.000000        0.705500   \n",
       "\n",
       "        Viscera weight  Shell weight     Sex_F     Sex_I     Sex_M     water   \n",
       "Target                                                                         \n",
       "1             0.000500      0.001500  0.000000  1.000000  0.000000  0.997000  \\\n",
       "2             0.004000      0.005000  0.000000  1.000000  0.000000  0.986500   \n",
       "3             0.006267      0.008933  0.000000  0.800000  0.200000  0.973033   \n",
       "4             0.012956      0.018000  0.000000  0.894737  0.105263  0.944325   \n",
       "5             0.027330      0.036770  0.034783  0.869565  0.095652  0.874204   \n",
       "6             0.058371      0.078388  0.061776  0.833977  0.104247  0.825025   \n",
       "7             0.085899      0.111648  0.112532  0.682864  0.204604  0.896010   \n",
       "8             0.138502      0.178609  0.214789  0.482394  0.302817  1.154961   \n",
       "9             0.187803      0.236509  0.345428  0.251089  0.403483  1.341597   \n",
       "10            0.223128      0.282976  0.391167  0.145110  0.463722  1.436270   \n",
       "11            0.252393      0.319866  0.410678  0.127310  0.462012  1.451485   \n",
       "12            0.239575      0.321157  0.479401  0.078652  0.441948  1.415925   \n",
       "13            0.235421      0.320828  0.433498  0.118227  0.448276  1.432759   \n",
       "14            0.235258      0.329960  0.444444  0.111111  0.444444  1.483782   \n",
       "15            0.227539      0.321650  0.398058  0.097087  0.504854  1.562903   \n",
       "16            0.239769      0.377858  0.447761  0.104478  0.447761  1.438269   \n",
       "17            0.251233      0.393414  0.448276  0.120690  0.431034  1.491750   \n",
       "18            0.242762      0.386762  0.452381  0.119048  0.428571  1.471262   \n",
       "19            0.248859      0.365312  0.468750  0.062500  0.468750  1.476453   \n",
       "20            0.251135      0.425885  0.461538  0.076923  0.461538  1.634096   \n",
       "21            0.246607      0.444643  0.500000  0.071429  0.428571  1.575571   \n",
       "22            0.217167      0.390000  0.500000  0.000000  0.500000  1.487833   \n",
       "23            0.254833      0.370556  0.666667  0.000000  0.333333  1.641833   \n",
       "24            0.332750      0.632500  0.500000  0.000000  0.500000  1.434500   \n",
       "25            0.228500      0.510000  1.000000  0.000000  0.000000  1.835000   \n",
       "26            0.190000      0.375000  0.000000  0.000000  1.000000  2.051000   \n",
       "27            0.298000      0.705000  0.500000  0.000000  0.500000  1.458000   \n",
       "29            0.321500      0.475000  1.000000  0.000000  0.000000  1.498000   \n",
       "\n",
       "           ratio      new1      new2      new3      new4  \n",
       "Target                                                    \n",
       "1       0.001000  0.140000  0.000041  0.011500  0.001500  \n",
       "2       0.004500  0.275000  0.000375  0.030000  0.008500  \n",
       "3       0.011767  0.346333  0.001047  0.050600  0.018033  \n",
       "4       0.024719  0.437018  0.002239  0.071947  0.037675  \n",
       "5       0.061696  0.566348  0.004771  0.106683  0.089026  \n",
       "6       0.109508  0.740289  0.010725  0.170453  0.181529  \n",
       "7       0.138183  0.849488  0.015846  0.217569  0.268556  \n",
       "8       0.162672  1.010581  0.026233  0.305616  0.432275  \n",
       "9       0.174605  1.114804  0.035425  0.379231  0.575740  \n",
       "10      0.180797  1.177445  0.042198  0.436501  0.670345  \n",
       "11      0.191770  1.231222  0.048357  0.481118  0.756371  \n",
       "12      0.184972  1.213184  0.046909  0.482449  0.712356  \n",
       "13      0.173144  1.196232  0.045341  0.481714  0.670059  \n",
       "14      0.167162  1.202063  0.045950  0.493532  0.662448  \n",
       "15      0.157745  1.193350  0.044150  0.483010  0.630010  \n",
       "16      0.168047  1.227836  0.049663  0.549724  0.661485  \n",
       "17      0.175992  1.250345  0.051967  0.567379  0.718284  \n",
       "18      0.172599  1.238929  0.049926  0.558310  0.689595  \n",
       "19      0.170422  1.236719  0.049597  0.535625  0.689484  \n",
       "20      0.163715  1.259808  0.051744  0.599731  0.709250  \n",
       "21      0.162470  1.274643  0.053843  0.615714  0.694071  \n",
       "22      0.155500  1.256667  0.054986  0.579167  0.622167  \n",
       "23      0.143852  1.223889  0.049744  0.543333  0.654278  \n",
       "24      0.200083  1.435000  0.075100  0.832500  0.933000  \n",
       "25      0.142167  1.350000  0.067951  0.725000  0.655000  \n",
       "26      0.128000  1.290000  0.057915  0.570000  0.574000  \n",
       "27      0.179667  1.310000  0.063042  0.907500  0.837000  \n",
       "29      0.235167  1.470000  0.075757  0.660000  1.027000  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"Target\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#조합변수 생성(3개의 칼럼사용)\n",
    "def abcd(df,col1,col2,col3,set_col,df_update = False, test = False):\n",
    "    result1 = []\n",
    "    df[set_col] = 2*df[col1]+df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = 2*df[col1]-df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = 2*df[col1]/df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = 2*df[col1]*df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] =3*df[col1]+df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = 3*df[col1]-df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = 3*df[col1]/df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = 3*df[col1]*df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "\n",
    "    \n",
    "    \n",
    "    df[set_col] = df[col1]+df[col2]+df[col3] #8\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = df[col1]*df[col2]*df[col3]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = df[col1]-df[col2]-df[col3]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = df[col1]/df[col2]/df[col3]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    \n",
    "    df[set_col] = df[col1]/df[col2]+df[col3]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = (df[col1]+df[col2])*df[col3]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = df[col1]-df[col2]*df[col3]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = 2*df[col1]*df[col2]/df[col3]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    \n",
    "    df[set_col] = df[col1]/df[col2]-df[col3]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = df[col1]*df[col2]-df[col3]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = df[col1]+df[col2]-df[col3]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "\n",
    "\n",
    "                     \n",
    "    max_corr = 0\n",
    "    for n, i in enumerate(result1):\n",
    "        if max_corr < abs(i):\n",
    "            max_corr = abs(i)\n",
    "            num = n\n",
    "            \n",
    "    if df_update != False:\n",
    "\n",
    "        if num == 0:\n",
    "            df[set_col] = 2*df[col1]+df[col2]\n",
    "            test[set_col] = 2*test[col1]+test[col2]\n",
    "        elif num == 1:\n",
    "            df[set_col] = 2*df[col1]-df[col2]\n",
    "            test[set_col] = 2*test[col1]-test[col2]\n",
    "        elif num == 2:\n",
    "            df[set_col] = 2*df[col1]/df[col2]\n",
    "            test[set_col] = 2*test[col1]/test[col2]\n",
    "        elif num == 3:\n",
    "            df[set_col] = 2*df[col1]*df[col2]\n",
    "            test[set_col] = 2*test[col1]*test[col2]\n",
    "        if num == 4:\n",
    "            df[set_col] = 3*df[col1]+df[col2]\n",
    "            test[set_col] = 3*test[col1]+test[col2]\n",
    "        elif num == 5:\n",
    "            df[set_col] = 3*df[col1]-df[col2]\n",
    "            test[set_col] = 3*test[col1]-test[col2]\n",
    "        elif num == 6:\n",
    "            df[set_col] = 3*df[col1]/df[col2]\n",
    "            test[set_col] = 3*test[col1]/test[col2]\n",
    "        elif num == 7:\n",
    "            df[set_col] = 3*df[col1]*df[col2]\n",
    "            test[set_col] = 3*test[col1]*test[col2]\n",
    "            \n",
    "        elif num == 8:\n",
    "            df[set_col] = df[col1]+df[col2]+df[col3]\n",
    "            test[set_col] = test[col1]+test[col2]+df[col3]\n",
    "        elif num == 9:\n",
    "            df[set_col] = df[col1]*df[col2]*df[col3]\n",
    "            test[set_col] = test[col1]*test[col2]*test[col3]\n",
    "        elif num == 10:\n",
    "            df[set_col] = df[col1]-df[col2]-df[col3]\n",
    "            test[set_col] = test[col1]-test[col2]-test[col3]\n",
    "        elif num == 11:\n",
    "            df[set_col] = df[col1]/df[col2]/df[col3]\n",
    "            test[set_col] = test[col1]/test[col2]/test[col3]\n",
    "            \n",
    "                                           \n",
    "        elif num == 12:\n",
    "            df[set_col] = df[col1]/df[col2]+df[col3]\n",
    "            test[set_col] = test[col1]/test[col2]+test[col3]                             \n",
    "        elif num == 13:\n",
    "            df[set_col] = (df[col1]+df[col2])*df[col3]\n",
    "            test[set_col] = (test[col1]+test[col2])*test[col3]   \n",
    "        elif num == 14:\n",
    "            df[set_col] = df[col1]-df[col2]*df[col3]\n",
    "            test[set_col] = test[col1]-test[col2]*test[col3] \n",
    "        elif num == 15:\n",
    "            df[set_col] = 2*df[col1]*df[col2]/df[col3]\n",
    "            test[set_col] = 2*test[col1]*test[col2]/test[col3] \n",
    "            \n",
    "        elif num == 16:\n",
    "            df[set_col] = df[col1]/df[col2]-df[col3]\n",
    "            test[set_col] = test[col1]/test[col2]-test[col3]                             \n",
    "        elif num == 17:\n",
    "            df[set_col] = df[col1]*df[col2]-df[col3]\n",
    "            test[set_col] = test[col1]*test[col2]-test[col3]   \n",
    "        elif num == 18:\n",
    "            df[set_col] = df[col1]+df[col2]-df[col3]\n",
    "            test[set_col] = test[col1]+test[col2]-test[col3] \n",
    "\n",
    "\n",
    "        return df, test\n",
    "                     \n",
    "                     \n",
    "    else:\n",
    "        return max_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = df.drop(['Target','Sex_F',\t'Sex_I', 'Sex_M'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5567195769296182\n",
      "0.5635045502751335\n",
      "0.5824580524394525\n",
      "0.5970773185643968\n",
      "0.6330777205886574\n",
      "0.6375729688557358\n",
      "0.6689083585983355\n",
      "0.6883295542726244\n",
      "0.7038205938320047\n"
     ]
    }
   ],
   "source": [
    "max_result = 0\n",
    "max_result_total = 0\n",
    "\n",
    "for col1 in X_features.columns:\n",
    "    for col2 in X_features.columns: \n",
    "        if (col1=='Target')|(col2=='bcd'):\n",
    "            continue\n",
    "        else:\n",
    "            result = abcd1(df, col1, col2,'new6')\n",
    "#             print(result,col1,col2)\n",
    "            if max_result < abs(result):\n",
    "                max_result = result\n",
    "                print(max_result)\n",
    "                columns = [col1, col2]\n",
    "df,test = abcd1(df,columns[0],columns[1],'new6',df_update=True,test = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.drop(['new4','new2','new1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean.to_csv('clean_model1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#조합변수 생성, 2개의 칼럼사용\n",
    "def abcd1(df,col1,col2,set_col,df_update = False, test = False):\n",
    "    result1 = []\n",
    "    df[set_col] = 2*df[col1]+df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = 2*df[col1]-df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = 2*df[col1]/df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = 2*df[col1]*df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] =3*df[col1]+df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = 3*df[col1]-df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = 3*df[col1]/df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    df[set_col] = 3*df[col1]*df[col2]\n",
    "    result = df.set_index(set_col).reset_index().corr()[set_col]['Target']\n",
    "    result1.append(result)\n",
    "    max_corr = 0\n",
    "    for n, i in enumerate(result1):\n",
    "        if max_corr < abs(i):\n",
    "            max_corr = abs(i)\n",
    "            num = n\n",
    "            \n",
    "    if df_update != False:\n",
    "\n",
    "        if num == 0:\n",
    "            df[set_col] = 2*df[col1]+df[col2]\n",
    "            test[set_col] = 2*test[col1]+test[col2]\n",
    "        elif num == 1:\n",
    "            df[set_col] = 2*df[col1]-df[col2]\n",
    "            test[set_col] = 2*test[col1]-test[col2]\n",
    "        elif num == 2:\n",
    "            df[set_col] = 2*df[col1]/df[col2]\n",
    "            test[set_col] = 2*test[col1]/test[col2]\n",
    "        elif num == 3:\n",
    "            df[set_col] = 2*df[col1]*df[col2]\n",
    "            test[set_col] = 2*test[col1]*test[col2]\n",
    "        if num == 4:\n",
    "            df[set_col] = 3*df[col1]+df[col2]\n",
    "            test[set_col] = 3*test[col1]+test[col2]\n",
    "        elif num == 5:\n",
    "            df[set_col] = 3*df[col1]-df[col2]\n",
    "            test[set_col] = 3*test[col1]-test[col2]\n",
    "        elif num == 6:\n",
    "            df[set_col] = 3*df[col1]/df[col2]\n",
    "            test[set_col] = 3*test[col1]/test[col2]\n",
    "        elif num == 7:\n",
    "            df[set_col] = 3*df[col1]*df[col2]\n",
    "            test[set_col] = 3*test[col1]*test[col2]\n",
    "\n",
    "        return df, test\n",
    "    else:\n",
    "        return max_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Length            0.556720\n",
       "Diameter          0.574660\n",
       "Height            0.610292\n",
       "Whole weight      0.512265\n",
       "Shucked weight    0.420884\n",
       "Viscera weight    0.503819\n",
       "Shell weight      0.627574\n",
       "Rings             1.000000\n",
       "Sex_F             0.250279\n",
       "Sex_I            -0.436063\n",
       "Sex_M             0.181831\n",
       "water             0.402079\n",
       "ratio             0.302868\n",
       "new1              0.580124\n",
       "new2              0.553462\n",
       "new3              0.635700\n",
       "new4              0.455207\n",
       "Name: Rings, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df.corr()['Rings']\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5567195769296183\n",
      "0.5746598513059247\n",
      "0.6102920256576804\n",
      "0.6275740445103185\n",
      "0.635699794021367\n",
      "0.7235713796162117\n",
      "0.7248146264945484\n",
      "0.7257223988669331\n",
      "a2\n",
      "0.7059253713204919\n",
      "0.7074871201421244\n",
      "0.7076163635121877\n",
      "0.7080355791148196\n",
      "0.708986758729633\n",
      "a3\n",
      "0.7059253713204919\n",
      "0.7074871201421244\n",
      "0.7076163635121877\n",
      "0.7080355791148196\n",
      "0.708986758729633\n",
      "0.718102385784465\n",
      "0.7224647160160703\n",
      "a4\n",
      "0.7224647160160717\n",
      "0.7244981660043733\n",
      "0.7248889849827949\n",
      "0.7250927241639409\n",
      "0.7261848693362479\n",
      "0.726260756388801\n",
      "a5\n",
      "0.7264346143287239\n",
      "0.72644687717704\n",
      "0.7267486628761616\n",
      "0.7277256731770174\n",
      "0.7290272518295857\n",
      "0.7299107544364131\n",
      "0.7303509602827355\n",
      "0.7303639528242662\n",
      "0.7304979512776675\n",
      "0.7308650663563389\n"
     ]
    }
   ],
   "source": [
    "max_result = 0\n",
    "max_result_total = 0\n",
    "\n",
    "# abc = shell+ height\n",
    "\n",
    "\n",
    "X_features = df.drop(['Target','Sex_F',\t'Sex_I', 'Sex_M', 'new6'],axis=1)\n",
    "for col1 in X_features.columns:\n",
    "    for col2 in X_features.columns: \n",
    "        for col3 in X_features.columns:\n",
    "\n",
    "            if (col1=='Target')|(col2=='bcd'):\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                result = abcd(df, col1, col2,col3,'a1')\n",
    "    #             print(result,col1,col2)\n",
    "\n",
    "                if max_result < abs(result):\n",
    "\n",
    "                    max_result = result\n",
    "                    print(max_result)\n",
    "                    columns = [col1, col2,col3]\n",
    "df,test = abcd(df,columns[0],columns[1],columns[2],'a1',df_update=True,test = test)\n",
    "\n",
    "max_result = 0\n",
    "max_result_total = 0\n",
    "\n",
    "print('a2')\n",
    "max_result = 0\n",
    "max_result_total = 0\n",
    "X_features = df.drop(['Target','Sex_F',\t'Sex_I', 'Sex_M','new6'],axis=1)\n",
    "for col2 in X_features.columns: \n",
    "    for col3 in X_features.columns:\n",
    "        col1 = 'a1'\n",
    "        if (col1=='Target')|(col2=='bcd'):\n",
    "            continue\n",
    "        else:\n",
    "\n",
    "            result = abcd(df, col1, col2,col3,'a2')\n",
    "#             print(result,col1,col2)\n",
    "\n",
    "            if max_result < abs(result):\n",
    "\n",
    "                max_result = result\n",
    "                print(max_result)\n",
    "                columns = [col1, col2,col3]\n",
    "df,test = abcd(df,columns[0],columns[1],columns[2],'a2',df_update=True,test = test)\n",
    "\n",
    "X_features = df.drop(['Target','Sex_F',\t'Sex_I', 'Sex_M', 'new6'],axis=1)\n",
    "max_result = 0\n",
    "max_result_total = 0\n",
    "print('a3')                \n",
    "col_lists = ['a1','a2']             \n",
    "max_result = 0\n",
    "max_result_total = 0\n",
    "\n",
    "\n",
    "for col1 in col_lists:\n",
    "    for col2 in X_features.columns: \n",
    "        for col3 in X_features.columns:\n",
    "\n",
    "            if (col1=='Target')|(col2=='bcd'):\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                result = abcd(df, col1, col2,col3,'a3')\n",
    "    #             print(result,col1,col2)\n",
    "\n",
    "                if max_result < abs(result):\n",
    "\n",
    "                    max_result = result\n",
    "                    print(max_result)\n",
    "                    columns = [col1, col2,col3]\n",
    "df,test = abcd(df,columns[0],columns[1],columns[2],'a3',df_update=True,test = test)\n",
    "print('a4') \n",
    "X_features = df.drop(['Target','Sex_F',\t'Sex_I', 'Sex_M','new6'],axis=1)\n",
    "col_lists = ['a1','a2','a3']  \n",
    "for col1 in col_lists:\n",
    "    for col2 in X_features.columns: \n",
    "        for col3 in X_features.columns:\n",
    "\n",
    "            if (col1=='Target')|(col2=='bcd'):\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                result = abcd(df, col1, col2,col3,'a4')\n",
    "    #             print(result,col1,col2)\n",
    "\n",
    "                if max_result < abs(result):\n",
    "\n",
    "                    max_result = result\n",
    "                    print(max_result)\n",
    "                    columns = [col1, col2,col3]\n",
    "df,test = abcd(df,columns[0],columns[1],columns[2],'a4',df_update=True,test = test)\n",
    "print('a5') \n",
    "X_features = df.drop(['Target','Sex_F',\t'Sex_I', 'Sex_M','new6'],axis=1)\n",
    "col_lists = ['a4']  \n",
    "for col1 in col_lists:\n",
    "    for col2 in X_features.columns: \n",
    "        for col3 in X_features.columns:\n",
    "\n",
    "            if (col1=='Target')|(col2=='bcd'):\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                result = abcd(df, col1, col2,col3,'a5')\n",
    "    #             print(result,col1,col2)\n",
    "\n",
    "                if max_result < abs(result):\n",
    "\n",
    "                    max_result = result\n",
    "                    print(max_result)\n",
    "                    columns = [col1, col2,col3]\n",
    "df,test = abcd(df,columns[0],columns[1],columns[2],'a5',df_update=True,test = test)\n",
    "\n",
    "X_features = df.drop(['Target','Sex_F',\t'Sex_I', 'Sex_M','new6'],axis=1)\n",
    "col_lists = ['a1','a2','a3','a4','a5']  \n",
    "for col1 in col_lists:\n",
    "    for col2 in X_features.columns: \n",
    "        for col3 in X_features.columns:\n",
    "\n",
    "            if (col1=='Target')|(col2=='bcd'):\n",
    "                continue\n",
    "            else:\n",
    "\n",
    "                result = abcd(df, col1, col2,col3,'a6')\n",
    "    #             print(result,col1,col2)\n",
    "\n",
    "                if max_result < abs(result):\n",
    "\n",
    "                    max_result = result\n",
    "                    print(max_result)\n",
    "                    columns = [col1, col2,col3]\n",
    "df,test = abcd(df,columns[0],columns[1],columns[2],'a6',df_update=True,test = test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['new1','new2','new3','new4','a1','a3','a4','a5','a6'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Length</th>\n",
       "      <th>Diameter</th>\n",
       "      <th>Height</th>\n",
       "      <th>Whole weight</th>\n",
       "      <th>Shucked weight</th>\n",
       "      <th>Viscera weight</th>\n",
       "      <th>Shell weight</th>\n",
       "      <th>Sex_F</th>\n",
       "      <th>Sex_I</th>\n",
       "      <th>Sex_M</th>\n",
       "      <th>water</th>\n",
       "      <th>ratio</th>\n",
       "      <th>new6</th>\n",
       "      <th>a2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Target</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.556720</td>\n",
       "      <td>0.574660</td>\n",
       "      <td>0.610292</td>\n",
       "      <td>0.512265</td>\n",
       "      <td>0.420884</td>\n",
       "      <td>0.503819</td>\n",
       "      <td>0.627574</td>\n",
       "      <td>0.250279</td>\n",
       "      <td>-0.436063</td>\n",
       "      <td>0.181831</td>\n",
       "      <td>0.402079</td>\n",
       "      <td>0.302868</td>\n",
       "      <td>0.703821</td>\n",
       "      <td>-0.708987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length</th>\n",
       "      <td>0.556720</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986812</td>\n",
       "      <td>0.900808</td>\n",
       "      <td>0.877635</td>\n",
       "      <td>0.897914</td>\n",
       "      <td>0.903018</td>\n",
       "      <td>0.897706</td>\n",
       "      <td>0.309666</td>\n",
       "      <td>-0.551465</td>\n",
       "      <td>0.236543</td>\n",
       "      <td>0.646363</td>\n",
       "      <td>0.772807</td>\n",
       "      <td>0.657793</td>\n",
       "      <td>-0.775896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diameter</th>\n",
       "      <td>0.574660</td>\n",
       "      <td>0.986812</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907106</td>\n",
       "      <td>0.878494</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.899724</td>\n",
       "      <td>0.905330</td>\n",
       "      <td>0.318626</td>\n",
       "      <td>-0.564315</td>\n",
       "      <td>0.240376</td>\n",
       "      <td>0.649156</td>\n",
       "      <td>0.763109</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>-0.797337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Height</th>\n",
       "      <td>0.610292</td>\n",
       "      <td>0.900808</td>\n",
       "      <td>0.907106</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833599</td>\n",
       "      <td>0.836760</td>\n",
       "      <td>0.866261</td>\n",
       "      <td>0.890891</td>\n",
       "      <td>0.316806</td>\n",
       "      <td>-0.557686</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.608466</td>\n",
       "      <td>0.689508</td>\n",
       "      <td>0.763744</td>\n",
       "      <td>-0.847649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Whole weight</th>\n",
       "      <td>0.512265</td>\n",
       "      <td>0.877635</td>\n",
       "      <td>0.878494</td>\n",
       "      <td>0.833599</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.866991</td>\n",
       "      <td>0.874543</td>\n",
       "      <td>0.862312</td>\n",
       "      <td>0.306389</td>\n",
       "      <td>-0.567531</td>\n",
       "      <td>0.255274</td>\n",
       "      <td>0.897198</td>\n",
       "      <td>0.522701</td>\n",
       "      <td>0.613037</td>\n",
       "      <td>-0.665960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shucked weight</th>\n",
       "      <td>0.420884</td>\n",
       "      <td>0.897914</td>\n",
       "      <td>0.893162</td>\n",
       "      <td>0.836760</td>\n",
       "      <td>0.866991</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931961</td>\n",
       "      <td>0.882617</td>\n",
       "      <td>0.263991</td>\n",
       "      <td>-0.521842</td>\n",
       "      <td>0.251793</td>\n",
       "      <td>0.583140</td>\n",
       "      <td>0.826140</td>\n",
       "      <td>0.483261</td>\n",
       "      <td>-0.630747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Viscera weight</th>\n",
       "      <td>0.503819</td>\n",
       "      <td>0.903018</td>\n",
       "      <td>0.899724</td>\n",
       "      <td>0.866261</td>\n",
       "      <td>0.874543</td>\n",
       "      <td>0.931961</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907656</td>\n",
       "      <td>0.308444</td>\n",
       "      <td>-0.556081</td>\n",
       "      <td>0.242194</td>\n",
       "      <td>0.603966</td>\n",
       "      <td>0.721885</td>\n",
       "      <td>0.616937</td>\n",
       "      <td>-0.726357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Shell weight</th>\n",
       "      <td>0.627574</td>\n",
       "      <td>0.897706</td>\n",
       "      <td>0.905330</td>\n",
       "      <td>0.890891</td>\n",
       "      <td>0.862312</td>\n",
       "      <td>0.882617</td>\n",
       "      <td>0.907656</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.306319</td>\n",
       "      <td>-0.546953</td>\n",
       "      <td>0.235391</td>\n",
       "      <td>0.598829</td>\n",
       "      <td>0.670495</td>\n",
       "      <td>0.828688</td>\n",
       "      <td>-0.895674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex_F</th>\n",
       "      <td>0.250279</td>\n",
       "      <td>0.309666</td>\n",
       "      <td>0.318626</td>\n",
       "      <td>0.316806</td>\n",
       "      <td>0.306389</td>\n",
       "      <td>0.263991</td>\n",
       "      <td>0.308444</td>\n",
       "      <td>0.306319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.464298</td>\n",
       "      <td>-0.512528</td>\n",
       "      <td>0.252752</td>\n",
       "      <td>0.188495</td>\n",
       "      <td>0.280121</td>\n",
       "      <td>-0.300124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex_I</th>\n",
       "      <td>-0.436063</td>\n",
       "      <td>-0.551465</td>\n",
       "      <td>-0.564315</td>\n",
       "      <td>-0.557686</td>\n",
       "      <td>-0.567531</td>\n",
       "      <td>-0.521842</td>\n",
       "      <td>-0.556081</td>\n",
       "      <td>-0.546953</td>\n",
       "      <td>-0.464298</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.522541</td>\n",
       "      <td>-0.461618</td>\n",
       "      <td>-0.377220</td>\n",
       "      <td>-0.435277</td>\n",
       "      <td>0.491957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex_M</th>\n",
       "      <td>0.181831</td>\n",
       "      <td>0.236543</td>\n",
       "      <td>0.240376</td>\n",
       "      <td>0.235700</td>\n",
       "      <td>0.255274</td>\n",
       "      <td>0.251793</td>\n",
       "      <td>0.242194</td>\n",
       "      <td>0.235391</td>\n",
       "      <td>-0.512528</td>\n",
       "      <td>-0.522541</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.204226</td>\n",
       "      <td>0.184259</td>\n",
       "      <td>0.152340</td>\n",
       "      <td>-0.188036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>water</th>\n",
       "      <td>0.402079</td>\n",
       "      <td>0.646363</td>\n",
       "      <td>0.649156</td>\n",
       "      <td>0.608466</td>\n",
       "      <td>0.897198</td>\n",
       "      <td>0.583140</td>\n",
       "      <td>0.603966</td>\n",
       "      <td>0.598829</td>\n",
       "      <td>0.252752</td>\n",
       "      <td>-0.461618</td>\n",
       "      <td>0.204226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.157104</td>\n",
       "      <td>0.461279</td>\n",
       "      <td>-0.440713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratio</th>\n",
       "      <td>0.302868</td>\n",
       "      <td>0.772807</td>\n",
       "      <td>0.763109</td>\n",
       "      <td>0.689508</td>\n",
       "      <td>0.522701</td>\n",
       "      <td>0.826140</td>\n",
       "      <td>0.721885</td>\n",
       "      <td>0.670495</td>\n",
       "      <td>0.188495</td>\n",
       "      <td>-0.377220</td>\n",
       "      <td>0.184259</td>\n",
       "      <td>0.157104</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.308777</td>\n",
       "      <td>-0.503549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new6</th>\n",
       "      <td>0.703821</td>\n",
       "      <td>0.657793</td>\n",
       "      <td>0.677900</td>\n",
       "      <td>0.763744</td>\n",
       "      <td>0.613037</td>\n",
       "      <td>0.483261</td>\n",
       "      <td>0.616937</td>\n",
       "      <td>0.828688</td>\n",
       "      <td>0.280121</td>\n",
       "      <td>-0.435277</td>\n",
       "      <td>0.152340</td>\n",
       "      <td>0.461279</td>\n",
       "      <td>0.308777</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.957996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a2</th>\n",
       "      <td>-0.708987</td>\n",
       "      <td>-0.775896</td>\n",
       "      <td>-0.797337</td>\n",
       "      <td>-0.847649</td>\n",
       "      <td>-0.665960</td>\n",
       "      <td>-0.630747</td>\n",
       "      <td>-0.726357</td>\n",
       "      <td>-0.895674</td>\n",
       "      <td>-0.300124</td>\n",
       "      <td>0.491957</td>\n",
       "      <td>-0.188036</td>\n",
       "      <td>-0.440713</td>\n",
       "      <td>-0.503549</td>\n",
       "      <td>-0.957996</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Target    Length  Diameter    Height  Whole weight   \n",
       "Target          1.000000  0.556720  0.574660  0.610292      0.512265  \\\n",
       "Length          0.556720  1.000000  0.986812  0.900808      0.877635   \n",
       "Diameter        0.574660  0.986812  1.000000  0.907106      0.878494   \n",
       "Height          0.610292  0.900808  0.907106  1.000000      0.833599   \n",
       "Whole weight    0.512265  0.877635  0.878494  0.833599      1.000000   \n",
       "Shucked weight  0.420884  0.897914  0.893162  0.836760      0.866991   \n",
       "Viscera weight  0.503819  0.903018  0.899724  0.866261      0.874543   \n",
       "Shell weight    0.627574  0.897706  0.905330  0.890891      0.862312   \n",
       "Sex_F           0.250279  0.309666  0.318626  0.316806      0.306389   \n",
       "Sex_I          -0.436063 -0.551465 -0.564315 -0.557686     -0.567531   \n",
       "Sex_M           0.181831  0.236543  0.240376  0.235700      0.255274   \n",
       "water           0.402079  0.646363  0.649156  0.608466      0.897198   \n",
       "ratio           0.302868  0.772807  0.763109  0.689508      0.522701   \n",
       "new6            0.703821  0.657793  0.677900  0.763744      0.613037   \n",
       "a2             -0.708987 -0.775896 -0.797337 -0.847649     -0.665960   \n",
       "\n",
       "                Shucked weight  Viscera weight  Shell weight     Sex_F   \n",
       "Target                0.420884        0.503819      0.627574  0.250279  \\\n",
       "Length                0.897914        0.903018      0.897706  0.309666   \n",
       "Diameter              0.893162        0.899724      0.905330  0.318626   \n",
       "Height                0.836760        0.866261      0.890891  0.316806   \n",
       "Whole weight          0.866991        0.874543      0.862312  0.306389   \n",
       "Shucked weight        1.000000        0.931961      0.882617  0.263991   \n",
       "Viscera weight        0.931961        1.000000      0.907656  0.308444   \n",
       "Shell weight          0.882617        0.907656      1.000000  0.306319   \n",
       "Sex_F                 0.263991        0.308444      0.306319  1.000000   \n",
       "Sex_I                -0.521842       -0.556081     -0.546953 -0.464298   \n",
       "Sex_M                 0.251793        0.242194      0.235391 -0.512528   \n",
       "water                 0.583140        0.603966      0.598829  0.252752   \n",
       "ratio                 0.826140        0.721885      0.670495  0.188495   \n",
       "new6                  0.483261        0.616937      0.828688  0.280121   \n",
       "a2                   -0.630747       -0.726357     -0.895674 -0.300124   \n",
       "\n",
       "                   Sex_I     Sex_M     water     ratio      new6        a2  \n",
       "Target         -0.436063  0.181831  0.402079  0.302868  0.703821 -0.708987  \n",
       "Length         -0.551465  0.236543  0.646363  0.772807  0.657793 -0.775896  \n",
       "Diameter       -0.564315  0.240376  0.649156  0.763109  0.677900 -0.797337  \n",
       "Height         -0.557686  0.235700  0.608466  0.689508  0.763744 -0.847649  \n",
       "Whole weight   -0.567531  0.255274  0.897198  0.522701  0.613037 -0.665960  \n",
       "Shucked weight -0.521842  0.251793  0.583140  0.826140  0.483261 -0.630747  \n",
       "Viscera weight -0.556081  0.242194  0.603966  0.721885  0.616937 -0.726357  \n",
       "Shell weight   -0.546953  0.235391  0.598829  0.670495  0.828688 -0.895674  \n",
       "Sex_F          -0.464298 -0.512528  0.252752  0.188495  0.280121 -0.300124  \n",
       "Sex_I           1.000000 -0.522541 -0.461618 -0.377220 -0.435277  0.491957  \n",
       "Sex_M          -0.522541  1.000000  0.204226  0.184259  0.152340 -0.188036  \n",
       "water          -0.461618  0.204226  1.000000  0.157104  0.461279 -0.440713  \n",
       "ratio          -0.377220  0.184259  0.157104  1.000000  0.308777 -0.503549  \n",
       "new6           -0.435277  0.152340  0.461279  0.308777  1.000000 -0.957996  \n",
       "a2              0.491957 -0.188036 -0.440713 -0.503549 -0.957996  1.000000  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mj985\\AppData\\Local\\Temp\\ipykernel_7160\\382423038.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X[feature_cols] = pipeline.fit_transform(X[feature_cols])\n"
     ]
    }
   ],
   "source": [
    "feature_cols = df.columns.tolist()\n",
    "feature_cols.remove('Target')\n",
    "\n",
    "target_cols = ['Target']\n",
    "X = df[feature_cols]\n",
    "y = df[target_cols]\n",
    "\n",
    "remove_list = ['Sex_I','Sex_F','Sex_M']\n",
    "for col in remove_list:\n",
    "    feature_cols.remove(col)\n",
    "pipeline = Pipeline([('normalizer', Normalizer()),\n",
    "                     ('scaler', StandardScaler())])\n",
    "scaler = StandardScaler()\n",
    "# X[feature_cols] = scaler.fit_transform(X[feature_cols])\n",
    "\n",
    "X[feature_cols] = pipeline.fit_transform(X[feature_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('final_model1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(df, test_size= 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, y_test = train_test_split(y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val = train_test_split(X_train, test_size = 0.2, random_state = 42)\n",
    "y_train, y_val = train_test_split(y_train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule = tensorflow.optimizers.schedules.PiecewiseConstantDecay(\n",
    "                [100, 150], [1e-0, 1e-1, 1e-2])\n",
    "step = tf.Variable(0, trainable=False)\n",
    "wd = lambda: 1e-3 * schedule(step)\n",
    "optimizer = AdamW(learning_rate=0.001, weight_decay=wd)\n",
    "def custom_opt(n):\n",
    "    schedule = tensorflow.optimizers.schedules.PiecewiseConstantDecay([100, 150], [1e-0, 1e-1, 1e-2])\n",
    "    step = tf.Variable(0, trainable=False)\n",
    "    wd = lambda: 1e-3 * schedule(step)\n",
    "    opt = AdamW(learning_rate = n, weight_decay = wd)\n",
    "    return opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "11/11 [==============================] - 1s 20ms/step - loss: 6.9270 - accuracy: 0.3352 - val_loss: 5.4942 - val_accuracy: 0.3883\n",
      "Epoch 2/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 2.4173 - accuracy: 0.7477 - val_loss: 4.9737 - val_accuracy: 0.4213\n",
      "Epoch 3/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.3444 - accuracy: 0.8540 - val_loss: 9.0421 - val_accuracy: 0.0431\n",
      "Epoch 4/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.1443 - accuracy: 0.8784 - val_loss: 3.8101 - val_accuracy: 0.5671\n",
      "Epoch 5/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0368 - accuracy: 0.8880 - val_loss: 4.0705 - val_accuracy: 0.5443\n",
      "Epoch 6/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 1.0173 - accuracy: 0.8906 - val_loss: 2.8984 - val_accuracy: 0.6588\n",
      "Epoch 7/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9165 - accuracy: 0.9022 - val_loss: 1.7441 - val_accuracy: 0.7855\n",
      "Epoch 8/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9100 - accuracy: 0.9032 - val_loss: 1.4401 - val_accuracy: 0.8237\n",
      "Epoch 9/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.9189 - accuracy: 0.9028 - val_loss: 1.5466 - val_accuracy: 0.8102\n",
      "Epoch 10/1000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.8956 - accuracy: 0.9046 - val_loss: 1.0503 - val_accuracy: 0.8769\n",
      "Epoch 11/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8435 - accuracy: 0.9099 - val_loss: 1.1043 - val_accuracy: 0.8706\n",
      "Epoch 12/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8189 - accuracy: 0.9122 - val_loss: 0.9491 - val_accuracy: 0.8944\n",
      "Epoch 13/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7867 - accuracy: 0.9169 - val_loss: 1.1022 - val_accuracy: 0.8813\n",
      "Epoch 14/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8423 - accuracy: 0.9110 - val_loss: 0.9326 - val_accuracy: 0.9018\n",
      "Epoch 15/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8355 - accuracy: 0.9112 - val_loss: 1.0989 - val_accuracy: 0.8849\n",
      "Epoch 16/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.8004 - accuracy: 0.9153 - val_loss: 1.0747 - val_accuracy: 0.8938\n",
      "Epoch 17/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7504 - accuracy: 0.9212 - val_loss: 1.0598 - val_accuracy: 0.8931\n",
      "Epoch 18/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7816 - accuracy: 0.9170 - val_loss: 1.0954 - val_accuracy: 0.8912\n",
      "Epoch 19/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7555 - accuracy: 0.9175 - val_loss: 1.1362 - val_accuracy: 0.8868\n",
      "Epoch 20/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7206 - accuracy: 0.9240 - val_loss: 1.1611 - val_accuracy: 0.8836\n",
      "Epoch 21/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7606 - accuracy: 0.9190 - val_loss: 1.0349 - val_accuracy: 0.9001\n",
      "Epoch 22/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7103 - accuracy: 0.9240 - val_loss: 0.9463 - val_accuracy: 0.9070\n",
      "Epoch 23/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.7310 - accuracy: 0.9226 - val_loss: 1.1107 - val_accuracy: 0.8912\n",
      "Epoch 24/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6825 - accuracy: 0.9278 - val_loss: 1.0457 - val_accuracy: 0.8961\n",
      "Epoch 25/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6750 - accuracy: 0.9274 - val_loss: 1.1507 - val_accuracy: 0.8897\n",
      "Epoch 26/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6792 - accuracy: 0.9294 - val_loss: 1.1180 - val_accuracy: 0.8940\n",
      "Epoch 27/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6825 - accuracy: 0.9275 - val_loss: 1.0286 - val_accuracy: 0.9066\n",
      "Epoch 28/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6695 - accuracy: 0.9280 - val_loss: 1.1443 - val_accuracy: 0.8907\n",
      "Epoch 29/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6554 - accuracy: 0.9308 - val_loss: 0.9525 - val_accuracy: 0.9112\n",
      "Epoch 30/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6850 - accuracy: 0.9276 - val_loss: 0.8484 - val_accuracy: 0.9245\n",
      "Epoch 31/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6736 - accuracy: 0.9301 - val_loss: 0.9920 - val_accuracy: 0.9079\n",
      "Epoch 32/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6519 - accuracy: 0.9311 - val_loss: 1.0060 - val_accuracy: 0.9063\n",
      "Epoch 33/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6490 - accuracy: 0.9317 - val_loss: 0.9138 - val_accuracy: 0.9179\n",
      "Epoch 34/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6147 - accuracy: 0.9358 - val_loss: 0.8833 - val_accuracy: 0.9179\n",
      "Epoch 35/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6170 - accuracy: 0.9360 - val_loss: 0.9940 - val_accuracy: 0.9102\n",
      "Epoch 36/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.6020 - accuracy: 0.9370 - val_loss: 0.9079 - val_accuracy: 0.9176\n",
      "Epoch 37/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5772 - accuracy: 0.9402 - val_loss: 1.0336 - val_accuracy: 0.9058\n",
      "Epoch 38/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5961 - accuracy: 0.9382 - val_loss: 0.8234 - val_accuracy: 0.9268\n",
      "Epoch 39/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5651 - accuracy: 0.9421 - val_loss: 0.9166 - val_accuracy: 0.9173\n",
      "Epoch 40/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5643 - accuracy: 0.9425 - val_loss: 0.8921 - val_accuracy: 0.9194\n",
      "Epoch 41/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5706 - accuracy: 0.9411 - val_loss: 0.8912 - val_accuracy: 0.9214\n",
      "Epoch 42/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5934 - accuracy: 0.9390 - val_loss: 0.8901 - val_accuracy: 0.9208\n",
      "Epoch 43/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5499 - accuracy: 0.9429 - val_loss: 0.8041 - val_accuracy: 0.9282\n",
      "Epoch 44/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5258 - accuracy: 0.9450 - val_loss: 0.9424 - val_accuracy: 0.9153\n",
      "Epoch 45/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5529 - accuracy: 0.9435 - val_loss: 0.7020 - val_accuracy: 0.9395\n",
      "Epoch 46/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5656 - accuracy: 0.9413 - val_loss: 0.8728 - val_accuracy: 0.9219\n",
      "Epoch 47/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5377 - accuracy: 0.9429 - val_loss: 0.8392 - val_accuracy: 0.9284\n",
      "Epoch 48/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5318 - accuracy: 0.9458 - val_loss: 0.8190 - val_accuracy: 0.9289\n",
      "Epoch 49/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5235 - accuracy: 0.9471 - val_loss: 0.7519 - val_accuracy: 0.9350\n",
      "Epoch 50/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5166 - accuracy: 0.9481 - val_loss: 0.7995 - val_accuracy: 0.9314\n",
      "Epoch 51/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5124 - accuracy: 0.9467 - val_loss: 0.7635 - val_accuracy: 0.9325\n",
      "Epoch 52/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5241 - accuracy: 0.9458 - val_loss: 0.8358 - val_accuracy: 0.9277\n",
      "Epoch 53/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5657 - accuracy: 0.9409 - val_loss: 0.8030 - val_accuracy: 0.9268\n",
      "Epoch 54/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4918 - accuracy: 0.9479 - val_loss: 0.7414 - val_accuracy: 0.9345\n",
      "Epoch 55/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5264 - accuracy: 0.9471 - val_loss: 0.7414 - val_accuracy: 0.9352\n",
      "Epoch 56/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4923 - accuracy: 0.9497 - val_loss: 0.7196 - val_accuracy: 0.9391\n",
      "Epoch 57/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.5347 - accuracy: 0.9468 - val_loss: 0.8398 - val_accuracy: 0.9280\n",
      "Epoch 58/1000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.4781 - accuracy: 0.9528 - val_loss: 0.6810 - val_accuracy: 0.9427\n",
      "Epoch 59/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4570 - accuracy: 0.9524 - val_loss: 0.7994 - val_accuracy: 0.9325\n",
      "Epoch 60/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4523 - accuracy: 0.9544 - val_loss: 0.7743 - val_accuracy: 0.9346\n",
      "Epoch 61/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4737 - accuracy: 0.9516 - val_loss: 0.7739 - val_accuracy: 0.9332\n",
      "Epoch 62/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4541 - accuracy: 0.9538 - val_loss: 0.7937 - val_accuracy: 0.9323\n",
      "Epoch 63/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4633 - accuracy: 0.9524 - val_loss: 0.7971 - val_accuracy: 0.9312\n",
      "Epoch 64/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4494 - accuracy: 0.9544 - val_loss: 0.8505 - val_accuracy: 0.9279\n",
      "Epoch 65/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4451 - accuracy: 0.9549 - val_loss: 0.7595 - val_accuracy: 0.9363\n",
      "Epoch 66/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4361 - accuracy: 0.9555 - val_loss: 0.6692 - val_accuracy: 0.9434\n",
      "Epoch 67/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4650 - accuracy: 0.9514 - val_loss: 0.7681 - val_accuracy: 0.9328\n",
      "Epoch 68/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4492 - accuracy: 0.9543 - val_loss: 0.8371 - val_accuracy: 0.9285\n",
      "Epoch 69/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4191 - accuracy: 0.9576 - val_loss: 0.7439 - val_accuracy: 0.9365\n",
      "Epoch 70/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4418 - accuracy: 0.9557 - val_loss: 1.0297 - val_accuracy: 0.9085\n",
      "Epoch 71/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4885 - accuracy: 0.9510 - val_loss: 0.5957 - val_accuracy: 0.9500\n",
      "Epoch 72/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4514 - accuracy: 0.9543 - val_loss: 0.8511 - val_accuracy: 0.9231\n",
      "Epoch 73/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4422 - accuracy: 0.9542 - val_loss: 0.5927 - val_accuracy: 0.9439\n",
      "Epoch 74/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4243 - accuracy: 0.9570 - val_loss: 0.6486 - val_accuracy: 0.9403\n",
      "Epoch 75/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4658 - accuracy: 0.9523 - val_loss: 0.7312 - val_accuracy: 0.9344\n",
      "Epoch 76/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4248 - accuracy: 0.9570 - val_loss: 0.6652 - val_accuracy: 0.9378\n",
      "Epoch 77/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3970 - accuracy: 0.9604 - val_loss: 0.7471 - val_accuracy: 0.9328\n",
      "Epoch 78/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4276 - accuracy: 0.9576 - val_loss: 0.6122 - val_accuracy: 0.9470\n",
      "Epoch 79/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3759 - accuracy: 0.9626 - val_loss: 0.6448 - val_accuracy: 0.9421\n",
      "Epoch 80/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4053 - accuracy: 0.9597 - val_loss: 0.5511 - val_accuracy: 0.9512\n",
      "Epoch 81/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3904 - accuracy: 0.9617 - val_loss: 0.6576 - val_accuracy: 0.9429\n",
      "Epoch 82/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4012 - accuracy: 0.9605 - val_loss: 0.5863 - val_accuracy: 0.9464\n",
      "Epoch 83/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4176 - accuracy: 0.9581 - val_loss: 0.7121 - val_accuracy: 0.9355\n",
      "Epoch 84/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3748 - accuracy: 0.9625 - val_loss: 0.5912 - val_accuracy: 0.9470\n",
      "Epoch 85/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3824 - accuracy: 0.9617 - val_loss: 0.6952 - val_accuracy: 0.9344\n",
      "Epoch 86/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4287 - accuracy: 0.9566 - val_loss: 0.7343 - val_accuracy: 0.9346\n",
      "Epoch 87/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3903 - accuracy: 0.9607 - val_loss: 0.7328 - val_accuracy: 0.9337\n",
      "Epoch 88/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3523 - accuracy: 0.9648 - val_loss: 0.7965 - val_accuracy: 0.9258\n",
      "Epoch 89/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3896 - accuracy: 0.9609 - val_loss: 0.8391 - val_accuracy: 0.9241\n",
      "Epoch 90/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3605 - accuracy: 0.9644 - val_loss: 0.6618 - val_accuracy: 0.9418\n",
      "Epoch 91/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3827 - accuracy: 0.9617 - val_loss: 0.8282 - val_accuracy: 0.9257\n",
      "Epoch 92/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3957 - accuracy: 0.9595 - val_loss: 0.8124 - val_accuracy: 0.9264\n",
      "Epoch 93/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3644 - accuracy: 0.9629 - val_loss: 0.7401 - val_accuracy: 0.9328\n",
      "Epoch 94/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4668 - accuracy: 0.9539 - val_loss: 0.8830 - val_accuracy: 0.9208\n",
      "Epoch 95/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3841 - accuracy: 0.9620 - val_loss: 0.7383 - val_accuracy: 0.9332\n",
      "Epoch 96/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3925 - accuracy: 0.9615 - val_loss: 0.8461 - val_accuracy: 0.9234\n",
      "Epoch 97/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3493 - accuracy: 0.9655 - val_loss: 0.6664 - val_accuracy: 0.9397\n",
      "Epoch 98/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3696 - accuracy: 0.9627 - val_loss: 0.6781 - val_accuracy: 0.9354\n",
      "Epoch 99/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3871 - accuracy: 0.9613 - val_loss: 0.7872 - val_accuracy: 0.9251\n",
      "Epoch 100/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3623 - accuracy: 0.9653 - val_loss: 0.7072 - val_accuracy: 0.9364\n",
      "Epoch 101/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3513 - accuracy: 0.9652 - val_loss: 0.7176 - val_accuracy: 0.9317\n",
      "Epoch 102/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.4167 - accuracy: 0.9590 - val_loss: 0.7523 - val_accuracy: 0.9292\n",
      "Epoch 103/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3331 - accuracy: 0.9670 - val_loss: 0.7233 - val_accuracy: 0.9324\n",
      "Epoch 104/1000\n",
      "11/11 [==============================] - 0s 7ms/step - loss: 0.3746 - accuracy: 0.9638 - val_loss: 0.7924 - val_accuracy: 0.9221\n",
      "Epoch 105/1000\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 0.3553 - accuracy: 0.9644 - val_loss: 0.5837 - val_accuracy: 0.9458\n",
      "====== Final_model =======\n",
      "train loss, train accuracy\n",
      "84/84 - 0s - loss: 0.5850 - accuracy: 0.9464 - 86ms/epoch - 1ms/step\n",
      "validation loss, validation accuracy\n",
      "21/21 - 0s - loss: 0.5520 - accuracy: 0.9479 - 35ms/epoch - 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# method_custom_metric 구현\n",
    "def accuracy(y_true, y_pred):\n",
    "    return 1 - tf.abs((y_true - y_pred) / y_true) \n",
    "\n",
    "# 모델 구현\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='elu', input_dim=X_train.shape[1]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation='elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))              \n",
    "model.add(Dense(16, activation='elu'))\n",
    "model.add(Dense(16, activation='elu'))\n",
    "model.add(Dense(1, activation='elu'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='mae', optimizer=optimizer , metrics=[accuracy])\n",
    "\n",
    "# early stopping 구현 - 커스텀 정확도 기준\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=25)\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=256, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "print(\"====== Final_model =======\")\n",
    "print(\"train loss, train accuracy\")\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose =2)\n",
    "print(\"validation loss, validation accuracy\")\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_opt2(n):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=n)\n",
    "    return opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "112/112 [==============================] - 2s 5ms/step - loss: 1.5915 - accuracy: 0.8310 - val_loss: 5.1605 - val_accuracy: 0.4581\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.9704 - accuracy: 0.8976 - val_loss: 1.7569 - val_accuracy: 0.7938\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8573 - accuracy: 0.9097 - val_loss: 0.4420 - val_accuracy: 0.9365\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8827 - accuracy: 0.9083 - val_loss: 1.1163 - val_accuracy: 0.8850\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7742 - accuracy: 0.9192 - val_loss: 0.6580 - val_accuracy: 0.9161\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7941 - accuracy: 0.9170 - val_loss: 0.3241 - val_accuracy: 0.9658\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7809 - accuracy: 0.9195 - val_loss: 0.3024 - val_accuracy: 0.9669\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7628 - accuracy: 0.9196 - val_loss: 1.3817 - val_accuracy: 0.8608\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7196 - accuracy: 0.9253 - val_loss: 1.0544 - val_accuracy: 0.8858\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7772 - accuracy: 0.9189 - val_loss: 1.3026 - val_accuracy: 0.8647\n",
      "Epoch 11/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6785 - accuracy: 0.9276 - val_loss: 0.2261 - val_accuracy: 0.9684\n",
      "Epoch 12/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7932 - accuracy: 0.9169 - val_loss: 0.6595 - val_accuracy: 0.9401\n",
      "Epoch 13/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6533 - accuracy: 0.9319 - val_loss: 1.1387 - val_accuracy: 0.8930\n",
      "Epoch 14/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6703 - accuracy: 0.9301 - val_loss: 0.4266 - val_accuracy: 0.9523\n",
      "Epoch 15/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6736 - accuracy: 0.9287 - val_loss: 0.5038 - val_accuracy: 0.9492\n",
      "Epoch 16/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6842 - accuracy: 0.9274 - val_loss: 0.3078 - val_accuracy: 0.9684\n",
      "Epoch 17/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6846 - accuracy: 0.9289 - val_loss: 1.1081 - val_accuracy: 0.8871\n",
      "Epoch 18/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6557 - accuracy: 0.9324 - val_loss: 0.7547 - val_accuracy: 0.9331\n",
      "Epoch 19/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7006 - accuracy: 0.9289 - val_loss: 0.8397 - val_accuracy: 0.9229\n",
      "Epoch 20/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6222 - accuracy: 0.9363 - val_loss: 0.7622 - val_accuracy: 0.9275\n",
      "Epoch 21/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6833 - accuracy: 0.9307 - val_loss: 0.2980 - val_accuracy: 0.9709\n",
      "Epoch 22/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5961 - accuracy: 0.9383 - val_loss: 0.5454 - val_accuracy: 0.9429\n",
      "Epoch 23/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6527 - accuracy: 0.9340 - val_loss: 0.2501 - val_accuracy: 0.9717\n",
      "Epoch 24/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5700 - accuracy: 0.9405 - val_loss: 0.5087 - val_accuracy: 0.9424\n",
      "Epoch 25/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6502 - accuracy: 0.9324 - val_loss: 0.6492 - val_accuracy: 0.9380\n",
      "Epoch 26/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6127 - accuracy: 0.9379 - val_loss: 0.6593 - val_accuracy: 0.9370\n",
      "Epoch 27/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6224 - accuracy: 0.9367 - val_loss: 0.2697 - val_accuracy: 0.9699\n",
      "Epoch 28/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6009 - accuracy: 0.9381 - val_loss: 0.3806 - val_accuracy: 0.9589\n",
      "Epoch 29/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5885 - accuracy: 0.9407 - val_loss: 0.4052 - val_accuracy: 0.9580\n",
      "Epoch 30/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5876 - accuracy: 0.9390 - val_loss: 0.2956 - val_accuracy: 0.9712\n",
      "Epoch 31/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5606 - accuracy: 0.9425 - val_loss: 0.3228 - val_accuracy: 0.9716\n",
      "Epoch 32/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.9451 - val_loss: 0.2285 - val_accuracy: 0.9741\n",
      "Epoch 33/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5662 - accuracy: 0.9428 - val_loss: 0.1600 - val_accuracy: 0.9821\n",
      "Epoch 34/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5432 - accuracy: 0.9425 - val_loss: 0.2684 - val_accuracy: 0.9630\n",
      "Epoch 35/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5583 - accuracy: 0.9420 - val_loss: 0.5361 - val_accuracy: 0.9365\n",
      "Epoch 36/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5459 - accuracy: 0.9432 - val_loss: 0.1890 - val_accuracy: 0.9769\n",
      "Epoch 37/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6157 - accuracy: 0.9382 - val_loss: 0.3049 - val_accuracy: 0.9683\n",
      "Epoch 38/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6141 - accuracy: 0.9398 - val_loss: 0.3471 - val_accuracy: 0.9662\n",
      "Epoch 39/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5448 - accuracy: 0.9463 - val_loss: 0.2258 - val_accuracy: 0.9737\n",
      "Epoch 40/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5636 - accuracy: 0.9441 - val_loss: 0.3288 - val_accuracy: 0.9629\n",
      "Epoch 41/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.4773 - accuracy: 0.9528 - val_loss: 0.2812 - val_accuracy: 0.9682\n",
      "Epoch 42/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5250 - accuracy: 0.9480 - val_loss: 0.1869 - val_accuracy: 0.9779\n",
      "Epoch 43/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5778 - accuracy: 0.9436 - val_loss: 0.4099 - val_accuracy: 0.9558\n",
      "0.01\n",
      "====== Final_model =======\n",
      "train loss, train accuracy\n",
      "84/84 - 0s - loss: 0.3807 - accuracy: 0.9592 - 106ms/epoch - 1ms/step\n",
      "validation loss, validation accuracy\n",
      "21/21 - 0s - loss: 0.4099 - accuracy: 0.9554 - 42ms/epoch - 2ms/step\n",
      "Epoch 1/1000\n",
      "112/112 [==============================] - 3s 5ms/step - loss: 1.8654 - accuracy: 0.8182 - val_loss: 6.1057 - val_accuracy: 0.3766\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8832 - accuracy: 0.9082 - val_loss: 1.4064 - val_accuracy: 0.8291\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.8599 - accuracy: 0.9077 - val_loss: 0.6170 - val_accuracy: 0.9290\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8316 - accuracy: 0.9125 - val_loss: 1.4345 - val_accuracy: 0.8448\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8378 - accuracy: 0.9117 - val_loss: 1.7015 - val_accuracy: 0.8490\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8289 - accuracy: 0.9153 - val_loss: 0.2390 - val_accuracy: 0.9719\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8194 - accuracy: 0.9152 - val_loss: 0.7003 - val_accuracy: 0.9376\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7944 - accuracy: 0.9162 - val_loss: 0.4862 - val_accuracy: 0.9539\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7600 - accuracy: 0.9206 - val_loss: 0.6191 - val_accuracy: 0.9451\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7483 - accuracy: 0.9222 - val_loss: 1.5075 - val_accuracy: 0.8656\n",
      "Epoch 11/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7439 - accuracy: 0.9226 - val_loss: 0.4737 - val_accuracy: 0.9481\n",
      "Epoch 12/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7374 - accuracy: 0.9223 - val_loss: 0.3565 - val_accuracy: 0.9653\n",
      "Epoch 13/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6961 - accuracy: 0.9281 - val_loss: 0.4207 - val_accuracy: 0.9575\n",
      "Epoch 14/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.9301 - val_loss: 0.4750 - val_accuracy: 0.9593\n",
      "Epoch 15/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7241 - accuracy: 0.9274 - val_loss: 0.4679 - val_accuracy: 0.9530\n",
      "Epoch 16/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6695 - accuracy: 0.9321 - val_loss: 0.8745 - val_accuracy: 0.9241\n",
      "0.009\n",
      "====== Final_model =======\n",
      "train loss, train accuracy\n",
      "84/84 - 0s - loss: 0.8952 - accuracy: 0.9223 - 114ms/epoch - 1ms/step\n",
      "validation loss, validation accuracy\n",
      "21/21 - 0s - loss: 0.8267 - accuracy: 0.9267 - 43ms/epoch - 2ms/step\n",
      "Epoch 1/1000\n",
      "112/112 [==============================] - 2s 5ms/step - loss: 1.7309 - accuracy: 0.8216 - val_loss: 2.7568 - val_accuracy: 0.6738\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8769 - accuracy: 0.9050 - val_loss: 1.0996 - val_accuracy: 0.8555\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8633 - accuracy: 0.9090 - val_loss: 0.5078 - val_accuracy: 0.9413\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8532 - accuracy: 0.9122 - val_loss: 0.1894 - val_accuracy: 0.9760\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7843 - accuracy: 0.9160 - val_loss: 1.3085 - val_accuracy: 0.8761\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8445 - accuracy: 0.9110 - val_loss: 0.3065 - val_accuracy: 0.9626\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8205 - accuracy: 0.9122 - val_loss: 2.1085 - val_accuracy: 0.8063\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7795 - accuracy: 0.9181 - val_loss: 0.2795 - val_accuracy: 0.9688\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7439 - accuracy: 0.9216 - val_loss: 1.1239 - val_accuracy: 0.8885\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7261 - accuracy: 0.9236 - val_loss: 2.0632 - val_accuracy: 0.8223\n",
      "Epoch 11/1000\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.7580 - accuracy: 0.9180 - val_loss: 0.2793 - val_accuracy: 0.9695\n",
      "Epoch 12/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7624 - accuracy: 0.9183 - val_loss: 0.4247 - val_accuracy: 0.9631\n",
      "Epoch 13/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6209 - accuracy: 0.9331 - val_loss: 0.6449 - val_accuracy: 0.9234\n",
      "Epoch 14/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6730 - accuracy: 0.9290 - val_loss: 0.2311 - val_accuracy: 0.9737\n",
      "0.006\n",
      "====== Final_model =======\n",
      "train loss, train accuracy\n",
      "84/84 - 0s - loss: 0.2326 - accuracy: 0.9724 - 112ms/epoch - 1ms/step\n",
      "validation loss, validation accuracy\n",
      "21/21 - 0s - loss: 0.2266 - accuracy: 0.9744 - 42ms/epoch - 2ms/step\n",
      "Epoch 1/1000\n",
      "112/112 [==============================] - 2s 5ms/step - loss: 1.7903 - accuracy: 0.8062 - val_loss: 2.4721 - val_accuracy: 0.7003\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.9916 - accuracy: 0.8947 - val_loss: 1.3577 - val_accuracy: 0.8324\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8778 - accuracy: 0.9071 - val_loss: 0.6727 - val_accuracy: 0.9276\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8212 - accuracy: 0.9148 - val_loss: 0.3568 - val_accuracy: 0.9573\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8650 - accuracy: 0.9083 - val_loss: 1.0224 - val_accuracy: 0.9131\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7569 - accuracy: 0.9197 - val_loss: 0.3884 - val_accuracy: 0.9547\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7721 - accuracy: 0.9194 - val_loss: 1.4522 - val_accuracy: 0.8708\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8147 - accuracy: 0.9129 - val_loss: 0.4974 - val_accuracy: 0.9535\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7375 - accuracy: 0.9244 - val_loss: 0.7673 - val_accuracy: 0.9281\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7632 - accuracy: 0.9193 - val_loss: 0.9065 - val_accuracy: 0.9141\n",
      "Epoch 11/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7026 - accuracy: 0.9263 - val_loss: 0.4964 - val_accuracy: 0.9461\n",
      "Epoch 12/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6838 - accuracy: 0.9283 - val_loss: 0.9352 - val_accuracy: 0.9104\n",
      "Epoch 13/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7578 - accuracy: 0.9201 - val_loss: 0.4215 - val_accuracy: 0.9511\n",
      "Epoch 14/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7024 - accuracy: 0.9252 - val_loss: 0.3690 - val_accuracy: 0.9626\n",
      "Epoch 15/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7124 - accuracy: 0.9244 - val_loss: 0.6720 - val_accuracy: 0.9397\n",
      "Epoch 16/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6442 - accuracy: 0.9302 - val_loss: 0.5407 - val_accuracy: 0.9538\n",
      "Epoch 17/1000\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.6844 - accuracy: 0.9276 - val_loss: 0.5623 - val_accuracy: 0.9461\n",
      "Epoch 18/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6459 - accuracy: 0.9316 - val_loss: 0.3019 - val_accuracy: 0.9684\n",
      "Epoch 19/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6404 - accuracy: 0.9333 - val_loss: 0.3095 - val_accuracy: 0.9666\n",
      "Epoch 20/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5965 - accuracy: 0.9371 - val_loss: 0.2621 - val_accuracy: 0.9695\n",
      "Epoch 21/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6614 - accuracy: 0.9285 - val_loss: 0.4249 - val_accuracy: 0.9616\n",
      "Epoch 22/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6206 - accuracy: 0.9344 - val_loss: 0.4005 - val_accuracy: 0.9636\n",
      "Epoch 23/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6798 - accuracy: 0.9279 - val_loss: 0.2481 - val_accuracy: 0.9784\n",
      "Epoch 24/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5834 - accuracy: 0.9377 - val_loss: 0.2809 - val_accuracy: 0.9651\n",
      "Epoch 25/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6227 - accuracy: 0.9330 - val_loss: 0.2597 - val_accuracy: 0.9752\n",
      "Epoch 26/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6065 - accuracy: 0.9362 - val_loss: 0.9889 - val_accuracy: 0.9109\n",
      "Epoch 27/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5664 - accuracy: 0.9411 - val_loss: 0.7598 - val_accuracy: 0.9237\n",
      "Epoch 28/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6454 - accuracy: 0.9307 - val_loss: 0.2356 - val_accuracy: 0.9752\n",
      "Epoch 29/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5636 - accuracy: 0.9405 - val_loss: 0.3749 - val_accuracy: 0.9601\n",
      "Epoch 30/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5806 - accuracy: 0.9370 - val_loss: 0.3627 - val_accuracy: 0.9649\n",
      "Epoch 31/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6094 - accuracy: 0.9349 - val_loss: 0.2272 - val_accuracy: 0.9662\n",
      "Epoch 32/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6623 - accuracy: 0.9314 - val_loss: 0.3205 - val_accuracy: 0.9679\n",
      "Epoch 33/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5456 - accuracy: 0.9419 - val_loss: 0.1697 - val_accuracy: 0.9797\n",
      "Epoch 34/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5552 - accuracy: 0.9411 - val_loss: 0.9691 - val_accuracy: 0.9082\n",
      "Epoch 35/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5807 - accuracy: 0.9386 - val_loss: 0.3262 - val_accuracy: 0.9685\n",
      "Epoch 36/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6459 - accuracy: 0.9328 - val_loss: 0.2506 - val_accuracy: 0.9703\n",
      "Epoch 37/1000\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.5727 - accuracy: 0.9391 - val_loss: 0.3243 - val_accuracy: 0.9723\n",
      "Epoch 38/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5732 - accuracy: 0.9395 - val_loss: 0.4271 - val_accuracy: 0.9626\n",
      "Epoch 39/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6756 - accuracy: 0.9310 - val_loss: 0.2822 - val_accuracy: 0.9721\n",
      "Epoch 40/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5468 - accuracy: 0.9435 - val_loss: 0.5368 - val_accuracy: 0.9507\n",
      "Epoch 41/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5669 - accuracy: 0.9402 - val_loss: 0.2535 - val_accuracy: 0.9729\n",
      "Epoch 42/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5745 - accuracy: 0.9397 - val_loss: 0.3117 - val_accuracy: 0.9680\n",
      "Epoch 43/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5709 - accuracy: 0.9394 - val_loss: 0.2123 - val_accuracy: 0.9772\n",
      "0.003\n",
      "====== Final_model =======\n",
      "train loss, train accuracy\n",
      "84/84 - 0s - loss: 0.2188 - accuracy: 0.9762 - 113ms/epoch - 1ms/step\n",
      "validation loss, validation accuracy\n",
      "21/21 - 0s - loss: 0.2166 - accuracy: 0.9762 - 43ms/epoch - 2ms/step\n",
      "Epoch 1/1000\n",
      "112/112 [==============================] - 2s 5ms/step - loss: 6.5690 - accuracy: 0.3426 - val_loss: 4.0868 - val_accuracy: 0.5485\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 1.2081 - accuracy: 0.8697 - val_loss: 1.3935 - val_accuracy: 0.8290\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.9483 - accuracy: 0.8985 - val_loss: 0.7419 - val_accuracy: 0.9071\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.9240 - accuracy: 0.9030 - val_loss: 0.5968 - val_accuracy: 0.9248\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.9436 - accuracy: 0.8956 - val_loss: 0.3190 - val_accuracy: 0.9607\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.9219 - accuracy: 0.9032 - val_loss: 0.5961 - val_accuracy: 0.9424\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8226 - accuracy: 0.9137 - val_loss: 0.4604 - val_accuracy: 0.9415\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8083 - accuracy: 0.9134 - val_loss: 0.6937 - val_accuracy: 0.9352\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8050 - accuracy: 0.9147 - val_loss: 0.4011 - val_accuracy: 0.9637\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7778 - accuracy: 0.9187 - val_loss: 0.5455 - val_accuracy: 0.9436\n",
      "Epoch 11/1000\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.7087 - accuracy: 0.9246 - val_loss: 0.2590 - val_accuracy: 0.9778\n",
      "Epoch 12/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6938 - accuracy: 0.9264 - val_loss: 0.3205 - val_accuracy: 0.9671\n",
      "Epoch 13/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7388 - accuracy: 0.9214 - val_loss: 0.4287 - val_accuracy: 0.9614\n",
      "Epoch 14/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7150 - accuracy: 0.9241 - val_loss: 0.3327 - val_accuracy: 0.9651\n",
      "Epoch 15/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6721 - accuracy: 0.9274 - val_loss: 0.1838 - val_accuracy: 0.9793\n",
      "Epoch 16/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6668 - accuracy: 0.9303 - val_loss: 0.1740 - val_accuracy: 0.9831\n",
      "Epoch 17/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6740 - accuracy: 0.9282 - val_loss: 0.5130 - val_accuracy: 0.9481\n",
      "Epoch 18/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6670 - accuracy: 0.9281 - val_loss: 0.3669 - val_accuracy: 0.9624\n",
      "Epoch 19/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6182 - accuracy: 0.9342 - val_loss: 0.4039 - val_accuracy: 0.9592\n",
      "Epoch 20/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7192 - accuracy: 0.9227 - val_loss: 0.2822 - val_accuracy: 0.9725\n",
      "Epoch 21/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6169 - accuracy: 0.9346 - val_loss: 0.2472 - val_accuracy: 0.9707\n",
      "Epoch 22/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6369 - accuracy: 0.9322 - val_loss: 0.2193 - val_accuracy: 0.9738\n",
      "Epoch 23/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6733 - accuracy: 0.9263 - val_loss: 0.3592 - val_accuracy: 0.9662\n",
      "Epoch 24/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6133 - accuracy: 0.9339 - val_loss: 0.1726 - val_accuracy: 0.9806\n",
      "Epoch 25/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.6055 - accuracy: 0.9363 - val_loss: 0.2243 - val_accuracy: 0.9720\n",
      "Epoch 26/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.5847 - accuracy: 0.9380 - val_loss: 0.3923 - val_accuracy: 0.9579\n",
      "0.001\n",
      "====== Final_model =======\n",
      "train loss, train accuracy\n",
      "84/84 - 0s - loss: 0.3973 - accuracy: 0.9574 - 112ms/epoch - 1ms/step\n",
      "validation loss, validation accuracy\n",
      "21/21 - 0s - loss: 0.3935 - accuracy: 0.9576 - 43ms/epoch - 2ms/step\n",
      "Epoch 1/1000\n",
      "112/112 [==============================] - 2s 5ms/step - loss: 4.8700 - accuracy: 0.5212 - val_loss: 1.6769 - val_accuracy: 0.8050\n",
      "Epoch 2/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 1.2788 - accuracy: 0.8604 - val_loss: 1.4098 - val_accuracy: 0.8142\n",
      "Epoch 3/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 1.1685 - accuracy: 0.8711 - val_loss: 0.7106 - val_accuracy: 0.9199\n",
      "Epoch 4/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 1.0337 - accuracy: 0.8877 - val_loss: 0.3685 - val_accuracy: 0.9526\n",
      "Epoch 5/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 1.0170 - accuracy: 0.8874 - val_loss: 0.2715 - val_accuracy: 0.9643\n",
      "Epoch 6/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.9948 - accuracy: 0.8934 - val_loss: 0.2470 - val_accuracy: 0.9720\n",
      "Epoch 7/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.9986 - accuracy: 0.8921 - val_loss: 0.2155 - val_accuracy: 0.9750\n",
      "Epoch 8/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.9251 - accuracy: 0.8995 - val_loss: 0.1869 - val_accuracy: 0.9784\n",
      "Epoch 9/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.9318 - accuracy: 0.8997 - val_loss: 0.5121 - val_accuracy: 0.9424\n",
      "Epoch 10/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.9529 - accuracy: 0.8974 - val_loss: 0.3717 - val_accuracy: 0.9661\n",
      "Epoch 11/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8781 - accuracy: 0.9073 - val_loss: 0.6414 - val_accuracy: 0.9377\n",
      "Epoch 12/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8336 - accuracy: 0.9125 - val_loss: 0.1836 - val_accuracy: 0.9810\n",
      "Epoch 13/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8412 - accuracy: 0.9109 - val_loss: 0.3029 - val_accuracy: 0.9705\n",
      "Epoch 14/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8289 - accuracy: 0.9121 - val_loss: 0.2506 - val_accuracy: 0.9745\n",
      "Epoch 15/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.8014 - accuracy: 0.9139 - val_loss: 0.2983 - val_accuracy: 0.9607\n",
      "Epoch 16/1000\n",
      "112/112 [==============================] - 0s 4ms/step - loss: 0.7779 - accuracy: 0.9160 - val_loss: 0.5025 - val_accuracy: 0.9527\n",
      "Epoch 17/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7753 - accuracy: 0.9185 - val_loss: 0.2084 - val_accuracy: 0.9746\n",
      "Epoch 18/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7873 - accuracy: 0.9180 - val_loss: 0.2526 - val_accuracy: 0.9742\n",
      "Epoch 19/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7829 - accuracy: 0.9174 - val_loss: 0.2250 - val_accuracy: 0.9752\n",
      "Epoch 20/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7314 - accuracy: 0.9217 - val_loss: 0.2750 - val_accuracy: 0.9752\n",
      "Epoch 21/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7559 - accuracy: 0.9191 - val_loss: 0.2513 - val_accuracy: 0.9741\n",
      "Epoch 22/1000\n",
      "112/112 [==============================] - 0s 3ms/step - loss: 0.7332 - accuracy: 0.9224 - val_loss: 0.2345 - val_accuracy: 0.9774\n",
      "0.0005\n",
      "====== Final_model =======\n",
      "train loss, train accuracy\n",
      "84/84 - 0s - loss: 0.2413 - accuracy: 0.9767 - 110ms/epoch - 1ms/step\n",
      "validation loss, validation accuracy\n",
      "21/21 - 0s - loss: 0.2231 - accuracy: 0.9785 - 42ms/epoch - 2ms/step\n"
     ]
    }
   ],
   "source": [
    "func = 'elu'\n",
    "opti = 'adam'\n",
    "batch = 256\n",
    "for i in [0.01, 0.009, 0.006, 0.003, 0.001, 0.0005]:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, activation=func, input_dim=X_train.shape[1]))\n",
    "    model.add(BatchNormalization())  # BatchNormalization 추가\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation=func))\n",
    "    model.add(BatchNormalization())  # BatchNormalization 추가\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(64, activation=func))\n",
    "    model.add(BatchNormalization())  # BatchNormalization 추가\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(32, activation=func))\n",
    "    model.add(Dense(16, activation=func))\n",
    "    model.add(Dense(8, activation=func))\n",
    "    model.add(Dense(8, activation=func))\n",
    "    model.add(Dense(1, activation=func))\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(loss='mae', optimizer=custom_opt2(i) , metrics=[accuracy])\n",
    "\n",
    "    # early stopping 구현 - 커스텀 정확도 기준\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=10)\n",
    "    model.fit(X_train, y_train, epochs=1000, batch_size=24, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "    print(i)\n",
    "\n",
    "    print(\"====== Final_model =======\")\n",
    "    print(\"train loss, train accuracy\")\n",
    "    train_loss, train_acc = model.evaluate(X_train, y_train, verbose =2)\n",
    "    print(\"validation loss, validation accuracy\")\n",
    "    val_loss, val_acc = model.evaluate(X_val, y_val, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
