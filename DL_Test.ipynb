{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Regression_data_preprocessing.csv')\n",
    "target = 'Rings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[target]\n",
    "x = df.drop(target, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x= scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.57455813 -0.43214879 -1.1529056  ... -0.67483383 -0.68801788\n",
      "   1.31667716]\n",
      " [-1.44898585 -1.439929   -1.28321426 ... -0.67483383 -0.68801788\n",
      "   1.31667716]\n",
      " [ 0.05003309  0.12213032 -0.11043635 ...  1.48184628 -0.68801788\n",
      "  -0.75948762]\n",
      " ...\n",
      " [ 0.6329849   0.67640943  1.71388483 ... -0.67483383 -0.68801788\n",
      "   1.31667716]\n",
      " [ 0.84118198  0.77718745  0.28048962 ...  1.48184628 -0.68801788\n",
      "  -0.75948762]\n",
      " [ 1.54905203  1.48263359  1.45326752 ... -0.67483383 -0.68801788\n",
      "   1.31667716]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정확도 계산\n",
    "def eval_accuracy(y, y_hat):\n",
    "\t# 오차율 구하는 과정    \n",
    "\t# np.mean() 메서드의 이유는 미니배치 처리를 고려하여 하나의 지표로 묶어주기 위함 입니다. \n",
    "    mdiff = np.mean(np.abs((y_hat - y) / y))\n",
    "    # 1 에서 오차율을 빼 정확도를 구합니다. \n",
    "    return 1 - mdiff"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. SGD (Stochastic Gradient Descent):\n",
    "가장 기본적인 옵티마이저로, 경사 하강법의 확률적인 버전입니다.\n",
    "각 학습 단계에서 미니 배치(mini-batch) 단위로 데이터를 사용하여 가중치를 업데이트합니다.\n",
    "단순하고 직관적인 방법이지만, 수렴 속도가 느리고 지역 최소값(local minimum)에 빠질 가능성이 있습니다.\n",
    "\n",
    "# 2. Adam (Adaptive Moment Estimation):\n",
    "학습률(learning rate)을 조정하는 방법을 통해 경사 하강법을 개선한 알고리즘입니다.\n",
    "학습 속도를 개선하기 위해 모멘텀(Momentum)과 학습률 스케줄링(learning rate scheduling)을 조합합니다.\n",
    "이동 평균(moving average)을 사용하여 각 가중치의 업데이트 속도를 조절하며, 자동으로 적응적인 학습률을 제공합니다.\n",
    "다양한 유형의 신경망 구조와 데이터에 대해 일반적으로 좋은 성능을 보입니다.\n",
    "\n",
    "# 3. RMSProp (Root Mean Square Propagation):\n",
    "과거 그래디언트(gradient)의 제곱을 이동 평균하여 학습률을 조정하는 알고리즘입니다.\n",
    "최근 그래디언트에 더 큰 가중치를 부여하여 중요한 그래디언트를 잘 반영합니다.\n",
    "이동 평균을 사용하여 각 가중치의 업데이트 속도를 조절하며, 최적의 학습률을 자동으로 조정합니다.\n",
    "비교적 안정적인 학습을 제공하고, RNN(Recurrent Neural Network)과 같은 모델에서 잘 작동하는 경향이 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 104.5835 - val_loss: 74.9382\n",
      "Epoch 2/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 30.3519 - val_loss: 7.4524\n",
      "Epoch 3/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 19.2211 - val_loss: 8.0033\n",
      "Epoch 4/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 17.9167 - val_loss: 6.1202\n",
      "Epoch 5/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 16.5371 - val_loss: 7.0418\n",
      "Epoch 6/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 17.2238 - val_loss: 5.9681\n",
      "Epoch 7/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 15.7440 - val_loss: 6.0806\n",
      "Epoch 8/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 14.9112 - val_loss: 6.6912\n",
      "Epoch 9/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 15.2202 - val_loss: 6.2042\n",
      "Epoch 10/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 13.4641 - val_loss: 6.6828\n",
      "Epoch 11/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 13.4808 - val_loss: 6.4898\n",
      "Epoch 1/1000\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 44.8917 - val_loss: 9.6477\n",
      "Epoch 2/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 6.5182 - val_loss: 5.0619\n",
      "Epoch 3/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.0093 - val_loss: 5.0025\n",
      "Epoch 4/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.8727 - val_loss: 5.0290\n",
      "Epoch 5/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.8005 - val_loss: 4.8673\n",
      "Epoch 6/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.7360 - val_loss: 4.8208\n",
      "Epoch 7/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6567 - val_loss: 4.7539\n",
      "Epoch 8/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6179 - val_loss: 4.7114\n",
      "Epoch 9/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5916 - val_loss: 4.6719\n",
      "Epoch 10/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5713 - val_loss: 4.8263\n",
      "Epoch 11/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5635 - val_loss: 4.6836\n",
      "Epoch 12/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5357 - val_loss: 4.6144\n",
      "Epoch 13/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5182 - val_loss: 4.6802\n",
      "Epoch 14/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5440 - val_loss: 4.6195\n",
      "Epoch 15/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4650 - val_loss: 4.5904\n",
      "Epoch 16/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4437 - val_loss: 4.8258\n",
      "Epoch 17/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4851 - val_loss: 4.7009\n",
      "Epoch 18/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4658 - val_loss: 4.7211\n",
      "Epoch 19/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4546 - val_loss: 4.5864\n",
      "Epoch 20/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4039 - val_loss: 4.5675\n",
      "Epoch 21/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4066 - val_loss: 4.6462\n",
      "Epoch 22/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4850 - val_loss: 4.5660\n",
      "Epoch 23/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3903 - val_loss: 4.5919\n",
      "Epoch 24/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3755 - val_loss: 4.5379\n",
      "Epoch 25/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3752 - val_loss: 4.4898\n",
      "Epoch 26/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3420 - val_loss: 4.7943\n",
      "Epoch 27/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3304 - val_loss: 4.4561\n",
      "Epoch 28/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3289 - val_loss: 4.4851\n",
      "Epoch 29/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3787 - val_loss: 4.4880\n",
      "Epoch 30/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3598 - val_loss: 4.5955\n",
      "Epoch 31/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3666 - val_loss: 4.5262\n",
      "Epoch 32/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3457 - val_loss: 4.6747\n",
      "Epoch 1/1000\n",
      "209/209 [==============================] - 2s 2ms/step - loss: 94.4521 - val_loss: 82.0787\n",
      "Epoch 2/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 74.8250 - val_loss: 67.7750\n",
      "Epoch 3/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 62.7335 - val_loss: 57.6100\n",
      "Epoch 4/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 53.6238 - val_loss: 49.5078\n",
      "Epoch 5/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 46.1667 - val_loss: 42.7466\n",
      "Epoch 6/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 39.8726 - val_loss: 37.0019\n",
      "Epoch 7/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 34.4847 - val_loss: 32.0645\n",
      "Epoch 8/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 29.7926 - val_loss: 27.6579\n",
      "Epoch 9/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 25.0060 - val_loss: 22.1843\n",
      "Epoch 10/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 19.4040 - val_loss: 17.4920\n",
      "Epoch 11/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 15.7553 - val_loss: 14.8001\n",
      "Epoch 12/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 13.5763 - val_loss: 13.1576\n",
      "Epoch 13/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 12.2179 - val_loss: 12.1419\n",
      "Epoch 14/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 11.3802 - val_loss: 11.5234\n",
      "Epoch 15/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.8790 - val_loss: 11.1779\n",
      "Epoch 16/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.5883 - val_loss: 10.9849\n",
      "Epoch 17/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.4281 - val_loss: 10.8857\n",
      "Epoch 18/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.3449 - val_loss: 10.8333\n",
      "Epoch 19/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.2856 - val_loss: 10.7568\n",
      "Epoch 20/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 9.3693 - val_loss: 8.3755\n",
      "Epoch 21/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 7.7375 - val_loss: 7.7548\n",
      "Epoch 22/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 7.3142 - val_loss: 7.4870\n",
      "Epoch 23/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 7.0882 - val_loss: 7.2208\n",
      "Epoch 24/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 6.8700 - val_loss: 6.9689\n",
      "Epoch 25/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 6.6801 - val_loss: 6.7754\n",
      "Epoch 26/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 6.4953 - val_loss: 6.5655\n",
      "Epoch 27/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 6.2253 - val_loss: 6.3001\n",
      "Epoch 28/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 5.9503 - val_loss: 6.1327\n",
      "Epoch 29/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.7440 - val_loss: 5.9309\n",
      "Epoch 30/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 5.5786 - val_loss: 5.8034\n",
      "Epoch 31/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.4386 - val_loss: 5.7825\n",
      "Epoch 32/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.3348 - val_loss: 5.5902\n",
      "Epoch 33/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.2403 - val_loss: 5.5280\n",
      "Epoch 34/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.1473 - val_loss: 5.4069\n",
      "Epoch 35/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.0632 - val_loss: 5.3554\n",
      "Epoch 36/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.0044 - val_loss: 5.2936\n",
      "Epoch 37/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.9351 - val_loss: 5.2676\n",
      "Epoch 38/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.8778 - val_loss: 5.2160\n",
      "Epoch 39/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.8387 - val_loss: 5.2163\n",
      "Epoch 40/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.8089 - val_loss: 5.1899\n",
      "Epoch 41/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.7534 - val_loss: 5.0935\n",
      "Epoch 42/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.7081 - val_loss: 5.0963\n",
      "Epoch 43/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6866 - val_loss: 5.0281\n",
      "Epoch 44/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6579 - val_loss: 5.0041\n",
      "Epoch 45/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6235 - val_loss: 4.9445\n",
      "Epoch 46/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6166 - val_loss: 4.9197\n",
      "Epoch 47/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5933 - val_loss: 4.9312\n",
      "Epoch 48/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5735 - val_loss: 4.8890\n",
      "Epoch 49/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5837 - val_loss: 4.8678\n",
      "Epoch 50/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5521 - val_loss: 4.9053\n",
      "Epoch 51/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5523 - val_loss: 4.9403\n",
      "Epoch 52/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5238 - val_loss: 4.8640\n",
      "Epoch 53/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5057 - val_loss: 4.8825\n",
      "Epoch 54/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4911 - val_loss: 4.7875\n",
      "Epoch 55/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4813 - val_loss: 4.8323\n",
      "Epoch 56/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4698 - val_loss: 4.7729\n",
      "Epoch 57/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4551 - val_loss: 4.8561\n",
      "Epoch 58/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4735 - val_loss: 4.7324\n",
      "Epoch 59/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4424 - val_loss: 4.8037\n",
      "Epoch 60/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4556 - val_loss: 4.7376\n",
      "Epoch 61/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4342 - val_loss: 4.7190\n",
      "Epoch 62/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4341 - val_loss: 4.7186\n",
      "Epoch 63/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4357 - val_loss: 4.7143\n",
      "Epoch 64/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4233 - val_loss: 4.7007\n",
      "Epoch 65/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4026 - val_loss: 4.7327\n",
      "Epoch 66/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4099 - val_loss: 4.7405\n",
      "Epoch 67/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3983 - val_loss: 4.6924\n",
      "Epoch 68/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3989 - val_loss: 4.7378\n",
      "Epoch 69/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3658 - val_loss: 4.6644\n",
      "Epoch 70/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3875 - val_loss: 4.6584\n",
      "Epoch 71/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3717 - val_loss: 4.6958\n",
      "Epoch 72/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3653 - val_loss: 4.7161\n",
      "Epoch 73/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3601 - val_loss: 4.6861\n",
      "Epoch 74/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3624 - val_loss: 4.6411\n",
      "Epoch 75/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3473 - val_loss: 4.6976\n",
      "Epoch 76/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3554 - val_loss: 4.6113\n",
      "Epoch 77/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3553 - val_loss: 4.6042\n",
      "Epoch 78/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3466 - val_loss: 4.6435\n",
      "Epoch 79/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3575 - val_loss: 4.7187\n",
      "Epoch 80/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3445 - val_loss: 4.5924\n",
      "Epoch 81/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3185 - val_loss: 4.6185\n",
      "Epoch 82/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3193 - val_loss: 4.6133\n",
      "Epoch 83/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3193 - val_loss: 4.6299\n",
      "Epoch 84/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3040 - val_loss: 4.5925\n",
      "Epoch 85/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3175 - val_loss: 4.5671\n",
      "Epoch 86/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2938 - val_loss: 4.5676\n",
      "Epoch 87/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3174 - val_loss: 4.5715\n",
      "Epoch 88/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3126 - val_loss: 4.5609\n",
      "Epoch 89/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2945 - val_loss: 4.7406\n",
      "Epoch 90/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3335 - val_loss: 4.6088\n",
      "Epoch 91/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2939 - val_loss: 4.5831\n",
      "Epoch 92/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2979 - val_loss: 4.5425\n",
      "Epoch 93/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2835 - val_loss: 4.5516\n",
      "Epoch 94/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2739 - val_loss: 4.5481\n",
      "Epoch 95/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2848 - val_loss: 4.5971\n",
      "Epoch 96/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2624 - val_loss: 4.5850\n",
      "Epoch 97/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2740 - val_loss: 4.5185\n",
      "Epoch 98/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2764 - val_loss: 4.5386\n",
      "Epoch 99/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2703 - val_loss: 4.5618\n",
      "Epoch 100/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2692 - val_loss: 4.5215\n",
      "Epoch 101/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2492 - val_loss: 4.5508\n",
      "Epoch 102/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.2615 - val_loss: 4.5224\n",
      "105/105 [==============================] - 0s 712us/step\n",
      "105/105 [==============================] - 0s 692us/step\n",
      "105/105 [==============================] - 0s 692us/step\n",
      "27/27 [==============================] - 0s 769us/step\n",
      "27/27 [==============================] - 0s 731us/step\n",
      "27/27 [==============================] - 0s 732us/step\n",
      "--------------------------\n",
      "adam 16\n",
      "131/131 - 0s - loss: 6.3279 - 102ms/epoch - 782us/step\n",
      "131/131 - 0s - loss: 4.4686 - 102ms/epoch - 777us/step\n",
      "131/131 - 0s - loss: 4.2680 - 108ms/epoch - 824us/step\n",
      "131/131 - 0s - loss: 6.3279 - 104ms/epoch - 794us/step\n",
      "6.327945709228516\n",
      "131/131 - 0s - loss: 4.4686 - 117ms/epoch - 893us/step\n",
      "4.468606472015381\n",
      "131/131 - 0s - loss: 4.2680 - 137ms/epoch - 1ms/step\n",
      "4.267954349517822\n",
      "      Rings       pred      pred2      pred3\n",
      "1486      9  10.484786  12.130307  11.886122\n",
      "4163      7        NaN        NaN        NaN\n",
      "310      21  11.393665  15.262578  15.883675\n",
      "3886      7        NaN        NaN        NaN\n",
      "552      12  11.085990  13.008571  12.083145\n",
      "40        9   3.890536   3.768874   4.883097\n",
      "1240      6   7.780694   9.209800   9.112597\n",
      "4094     13        NaN        NaN        NaN\n",
      "4009      8        NaN        NaN        NaN\n",
      "1207     11   9.830989  11.627181  10.926258\n",
      "927       8   5.075276   5.687376   5.648869\n",
      "2685     10   9.673757  10.701029  10.422593\n",
      "1145      9  10.081898  12.785173  13.354122\n",
      "1410     10  10.655235  11.843078  11.803532\n",
      "1853      8   9.833433  12.142190  11.867817\n",
      "1016      8  10.195682  13.246543  13.670439\n",
      "2561      6   5.276284   6.049495   5.879917\n",
      "3252     12   6.047994   6.725982   6.694645\n",
      "1411     11   9.255498  10.842620   9.879866\n",
      "3646      9        NaN        NaN        NaN\n",
      "--------------------------\n",
      "Epoch 1/1000\n",
      "105/105 [==============================] - 1s 3ms/step - loss: 82.0555 - val_loss: 25.4974\n",
      "Epoch 2/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 26.6839 - val_loss: 9.8167\n",
      "Epoch 3/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 21.1369 - val_loss: 8.1001\n",
      "Epoch 4/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 19.5436 - val_loss: 6.9339\n",
      "Epoch 5/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 18.4260 - val_loss: 7.4680\n",
      "Epoch 6/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 19.3280 - val_loss: 6.9698\n",
      "Epoch 7/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 17.8902 - val_loss: 6.4289\n",
      "Epoch 8/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 17.9999 - val_loss: 6.8384\n",
      "Epoch 9/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 17.9280 - val_loss: 6.2608\n",
      "Epoch 10/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 18.6280 - val_loss: 6.7604\n",
      "Epoch 11/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 17.0163 - val_loss: 6.2767\n",
      "Epoch 12/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 17.7932 - val_loss: 6.6235\n",
      "Epoch 13/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 16.3922 - val_loss: 6.3456\n",
      "Epoch 14/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 16.4874 - val_loss: 5.8925\n",
      "Epoch 15/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 16.5627 - val_loss: 6.6988\n",
      "Epoch 16/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 16.0709 - val_loss: 7.1657\n",
      "Epoch 17/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 16.5330 - val_loss: 6.3848\n",
      "Epoch 18/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 16.0246 - val_loss: 6.1264\n",
      "Epoch 19/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 15.3191 - val_loss: 5.9863\n",
      "Epoch 1/1000\n",
      "105/105 [==============================] - 1s 3ms/step - loss: 84.4490 - val_loss: 30.1801\n",
      "Epoch 2/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 16.1714 - val_loss: 8.5145\n",
      "Epoch 3/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 6.8021 - val_loss: 5.7209\n",
      "Epoch 4/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.5096 - val_loss: 5.4081\n",
      "Epoch 5/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.1791 - val_loss: 5.2417\n",
      "Epoch 6/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.0174 - val_loss: 5.2840\n",
      "Epoch 7/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.8946 - val_loss: 5.0408\n",
      "Epoch 8/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.8227 - val_loss: 5.1720\n",
      "Epoch 9/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.7733 - val_loss: 4.9644\n",
      "Epoch 10/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.7396 - val_loss: 4.8641\n",
      "Epoch 11/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.6879 - val_loss: 4.8492\n",
      "Epoch 12/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.6549 - val_loss: 4.8321\n",
      "Epoch 13/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.6161 - val_loss: 4.7713\n",
      "Epoch 14/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.6154 - val_loss: 4.8582\n",
      "Epoch 15/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5863 - val_loss: 4.7606\n",
      "Epoch 16/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5788 - val_loss: 4.7796\n",
      "Epoch 17/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5252 - val_loss: 4.8116\n",
      "Epoch 18/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.5260 - val_loss: 4.7173\n",
      "Epoch 19/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.4975 - val_loss: 4.6812\n",
      "Epoch 20/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5188 - val_loss: 4.7744\n",
      "Epoch 21/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.5055 - val_loss: 4.6498\n",
      "Epoch 22/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.4611 - val_loss: 4.6504\n",
      "Epoch 23/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.4998 - val_loss: 4.6601\n",
      "Epoch 24/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4327 - val_loss: 4.6001\n",
      "Epoch 25/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4489 - val_loss: 4.7351\n",
      "Epoch 26/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4502 - val_loss: 4.6325\n",
      "Epoch 27/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4245 - val_loss: 4.6353\n",
      "Epoch 28/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4127 - val_loss: 4.6638\n",
      "Epoch 29/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.4280 - val_loss: 4.6347\n",
      "Epoch 1/1000\n",
      "105/105 [==============================] - 1s 2ms/step - loss: 94.5524 - val_loss: 84.9099\n",
      "Epoch 2/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 78.2333 - val_loss: 71.5963\n",
      "Epoch 3/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 67.1244 - val_loss: 62.4232\n",
      "Epoch 4/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 58.9926 - val_loss: 55.2587\n",
      "Epoch 5/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 52.4236 - val_loss: 49.3158\n",
      "Epoch 6/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 46.8637 - val_loss: 44.2079\n",
      "Epoch 7/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 42.0447 - val_loss: 39.7648\n",
      "Epoch 8/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 37.8271 - val_loss: 35.8470\n",
      "Epoch 9/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 34.1066 - val_loss: 32.4035\n",
      "Epoch 10/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 30.8200 - val_loss: 29.3449\n",
      "Epoch 11/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 27.9109 - val_loss: 26.6496\n",
      "Epoch 12/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 25.3431 - val_loss: 24.2838\n",
      "Epoch 13/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 23.0787 - val_loss: 22.1871\n",
      "Epoch 14/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 21.0916 - val_loss: 20.3654\n",
      "Epoch 15/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 19.3566 - val_loss: 18.7860\n",
      "Epoch 16/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 17.8445 - val_loss: 17.4109\n",
      "Epoch 17/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 16.5400 - val_loss: 16.2250\n",
      "Epoch 18/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 15.4184 - val_loss: 15.2108\n",
      "Epoch 19/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 14.4618 - val_loss: 14.3606\n",
      "Epoch 20/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 13.6541 - val_loss: 13.6422\n",
      "Epoch 21/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 12.9714 - val_loss: 13.0397\n",
      "Epoch 22/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 12.4047 - val_loss: 12.5502\n",
      "Epoch 23/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 11.9424 - val_loss: 12.1483\n",
      "Epoch 24/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 11.5637 - val_loss: 11.8312\n",
      "Epoch 25/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 11.2574 - val_loss: 11.5640\n",
      "Epoch 26/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 11.0106 - val_loss: 11.3657\n",
      "Epoch 27/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 10.8203 - val_loss: 11.2113\n",
      "Epoch 28/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 10.6728 - val_loss: 11.0929\n",
      "Epoch 29/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 10.5613 - val_loss: 11.0036\n",
      "Epoch 30/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 10.4757 - val_loss: 10.9410\n",
      "Epoch 31/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 10.4135 - val_loss: 10.8931\n",
      "Epoch 32/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 10.3665 - val_loss: 10.8586\n",
      "Epoch 33/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 10.3285 - val_loss: 10.8259\n",
      "Epoch 34/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 10.2836 - val_loss: 10.7648\n",
      "Epoch 35/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 10.0758 - val_loss: 10.0327\n",
      "Epoch 36/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 8.6727 - val_loss: 8.4091\n",
      "Epoch 37/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 7.8962 - val_loss: 7.9944\n",
      "Epoch 38/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 7.5672 - val_loss: 7.7634\n",
      "Epoch 39/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 7.3771 - val_loss: 7.5973\n",
      "Epoch 40/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 7.2270 - val_loss: 7.4419\n",
      "Epoch 41/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 7.0891 - val_loss: 7.2705\n",
      "Epoch 42/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 6.9619 - val_loss: 7.1223\n",
      "Epoch 43/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 6.8232 - val_loss: 6.9827\n",
      "Epoch 44/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 6.7185 - val_loss: 6.8630\n",
      "Epoch 45/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 6.6224 - val_loss: 6.7572\n",
      "Epoch 46/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 6.5202 - val_loss: 6.6399\n",
      "Epoch 47/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 6.3788 - val_loss: 6.4913\n",
      "Epoch 48/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 6.2058 - val_loss: 6.3251\n",
      "Epoch 49/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 6.0352 - val_loss: 6.2012\n",
      "Epoch 50/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.8843 - val_loss: 6.0874\n",
      "Epoch 51/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.7752 - val_loss: 5.9803\n",
      "Epoch 52/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.6603 - val_loss: 5.8881\n",
      "Epoch 53/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.5747 - val_loss: 5.8301\n",
      "Epoch 54/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.4825 - val_loss: 5.7755\n",
      "Epoch 55/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.4097 - val_loss: 5.6934\n",
      "Epoch 56/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.3274 - val_loss: 5.6111\n",
      "Epoch 57/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.2889 - val_loss: 5.5671\n",
      "Epoch 58/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.2094 - val_loss: 5.5109\n",
      "Epoch 59/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.1661 - val_loss: 5.4664\n",
      "Epoch 60/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.1190 - val_loss: 5.3979\n",
      "Epoch 61/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.0695 - val_loss: 5.4079\n",
      "Epoch 62/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.0275 - val_loss: 5.3116\n",
      "Epoch 63/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.9789 - val_loss: 5.3541\n",
      "Epoch 64/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.9581 - val_loss: 5.2410\n",
      "Epoch 65/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.9117 - val_loss: 5.2536\n",
      "Epoch 66/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.8806 - val_loss: 5.1907\n",
      "Epoch 67/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.8716 - val_loss: 5.1508\n",
      "Epoch 68/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.8553 - val_loss: 5.1173\n",
      "Epoch 69/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.7938 - val_loss: 5.1103\n",
      "Epoch 70/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.7746 - val_loss: 5.1272\n",
      "Epoch 71/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.7477 - val_loss: 5.0483\n",
      "Epoch 72/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.7162 - val_loss: 5.0306\n",
      "Epoch 73/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.7128 - val_loss: 5.0172\n",
      "Epoch 74/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.7080 - val_loss: 5.0239\n",
      "Epoch 75/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.6955 - val_loss: 4.9678\n",
      "Epoch 76/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.6622 - val_loss: 4.9898\n",
      "Epoch 77/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.6309 - val_loss: 4.9274\n",
      "Epoch 78/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.6286 - val_loss: 4.9717\n",
      "Epoch 79/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5997 - val_loss: 4.9574\n",
      "Epoch 80/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.6166 - val_loss: 4.8849\n",
      "Epoch 81/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5842 - val_loss: 4.8681\n",
      "Epoch 82/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5703 - val_loss: 4.8484\n",
      "Epoch 83/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.5746 - val_loss: 4.8408\n",
      "Epoch 84/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5665 - val_loss: 4.9192\n",
      "Epoch 85/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.5426 - val_loss: 4.8452\n",
      "Epoch 86/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5453 - val_loss: 4.7897\n",
      "Epoch 87/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5245 - val_loss: 4.8060\n",
      "Epoch 88/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5078 - val_loss: 4.7923\n",
      "Epoch 89/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5043 - val_loss: 4.7652\n",
      "Epoch 90/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4851 - val_loss: 4.7857\n",
      "Epoch 91/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.4779 - val_loss: 4.7592\n",
      "Epoch 92/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4780 - val_loss: 4.8417\n",
      "Epoch 93/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4825 - val_loss: 4.7656\n",
      "Epoch 94/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.4556 - val_loss: 4.7609\n",
      "Epoch 95/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4625 - val_loss: 4.7701\n",
      "Epoch 96/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4619 - val_loss: 4.7189\n",
      "Epoch 97/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.4481 - val_loss: 4.7214\n",
      "Epoch 98/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.4443 - val_loss: 4.7124\n",
      "Epoch 99/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4348 - val_loss: 4.7186\n",
      "Epoch 100/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4593 - val_loss: 4.7008\n",
      "Epoch 101/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.4228 - val_loss: 4.7256\n",
      "Epoch 102/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.4213 - val_loss: 4.6772\n",
      "Epoch 103/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4097 - val_loss: 4.6945\n",
      "Epoch 104/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.4100 - val_loss: 4.6880\n",
      "Epoch 105/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.4062 - val_loss: 4.6979\n",
      "Epoch 106/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.3917 - val_loss: 4.6584\n",
      "Epoch 107/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3845 - val_loss: 4.6880\n",
      "Epoch 108/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3972 - val_loss: 4.6397\n",
      "Epoch 109/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3785 - val_loss: 4.6461\n",
      "Epoch 110/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.4008 - val_loss: 4.6417\n",
      "Epoch 111/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3617 - val_loss: 4.6674\n",
      "Epoch 112/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3916 - val_loss: 4.6376\n",
      "Epoch 113/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3808 - val_loss: 4.6424\n",
      "Epoch 114/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.3488 - val_loss: 4.7299\n",
      "Epoch 115/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3626 - val_loss: 4.6178\n",
      "Epoch 116/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3493 - val_loss: 4.6188\n",
      "Epoch 117/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.3530 - val_loss: 4.5981\n",
      "Epoch 118/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3418 - val_loss: 4.6139\n",
      "Epoch 119/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3540 - val_loss: 4.6173\n",
      "Epoch 120/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3581 - val_loss: 4.6276\n",
      "Epoch 121/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3288 - val_loss: 4.5762\n",
      "Epoch 122/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3303 - val_loss: 4.5897\n",
      "Epoch 123/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.3243 - val_loss: 4.5766\n",
      "Epoch 124/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3359 - val_loss: 4.6051\n",
      "Epoch 125/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.3106 - val_loss: 4.6171\n",
      "Epoch 126/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3269 - val_loss: 4.5742\n",
      "Epoch 127/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3334 - val_loss: 4.5759\n",
      "Epoch 128/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3122 - val_loss: 4.5816\n",
      "Epoch 129/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3234 - val_loss: 4.5542\n",
      "Epoch 130/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3256 - val_loss: 4.5624\n",
      "Epoch 131/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3074 - val_loss: 4.6565\n",
      "Epoch 132/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3048 - val_loss: 4.5991\n",
      "Epoch 133/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.3089 - val_loss: 4.6735\n",
      "Epoch 134/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.2967 - val_loss: 4.5935\n",
      "105/105 [==============================] - 0s 721us/step\n",
      "105/105 [==============================] - 0s 692us/step\n",
      "105/105 [==============================] - 0s 711us/step\n",
      "27/27 [==============================] - 0s 713us/step\n",
      "27/27 [==============================] - 0s 769us/step\n",
      "27/27 [==============================] - 0s 768us/step\n",
      "--------------------------\n",
      "adam 32\n",
      "131/131 - 0s - loss: 5.8055 - 101ms/epoch - 771us/step\n",
      "131/131 - 0s - loss: 4.4674 - 105ms/epoch - 802us/step\n",
      "131/131 - 0s - loss: 4.3675 - 104ms/epoch - 794us/step\n",
      "131/131 - 0s - loss: 5.8055 - 106ms/epoch - 809us/step\n",
      "5.805517673492432\n",
      "131/131 - 0s - loss: 4.4674 - 106ms/epoch - 809us/step\n",
      "4.4674153327941895\n",
      "131/131 - 0s - loss: 4.3675 - 107ms/epoch - 817us/step\n",
      "4.367512226104736\n",
      "      Rings       pred      pred2      pred3\n",
      "200       9   7.249785   7.657016   8.018969\n",
      "633       9   9.965288  11.192570  10.126593\n",
      "601      10  10.417198  11.186096  10.738018\n",
      "3767     10        NaN        NaN        NaN\n",
      "1952     10   8.904627  10.506750   9.544514\n",
      "2152      8   9.446864  10.320569   9.831449\n",
      "2556      6   4.271589   4.412486   5.088657\n",
      "1085      7   5.300204   5.937037   6.013456\n",
      "878       9   9.049541   9.937695   9.794570\n",
      "3525      6        NaN        NaN        NaN\n",
      "3520     11        NaN        NaN        NaN\n",
      "2249     12   8.805439   9.961309   9.961878\n",
      "3395     13        NaN        NaN        NaN\n",
      "2375     10  13.001209  15.253380  15.345886\n",
      "2109     13   9.690989  12.634634  12.233407\n",
      "613      18   7.719237   7.582561   7.908638\n",
      "3582     10        NaN        NaN        NaN\n",
      "1440      7   9.241493  11.173593  10.297836\n",
      "2465     11   8.426545  10.252581   9.800451\n",
      "2558      6  12.691985  14.120758  13.890811\n",
      "--------------------------\n",
      "Epoch 1/1000\n",
      "53/53 [==============================] - 1s 4ms/step - loss: 79.2573 - val_loss: 45.4887\n",
      "Epoch 2/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 31.1109 - val_loss: 14.8475\n",
      "Epoch 3/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 20.8600 - val_loss: 8.3535\n",
      "Epoch 4/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 16.8170 - val_loss: 6.9412\n",
      "Epoch 5/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 15.2565 - val_loss: 6.3412\n",
      "Epoch 6/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.6380 - val_loss: 6.1726\n",
      "Epoch 7/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.9892 - val_loss: 5.9641\n",
      "Epoch 8/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.0748 - val_loss: 6.2035\n",
      "Epoch 9/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.8367 - val_loss: 5.9539\n",
      "Epoch 10/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.6982 - val_loss: 5.6745\n",
      "Epoch 11/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.8553 - val_loss: 5.7714\n",
      "Epoch 12/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.5354 - val_loss: 5.4585\n",
      "Epoch 13/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.7030 - val_loss: 5.6901\n",
      "Epoch 14/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.3339 - val_loss: 6.0109\n",
      "Epoch 15/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.4167 - val_loss: 5.3020\n",
      "Epoch 16/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.9528 - val_loss: 5.9478\n",
      "Epoch 17/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3939 - val_loss: 5.7012\n",
      "Epoch 18/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2009 - val_loss: 5.5973\n",
      "Epoch 19/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.4729 - val_loss: 5.1772\n",
      "Epoch 20/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9.8754 - val_loss: 5.3612\n",
      "Epoch 21/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.1571 - val_loss: 5.4825\n",
      "Epoch 22/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9.6500 - val_loss: 5.1450\n",
      "Epoch 23/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.0030 - val_loss: 5.1182\n",
      "Epoch 24/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9.2145 - val_loss: 5.2903\n",
      "Epoch 25/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9.7626 - val_loss: 4.9725\n",
      "Epoch 26/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9.5580 - val_loss: 5.3137\n",
      "Epoch 27/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9.7095 - val_loss: 5.3523\n",
      "Epoch 28/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9.4499 - val_loss: 5.0747\n",
      "Epoch 29/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9.2781 - val_loss: 5.3759\n",
      "Epoch 30/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8.8510 - val_loss: 5.3363\n",
      "Epoch 1/1000\n",
      "53/53 [==============================] - 1s 3ms/step - loss: 110.0381 - val_loss: 107.6115\n",
      "Epoch 2/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 107.6401 - val_loss: 106.5726\n",
      "Epoch 3/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 106.6041 - val_loss: 105.5429\n",
      "Epoch 4/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 105.5743 - val_loss: 104.5271\n",
      "Epoch 5/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 104.5544 - val_loss: 103.5159\n",
      "Epoch 6/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 103.5421 - val_loss: 102.5119\n",
      "Epoch 7/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 102.5372 - val_loss: 101.5152\n",
      "Epoch 8/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 101.5394 - val_loss: 100.5293\n",
      "Epoch 9/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 100.5510 - val_loss: 99.5518\n",
      "Epoch 10/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 99.5691 - val_loss: 98.5778\n",
      "Epoch 11/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 98.5935 - val_loss: 97.6131\n",
      "Epoch 12/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 97.6277 - val_loss: 96.6578\n",
      "Epoch 13/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 96.6705 - val_loss: 95.7087\n",
      "Epoch 14/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 95.7205 - val_loss: 94.7687\n",
      "Epoch 15/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 94.7762 - val_loss: 93.8334\n",
      "Epoch 16/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 93.8377 - val_loss: 92.9051\n",
      "Epoch 17/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 92.9085 - val_loss: 91.9846\n",
      "Epoch 18/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 91.9871 - val_loss: 91.0710\n",
      "Epoch 19/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 91.0720 - val_loss: 90.1677\n",
      "Epoch 20/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 90.1620 - val_loss: 89.2650\n",
      "Epoch 21/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 89.2600 - val_loss: 88.3722\n",
      "Epoch 22/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 88.3624 - val_loss: 87.4854\n",
      "Epoch 23/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 87.4741 - val_loss: 86.6042\n",
      "Epoch 24/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 86.5938 - val_loss: 85.7329\n",
      "Epoch 25/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 85.7190 - val_loss: 84.8677\n",
      "Epoch 26/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 84.8509 - val_loss: 84.0088\n",
      "Epoch 27/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 83.9900 - val_loss: 83.1583\n",
      "Epoch 28/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 83.1371 - val_loss: 82.3111\n",
      "Epoch 29/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 82.2868 - val_loss: 81.4711\n",
      "Epoch 30/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 81.4453 - val_loss: 80.6400\n",
      "Epoch 31/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 80.6101 - val_loss: 79.8122\n",
      "Epoch 32/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 79.7825 - val_loss: 78.9936\n",
      "Epoch 33/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 78.9619 - val_loss: 78.1791\n",
      "Epoch 34/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 78.1463 - val_loss: 77.3753\n",
      "Epoch 35/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 77.3389 - val_loss: 76.5767\n",
      "Epoch 36/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 76.5355 - val_loss: 75.7818\n",
      "Epoch 37/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 75.7375 - val_loss: 74.9911\n",
      "Epoch 38/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 74.9474 - val_loss: 74.2074\n",
      "Epoch 39/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 74.1627 - val_loss: 73.4340\n",
      "Epoch 40/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 73.3852 - val_loss: 72.6641\n",
      "Epoch 41/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 72.6142 - val_loss: 71.9012\n",
      "Epoch 42/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 71.8473 - val_loss: 71.1435\n",
      "Epoch 43/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 71.0878 - val_loss: 70.3922\n",
      "Epoch 44/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 70.3327 - val_loss: 69.6451\n",
      "Epoch 45/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 69.5838 - val_loss: 68.9038\n",
      "Epoch 46/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 68.8409 - val_loss: 68.1676\n",
      "Epoch 47/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 68.1003 - val_loss: 67.4383\n",
      "Epoch 48/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 67.3679 - val_loss: 66.7119\n",
      "Epoch 49/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 66.6417 - val_loss: 65.9946\n",
      "Epoch 50/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 65.9216 - val_loss: 65.2810\n",
      "Epoch 51/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 65.2061 - val_loss: 64.5790\n",
      "Epoch 52/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 64.4987 - val_loss: 63.8785\n",
      "Epoch 53/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 63.7951 - val_loss: 63.1788\n",
      "Epoch 54/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 63.0964 - val_loss: 62.4897\n",
      "Epoch 55/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 62.4050 - val_loss: 61.8072\n",
      "Epoch 56/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 61.7185 - val_loss: 61.1297\n",
      "Epoch 57/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 61.0385 - val_loss: 60.4534\n",
      "Epoch 58/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 60.3623 - val_loss: 59.7892\n",
      "Epoch 59/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 59.6940 - val_loss: 59.1265\n",
      "Epoch 60/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 59.0273 - val_loss: 58.4672\n",
      "Epoch 61/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 58.3670 - val_loss: 57.8174\n",
      "Epoch 62/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 57.7115 - val_loss: 57.1699\n",
      "Epoch 63/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 57.0625 - val_loss: 56.5307\n",
      "Epoch 64/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 56.4193 - val_loss: 55.8924\n",
      "Epoch 65/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 55.7788 - val_loss: 55.2590\n",
      "Epoch 66/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 55.1440 - val_loss: 54.6342\n",
      "Epoch 67/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 54.5144 - val_loss: 54.0108\n",
      "Epoch 68/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 53.8915 - val_loss: 53.3977\n",
      "Epoch 69/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 53.2742 - val_loss: 52.7861\n",
      "Epoch 70/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 52.6610 - val_loss: 52.1818\n",
      "Epoch 71/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 52.0544 - val_loss: 51.5826\n",
      "Epoch 72/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 51.4536 - val_loss: 50.9908\n",
      "Epoch 73/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 50.8568 - val_loss: 50.3999\n",
      "Epoch 74/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 50.2641 - val_loss: 49.8178\n",
      "Epoch 75/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 49.6785 - val_loss: 49.2368\n",
      "Epoch 76/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 49.0979 - val_loss: 48.6641\n",
      "Epoch 77/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 48.5198 - val_loss: 48.0944\n",
      "Epoch 78/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 47.9482 - val_loss: 47.5302\n",
      "Epoch 79/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 47.3802 - val_loss: 46.9709\n",
      "Epoch 80/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 46.8186 - val_loss: 46.4163\n",
      "Epoch 81/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 46.2624 - val_loss: 45.8673\n",
      "Epoch 82/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 45.7115 - val_loss: 45.3238\n",
      "Epoch 83/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 45.1654 - val_loss: 44.7871\n",
      "Epoch 84/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 44.6233 - val_loss: 44.2535\n",
      "Epoch 85/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 44.0869 - val_loss: 43.7228\n",
      "Epoch 86/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 43.5544 - val_loss: 43.1975\n",
      "Epoch 87/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 43.0271 - val_loss: 42.6775\n",
      "Epoch 88/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 42.5060 - val_loss: 42.1644\n",
      "Epoch 89/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 41.9888 - val_loss: 41.6551\n",
      "Epoch 90/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 41.4764 - val_loss: 41.1500\n",
      "Epoch 91/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 40.9686 - val_loss: 40.6497\n",
      "Epoch 92/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 40.4652 - val_loss: 40.1514\n",
      "Epoch 93/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 39.9667 - val_loss: 39.6631\n",
      "Epoch 94/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 39.4749 - val_loss: 39.1768\n",
      "Epoch 95/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 38.9857 - val_loss: 38.6974\n",
      "Epoch 96/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 38.5000 - val_loss: 38.2195\n",
      "Epoch 97/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 38.0227 - val_loss: 37.7482\n",
      "Epoch 98/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 37.5489 - val_loss: 37.2818\n",
      "Epoch 99/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 37.0785 - val_loss: 36.8180\n",
      "Epoch 100/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 36.6141 - val_loss: 36.3615\n",
      "Epoch 101/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 36.1559 - val_loss: 35.9109\n",
      "Epoch 102/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 35.6992 - val_loss: 35.4622\n",
      "Epoch 103/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 35.2479 - val_loss: 35.0181\n",
      "Epoch 104/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 34.8014 - val_loss: 34.5779\n",
      "Epoch 105/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 34.3602 - val_loss: 34.1457\n",
      "Epoch 106/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 33.9241 - val_loss: 33.7164\n",
      "Epoch 107/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 33.4917 - val_loss: 33.2898\n",
      "Epoch 108/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 33.0642 - val_loss: 32.8681\n",
      "Epoch 109/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 32.6418 - val_loss: 32.4573\n",
      "Epoch 110/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 32.2247 - val_loss: 32.0432\n",
      "Epoch 111/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 31.8087 - val_loss: 31.6366\n",
      "Epoch 112/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 31.4001 - val_loss: 31.2338\n",
      "Epoch 113/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 30.9947 - val_loss: 30.8389\n",
      "Epoch 114/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 30.5963 - val_loss: 30.4449\n",
      "Epoch 115/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 30.2014 - val_loss: 30.0585\n",
      "Epoch 116/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 29.8092 - val_loss: 29.6721\n",
      "Epoch 117/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 29.4230 - val_loss: 29.2924\n",
      "Epoch 118/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 29.0414 - val_loss: 28.9213\n",
      "Epoch 119/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 28.6665 - val_loss: 28.5504\n",
      "Epoch 120/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 28.2962 - val_loss: 28.1879\n",
      "Epoch 121/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 27.9278 - val_loss: 27.8255\n",
      "Epoch 122/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 27.5639 - val_loss: 27.4712\n",
      "Epoch 123/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 27.2044 - val_loss: 27.1184\n",
      "Epoch 124/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 26.8508 - val_loss: 26.7718\n",
      "Epoch 125/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 26.5001 - val_loss: 26.4266\n",
      "Epoch 126/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 26.1557 - val_loss: 26.0885\n",
      "Epoch 127/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 25.8145 - val_loss: 25.7550\n",
      "Epoch 128/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 25.4783 - val_loss: 25.4258\n",
      "Epoch 129/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 25.1457 - val_loss: 25.1012\n",
      "Epoch 130/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 24.8177 - val_loss: 24.7788\n",
      "Epoch 131/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 24.4922 - val_loss: 24.4631\n",
      "Epoch 132/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 24.1733 - val_loss: 24.1486\n",
      "Epoch 133/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 23.8576 - val_loss: 23.8405\n",
      "Epoch 134/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 23.5470 - val_loss: 23.5368\n",
      "Epoch 135/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 23.2410 - val_loss: 23.2365\n",
      "Epoch 136/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 22.9384 - val_loss: 22.9409\n",
      "Epoch 137/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 22.6402 - val_loss: 22.6495\n",
      "Epoch 138/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 22.3463 - val_loss: 22.3629\n",
      "Epoch 139/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 22.0569 - val_loss: 22.0811\n",
      "Epoch 140/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 21.7718 - val_loss: 21.8007\n",
      "Epoch 141/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 21.4905 - val_loss: 21.5267\n",
      "Epoch 142/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 21.2132 - val_loss: 21.2556\n",
      "Epoch 143/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 20.9393 - val_loss: 20.9892\n",
      "Epoch 144/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 20.6702 - val_loss: 20.7272\n",
      "Epoch 145/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 20.4061 - val_loss: 20.4688\n",
      "Epoch 146/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 20.1452 - val_loss: 20.2162\n",
      "Epoch 147/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 19.8891 - val_loss: 19.9659\n",
      "Epoch 148/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 19.6369 - val_loss: 19.7203\n",
      "Epoch 149/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 19.3880 - val_loss: 19.4752\n",
      "Epoch 150/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 19.1422 - val_loss: 19.2397\n",
      "Epoch 151/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 18.9017 - val_loss: 19.0032\n",
      "Epoch 152/1000\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 18.6646 - val_loss: 18.7708\n",
      "Epoch 153/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 18.4326 - val_loss: 18.5506\n",
      "Epoch 154/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 18.2042 - val_loss: 18.3237\n",
      "Epoch 155/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 17.9759 - val_loss: 18.1054\n",
      "Epoch 156/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 17.7559 - val_loss: 17.8917\n",
      "Epoch 157/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 17.5414 - val_loss: 17.6820\n",
      "Epoch 158/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 17.3286 - val_loss: 17.4775\n",
      "Epoch 159/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 17.1209 - val_loss: 17.2742\n",
      "Epoch 160/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 16.9153 - val_loss: 17.0768\n",
      "Epoch 161/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 16.7144 - val_loss: 16.8814\n",
      "Epoch 162/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 16.5171 - val_loss: 16.6899\n",
      "Epoch 163/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 16.3232 - val_loss: 16.5034\n",
      "Epoch 164/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 16.1341 - val_loss: 16.3181\n",
      "Epoch 165/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 15.9476 - val_loss: 16.1405\n",
      "Epoch 166/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 15.7667 - val_loss: 15.9623\n",
      "Epoch 167/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 15.5868 - val_loss: 15.7919\n",
      "Epoch 168/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 15.4130 - val_loss: 15.6227\n",
      "Epoch 169/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 15.2427 - val_loss: 15.4587\n",
      "Epoch 170/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 15.0751 - val_loss: 15.2980\n",
      "Epoch 171/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.9105 - val_loss: 15.1388\n",
      "Epoch 172/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.7513 - val_loss: 14.9841\n",
      "Epoch 173/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.5953 - val_loss: 14.8363\n",
      "Epoch 174/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.4434 - val_loss: 14.6893\n",
      "Epoch 175/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.2927 - val_loss: 14.5452\n",
      "Epoch 176/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.1468 - val_loss: 14.4028\n",
      "Epoch 177/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.0041 - val_loss: 14.2671\n",
      "Epoch 178/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.8664 - val_loss: 14.1341\n",
      "Epoch 179/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.7310 - val_loss: 14.0035\n",
      "Epoch 180/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.5995 - val_loss: 13.8790\n",
      "Epoch 181/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.4714 - val_loss: 13.7574\n",
      "Epoch 182/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.3461 - val_loss: 13.6385\n",
      "Epoch 183/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.2239 - val_loss: 13.5210\n",
      "Epoch 184/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.1039 - val_loss: 13.4063\n",
      "Epoch 185/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.9878 - val_loss: 13.2964\n",
      "Epoch 186/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.8755 - val_loss: 13.1886\n",
      "Epoch 187/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.7648 - val_loss: 13.0843\n",
      "Epoch 188/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.6581 - val_loss: 12.9803\n",
      "Epoch 189/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.5550 - val_loss: 12.8838\n",
      "Epoch 190/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.4553 - val_loss: 12.7889\n",
      "Epoch 191/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.3591 - val_loss: 12.6984\n",
      "Epoch 192/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.2651 - val_loss: 12.6077\n",
      "Epoch 193/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.1730 - val_loss: 12.5223\n",
      "Epoch 194/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.0845 - val_loss: 12.4387\n",
      "Epoch 195/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.9990 - val_loss: 12.3590\n",
      "Epoch 196/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.9169 - val_loss: 12.2813\n",
      "Epoch 197/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.8370 - val_loss: 12.2049\n",
      "Epoch 198/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.7590 - val_loss: 12.1341\n",
      "Epoch 199/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.6843 - val_loss: 12.0626\n",
      "Epoch 200/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.6114 - val_loss: 11.9942\n",
      "Epoch 201/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.5417 - val_loss: 11.9303\n",
      "Epoch 202/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.4750 - val_loss: 11.8675\n",
      "Epoch 203/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.4109 - val_loss: 11.8073\n",
      "Epoch 204/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.3479 - val_loss: 11.7496\n",
      "Epoch 205/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.2880 - val_loss: 11.6936\n",
      "Epoch 206/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.2303 - val_loss: 11.6405\n",
      "Epoch 207/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.1746 - val_loss: 11.5889\n",
      "Epoch 208/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.1221 - val_loss: 11.5412\n",
      "Epoch 209/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.0720 - val_loss: 11.4952\n",
      "Epoch 210/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.0232 - val_loss: 11.4518\n",
      "Epoch 211/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.9769 - val_loss: 11.4077\n",
      "Epoch 212/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.9328 - val_loss: 11.3679\n",
      "Epoch 213/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.8912 - val_loss: 11.3304\n",
      "Epoch 214/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.8527 - val_loss: 11.2968\n",
      "Epoch 215/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.8148 - val_loss: 11.2608\n",
      "Epoch 216/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.7791 - val_loss: 11.2303\n",
      "Epoch 217/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.7451 - val_loss: 11.1989\n",
      "Epoch 218/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.7121 - val_loss: 11.1696\n",
      "Epoch 219/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.6822 - val_loss: 11.1434\n",
      "Epoch 220/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.6531 - val_loss: 11.1179\n",
      "Epoch 221/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.6256 - val_loss: 11.0933\n",
      "Epoch 222/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.5996 - val_loss: 11.0709\n",
      "Epoch 223/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.5756 - val_loss: 11.0506\n",
      "Epoch 224/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.5525 - val_loss: 11.0297\n",
      "Epoch 225/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.5311 - val_loss: 11.0120\n",
      "Epoch 226/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.5110 - val_loss: 10.9934\n",
      "Epoch 227/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.4916 - val_loss: 10.9777\n",
      "Epoch 228/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.4737 - val_loss: 10.9625\n",
      "Epoch 229/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.4575 - val_loss: 10.9485\n",
      "Epoch 230/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.4422 - val_loss: 10.9351\n",
      "Epoch 231/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.4279 - val_loss: 10.9245\n",
      "Epoch 232/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.4150 - val_loss: 10.9143\n",
      "Epoch 233/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.4025 - val_loss: 10.9039\n",
      "Epoch 234/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3912 - val_loss: 10.8946\n",
      "Epoch 235/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3816 - val_loss: 10.8872\n",
      "Epoch 236/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3716 - val_loss: 10.8793\n",
      "Epoch 237/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3628 - val_loss: 10.8724\n",
      "Epoch 238/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3542 - val_loss: 10.8663\n",
      "Epoch 239/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3468 - val_loss: 10.8605\n",
      "Epoch 240/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3398 - val_loss: 10.8557\n",
      "Epoch 241/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3340 - val_loss: 10.8507\n",
      "Epoch 242/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3280 - val_loss: 10.8474\n",
      "Epoch 243/1000\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10.3232 - val_loss: 10.8441\n",
      "Epoch 244/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3193 - val_loss: 10.8413\n",
      "Epoch 245/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3153 - val_loss: 10.8389\n",
      "Epoch 246/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3117 - val_loss: 10.8362\n",
      "Epoch 247/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3086 - val_loss: 10.8349\n",
      "Epoch 248/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3056 - val_loss: 10.8326\n",
      "Epoch 249/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3030 - val_loss: 10.8315\n",
      "Epoch 250/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3006 - val_loss: 10.8301\n",
      "Epoch 251/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2982 - val_loss: 10.8286\n",
      "Epoch 252/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2962 - val_loss: 10.8278\n",
      "Epoch 253/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2943 - val_loss: 10.8270\n",
      "Epoch 254/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2933 - val_loss: 10.8264\n",
      "Epoch 255/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2920 - val_loss: 10.8262\n",
      "Epoch 256/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2908 - val_loss: 10.8258\n",
      "Epoch 257/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2899 - val_loss: 10.8255\n",
      "Epoch 258/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2891 - val_loss: 10.8253\n",
      "Epoch 259/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2884 - val_loss: 10.8252\n",
      "Epoch 260/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2878 - val_loss: 10.8252\n",
      "Epoch 261/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2874 - val_loss: 10.8252\n",
      "Epoch 262/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2868 - val_loss: 10.8252\n",
      "Epoch 263/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2864 - val_loss: 10.8252\n",
      "Epoch 264/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2861 - val_loss: 10.8254\n",
      "Epoch 265/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2857 - val_loss: 10.8254\n",
      "Epoch 266/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2853 - val_loss: 10.8257\n",
      "Epoch 1/1000\n",
      "53/53 [==============================] - 1s 4ms/step - loss: 124.6007 - val_loss: 119.2018\n",
      "Epoch 2/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 115.8009 - val_loss: 111.1533\n",
      "Epoch 3/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 108.3059 - val_loss: 104.4099\n",
      "Epoch 4/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 102.2254 - val_loss: 99.0790\n",
      "Epoch 5/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 97.4469 - val_loss: 94.8870\n",
      "Epoch 6/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 93.6543 - val_loss: 91.5059\n",
      "Epoch 7/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 90.5231 - val_loss: 88.6438\n",
      "Epoch 8/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 87.8175 - val_loss: 86.1165\n",
      "Epoch 9/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 85.4017 - val_loss: 83.8326\n",
      "Epoch 10/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 83.1804 - val_loss: 81.7081\n",
      "Epoch 11/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 81.1103 - val_loss: 79.7102\n",
      "Epoch 12/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 79.1487 - val_loss: 77.8103\n",
      "Epoch 13/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 77.2770 - val_loss: 75.9950\n",
      "Epoch 14/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 75.4766 - val_loss: 74.2375\n",
      "Epoch 15/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 73.7398 - val_loss: 72.5438\n",
      "Epoch 16/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 72.0612 - val_loss: 70.8989\n",
      "Epoch 17/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 70.4291 - val_loss: 69.3035\n",
      "Epoch 18/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 68.8374 - val_loss: 67.7372\n",
      "Epoch 19/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 67.2863 - val_loss: 66.2268\n",
      "Epoch 20/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 65.7733 - val_loss: 64.7386\n",
      "Epoch 21/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 64.2936 - val_loss: 63.2914\n",
      "Epoch 22/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 62.8489 - val_loss: 61.8683\n",
      "Epoch 23/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 61.4331 - val_loss: 60.4790\n",
      "Epoch 24/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 60.0434 - val_loss: 59.1142\n",
      "Epoch 25/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 58.6804 - val_loss: 57.7727\n",
      "Epoch 26/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 57.3453 - val_loss: 56.4592\n",
      "Epoch 27/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 56.0249 - val_loss: 55.1542\n",
      "Epoch 28/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 54.7132 - val_loss: 53.8559\n",
      "Epoch 29/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 53.3947 - val_loss: 52.5298\n",
      "Epoch 30/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 52.0254 - val_loss: 51.1174\n",
      "Epoch 31/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 50.4835 - val_loss: 49.4100\n",
      "Epoch 32/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 48.4437 - val_loss: 46.9646\n",
      "Epoch 33/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 45.5930 - val_loss: 43.8718\n",
      "Epoch 34/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 42.5264 - val_loss: 40.9965\n",
      "Epoch 35/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 39.8629 - val_loss: 38.5980\n",
      "Epoch 36/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 37.6038 - val_loss: 36.5289\n",
      "Epoch 37/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 35.6306 - val_loss: 34.6973\n",
      "Epoch 38/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 33.8721 - val_loss: 33.0469\n",
      "Epoch 39/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 32.2817 - val_loss: 31.5474\n",
      "Epoch 40/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 30.8226 - val_loss: 30.1704\n",
      "Epoch 41/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 29.4845 - val_loss: 28.8913\n",
      "Epoch 42/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 28.2362 - val_loss: 27.7135\n",
      "Epoch 43/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 27.0793 - val_loss: 26.6035\n",
      "Epoch 44/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 25.9902 - val_loss: 25.5699\n",
      "Epoch 45/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 24.9720 - val_loss: 24.5941\n",
      "Epoch 46/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 24.0139 - val_loss: 23.6848\n",
      "Epoch 47/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 23.1109 - val_loss: 22.8272\n",
      "Epoch 48/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 22.2663 - val_loss: 22.0212\n",
      "Epoch 49/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 21.4657 - val_loss: 21.2552\n",
      "Epoch 50/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 20.7108 - val_loss: 20.5335\n",
      "Epoch 51/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 20.0018 - val_loss: 19.8613\n",
      "Epoch 52/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 19.3366 - val_loss: 19.2301\n",
      "Epoch 53/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 18.7041 - val_loss: 18.6196\n",
      "Epoch 54/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 18.1103 - val_loss: 18.0643\n",
      "Epoch 55/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 17.5491 - val_loss: 17.5286\n",
      "Epoch 56/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 17.0171 - val_loss: 17.0250\n",
      "Epoch 57/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 16.5208 - val_loss: 16.5504\n",
      "Epoch 58/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 16.0553 - val_loss: 16.1062\n",
      "Epoch 59/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 15.6123 - val_loss: 15.6980\n",
      "Epoch 60/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 15.2002 - val_loss: 15.2992\n",
      "Epoch 61/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.8091 - val_loss: 14.9367\n",
      "Epoch 62/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.4465 - val_loss: 14.5930\n",
      "Epoch 63/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.1044 - val_loss: 14.2703\n",
      "Epoch 64/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.7857 - val_loss: 13.9738\n",
      "Epoch 65/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.4894 - val_loss: 13.6932\n",
      "Epoch 66/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.2122 - val_loss: 13.4375\n",
      "Epoch 67/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.9544 - val_loss: 13.1994\n",
      "Epoch 68/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.7162 - val_loss: 12.9768\n",
      "Epoch 69/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.4939 - val_loss: 12.7713\n",
      "Epoch 70/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.2882 - val_loss: 12.5821\n",
      "Epoch 71/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.0962 - val_loss: 12.4047\n",
      "Epoch 72/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.9202 - val_loss: 12.2450\n",
      "Epoch 73/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.7538 - val_loss: 12.0884\n",
      "Epoch 74/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.6036 - val_loss: 11.9517\n",
      "Epoch 75/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.4684 - val_loss: 11.8299\n",
      "Epoch 76/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.3454 - val_loss: 11.7180\n",
      "Epoch 77/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.2302 - val_loss: 11.6161\n",
      "Epoch 78/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.1264 - val_loss: 11.5232\n",
      "Epoch 79/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.0326 - val_loss: 11.4362\n",
      "Epoch 80/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.9450 - val_loss: 11.3627\n",
      "Epoch 81/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.8671 - val_loss: 11.2949\n",
      "Epoch 82/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.7988 - val_loss: 11.2328\n",
      "Epoch 83/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.7346 - val_loss: 11.1742\n",
      "Epoch 84/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.6770 - val_loss: 11.1275\n",
      "Epoch 85/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.6281 - val_loss: 11.0842\n",
      "Epoch 86/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.5827 - val_loss: 11.0460\n",
      "Epoch 87/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.5418 - val_loss: 11.0132\n",
      "Epoch 88/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.5071 - val_loss: 10.9839\n",
      "Epoch 89/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.4760 - val_loss: 10.9582\n",
      "Epoch 90/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.4475 - val_loss: 10.9353\n",
      "Epoch 91/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.4235 - val_loss: 10.9156\n",
      "Epoch 92/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.4014 - val_loss: 10.8988\n",
      "Epoch 93/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3824 - val_loss: 10.8823\n",
      "Epoch 94/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3654 - val_loss: 10.8707\n",
      "Epoch 95/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3509 - val_loss: 10.8586\n",
      "Epoch 96/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3374 - val_loss: 10.8483\n",
      "Epoch 97/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3247 - val_loss: 10.8362\n",
      "Epoch 98/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3097 - val_loss: 10.8225\n",
      "Epoch 99/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2887 - val_loss: 10.7917\n",
      "Epoch 100/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2169 - val_loss: 10.6007\n",
      "Epoch 101/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9.5403 - val_loss: 9.1660\n",
      "Epoch 102/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8.5780 - val_loss: 8.6752\n",
      "Epoch 103/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8.2384 - val_loss: 8.4150\n",
      "Epoch 104/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.9964 - val_loss: 8.2226\n",
      "Epoch 105/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.8222 - val_loss: 8.0836\n",
      "Epoch 106/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.6905 - val_loss: 7.9930\n",
      "Epoch 107/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.5914 - val_loss: 7.8904\n",
      "Epoch 108/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.5046 - val_loss: 7.8093\n",
      "Epoch 109/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.4279 - val_loss: 7.7237\n",
      "Epoch 110/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.3537 - val_loss: 7.6374\n",
      "Epoch 111/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.2744 - val_loss: 7.5374\n",
      "Epoch 112/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.1905 - val_loss: 7.4132\n",
      "Epoch 113/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.1009 - val_loss: 7.3198\n",
      "Epoch 114/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.0211 - val_loss: 7.2299\n",
      "Epoch 115/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.9516 - val_loss: 7.1438\n",
      "Epoch 116/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.8833 - val_loss: 7.0721\n",
      "Epoch 117/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.8193 - val_loss: 7.0089\n",
      "Epoch 118/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.7524 - val_loss: 6.9372\n",
      "Epoch 119/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.6979 - val_loss: 6.8903\n",
      "Epoch 120/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.6332 - val_loss: 6.8140\n",
      "Epoch 121/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.5561 - val_loss: 6.7416\n",
      "Epoch 122/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.4813 - val_loss: 6.6751\n",
      "Epoch 123/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.3885 - val_loss: 6.5997\n",
      "Epoch 124/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.3126 - val_loss: 6.5195\n",
      "Epoch 125/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.2134 - val_loss: 6.4294\n",
      "Epoch 126/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.1411 - val_loss: 6.3536\n",
      "Epoch 127/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.0748 - val_loss: 6.3179\n",
      "Epoch 128/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.9851 - val_loss: 6.2904\n",
      "Epoch 129/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.9324 - val_loss: 6.1892\n",
      "Epoch 130/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.8734 - val_loss: 6.1310\n",
      "Epoch 131/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.8144 - val_loss: 6.0702\n",
      "Epoch 132/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.7546 - val_loss: 6.0259\n",
      "Epoch 133/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.7125 - val_loss: 5.9747\n",
      "Epoch 134/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.6425 - val_loss: 5.9325\n",
      "Epoch 135/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.5980 - val_loss: 5.8984\n",
      "Epoch 136/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.5607 - val_loss: 5.8398\n",
      "Epoch 137/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.5033 - val_loss: 5.8019\n",
      "Epoch 138/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.4728 - val_loss: 5.7676\n",
      "Epoch 139/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.4298 - val_loss: 5.7481\n",
      "Epoch 140/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.4017 - val_loss: 5.6881\n",
      "Epoch 141/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.3571 - val_loss: 5.6640\n",
      "Epoch 142/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.3210 - val_loss: 5.6272\n",
      "Epoch 143/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.2963 - val_loss: 5.5864\n",
      "Epoch 144/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.2747 - val_loss: 5.5823\n",
      "Epoch 145/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.2465 - val_loss: 5.5503\n",
      "Epoch 146/1000\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.2008 - val_loss: 5.5134\n",
      "Epoch 147/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.1721 - val_loss: 5.4874\n",
      "Epoch 148/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.1546 - val_loss: 5.4608\n",
      "Epoch 149/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.1196 - val_loss: 5.4280\n",
      "Epoch 150/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.1019 - val_loss: 5.3980\n",
      "Epoch 151/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.0809 - val_loss: 5.4125\n",
      "Epoch 152/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.0561 - val_loss: 5.3865\n",
      "Epoch 153/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.0352 - val_loss: 5.3530\n",
      "Epoch 154/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.0259 - val_loss: 5.3125\n",
      "Epoch 155/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.9787 - val_loss: 5.3040\n",
      "Epoch 156/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.9531 - val_loss: 5.2909\n",
      "Epoch 157/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.9463 - val_loss: 5.2535\n",
      "Epoch 158/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.9292 - val_loss: 5.2314\n",
      "Epoch 159/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8971 - val_loss: 5.2279\n",
      "Epoch 160/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8932 - val_loss: 5.2362\n",
      "Epoch 161/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8753 - val_loss: 5.2276\n",
      "Epoch 162/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8555 - val_loss: 5.1636\n",
      "Epoch 163/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8346 - val_loss: 5.1737\n",
      "Epoch 164/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8174 - val_loss: 5.1927\n",
      "Epoch 165/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8014 - val_loss: 5.1292\n",
      "Epoch 166/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7933 - val_loss: 5.1374\n",
      "Epoch 167/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7686 - val_loss: 5.0919\n",
      "Epoch 168/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7643 - val_loss: 5.0995\n",
      "Epoch 169/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7358 - val_loss: 5.0744\n",
      "Epoch 170/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7463 - val_loss: 5.0365\n",
      "Epoch 171/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7194 - val_loss: 5.0325\n",
      "Epoch 172/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7077 - val_loss: 5.0222\n",
      "Epoch 173/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7092 - val_loss: 5.1103\n",
      "Epoch 174/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6840 - val_loss: 4.9903\n",
      "Epoch 175/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6757 - val_loss: 4.9950\n",
      "Epoch 176/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6658 - val_loss: 5.0689\n",
      "Epoch 177/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6719 - val_loss: 4.9670\n",
      "Epoch 178/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6416 - val_loss: 4.9821\n",
      "Epoch 179/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6395 - val_loss: 4.9470\n",
      "Epoch 180/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6157 - val_loss: 4.9508\n",
      "Epoch 181/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6118 - val_loss: 4.9152\n",
      "Epoch 182/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5999 - val_loss: 4.9063\n",
      "Epoch 183/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5931 - val_loss: 4.9006\n",
      "Epoch 184/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5848 - val_loss: 4.9135\n",
      "Epoch 185/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5671 - val_loss: 4.8927\n",
      "Epoch 186/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5612 - val_loss: 4.9041\n",
      "Epoch 187/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5623 - val_loss: 4.8649\n",
      "Epoch 188/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5474 - val_loss: 4.8814\n",
      "Epoch 189/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5279 - val_loss: 4.8743\n",
      "Epoch 190/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5264 - val_loss: 4.8502\n",
      "Epoch 191/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5307 - val_loss: 4.8389\n",
      "Epoch 192/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5236 - val_loss: 4.8309\n",
      "Epoch 193/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5209 - val_loss: 4.8686\n",
      "Epoch 194/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5040 - val_loss: 4.8020\n",
      "Epoch 195/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4929 - val_loss: 4.7937\n",
      "Epoch 196/1000\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.4987 - val_loss: 4.8310\n",
      "Epoch 197/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4870 - val_loss: 4.8005\n",
      "Epoch 198/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4730 - val_loss: 4.7834\n",
      "Epoch 199/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4677 - val_loss: 4.7954\n",
      "Epoch 200/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4709 - val_loss: 4.7814\n",
      "Epoch 201/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4935 - val_loss: 4.7430\n",
      "Epoch 202/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4566 - val_loss: 4.7466\n",
      "Epoch 203/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4620 - val_loss: 4.7498\n",
      "Epoch 204/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4537 - val_loss: 4.7432\n",
      "Epoch 205/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4424 - val_loss: 4.7349\n",
      "Epoch 206/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4435 - val_loss: 4.7168\n",
      "Epoch 207/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4284 - val_loss: 4.7694\n",
      "Epoch 208/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4327 - val_loss: 4.7203\n",
      "Epoch 209/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4186 - val_loss: 4.7293\n",
      "Epoch 210/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4125 - val_loss: 4.7045\n",
      "Epoch 211/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4124 - val_loss: 4.7108\n",
      "Epoch 212/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4221 - val_loss: 4.6926\n",
      "Epoch 213/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4085 - val_loss: 4.6876\n",
      "Epoch 214/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3904 - val_loss: 4.7847\n",
      "Epoch 215/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3975 - val_loss: 4.6682\n",
      "Epoch 216/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3870 - val_loss: 4.6602\n",
      "Epoch 217/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3842 - val_loss: 4.7413\n",
      "Epoch 218/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4064 - val_loss: 4.6700\n",
      "Epoch 219/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3705 - val_loss: 4.6628\n",
      "Epoch 220/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3798 - val_loss: 4.6461\n",
      "Epoch 221/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3700 - val_loss: 4.6380\n",
      "Epoch 222/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3702 - val_loss: 4.6327\n",
      "Epoch 223/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3845 - val_loss: 4.6671\n",
      "Epoch 224/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3597 - val_loss: 4.6441\n",
      "Epoch 225/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3593 - val_loss: 4.6417\n",
      "Epoch 226/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3703 - val_loss: 4.6156\n",
      "Epoch 227/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3547 - val_loss: 4.6176\n",
      "Epoch 228/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3376 - val_loss: 4.6165\n",
      "Epoch 229/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3500 - val_loss: 4.6182\n",
      "Epoch 230/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3320 - val_loss: 4.6979\n",
      "Epoch 231/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3312 - val_loss: 4.5923\n",
      "Epoch 232/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3256 - val_loss: 4.6098\n",
      "Epoch 233/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3252 - val_loss: 4.6111\n",
      "Epoch 234/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3460 - val_loss: 4.6079\n",
      "Epoch 235/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3180 - val_loss: 4.6046\n",
      "Epoch 236/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3306 - val_loss: 4.5923\n",
      "105/105 [==============================] - 0s 779us/step\n",
      "105/105 [==============================] - 0s 750us/step\n",
      "105/105 [==============================] - 0s 731us/step\n",
      "27/27 [==============================] - 0s 730us/step\n",
      "27/27 [==============================] - 0s 769us/step\n",
      "27/27 [==============================] - 0s 770us/step\n",
      "--------------------------\n",
      "adam 64\n",
      "131/131 - 0s - loss: 5.0804 - 104ms/epoch - 794us/step\n",
      "131/131 - 0s - loss: 10.3932 - 105ms/epoch - 802us/step\n",
      "131/131 - 0s - loss: 4.3654 - 105ms/epoch - 802us/step\n",
      "131/131 - 0s - loss: 5.0804 - 103ms/epoch - 786us/step\n",
      "5.080408096313477\n",
      "131/131 - 0s - loss: 10.3932 - 105ms/epoch - 802us/step\n",
      "10.39317798614502\n",
      "131/131 - 0s - loss: 4.3654 - 113ms/epoch - 863us/step\n",
      "4.365355491638184\n",
      "      Rings       pred     pred2      pred3\n",
      "1781      8  10.711509  9.913507  11.049501\n",
      "3899      4        NaN       NaN        NaN\n",
      "1543      7   7.147095  9.913507   7.700689\n",
      "1585      9  11.453785  9.913507  12.340792\n",
      "67       13  10.452949  9.913507  11.806084\n",
      "1182     11   9.203010  9.913507   9.738611\n",
      "1699     13   9.272592  9.913507   9.643979\n",
      "2224      9   9.258733  9.913507   9.548659\n",
      "477      17   9.623614  9.913507  10.520039\n",
      "3588     12        NaN       NaN        NaN\n",
      "2975      6   6.697905  9.913507   7.162208\n",
      "2135     12   9.222464  9.913507   9.623760\n",
      "2187     14  10.893506  9.913507  11.497568\n",
      "2617      9   9.381689  9.913507  10.065832\n",
      "819       7  12.147416  9.913507  13.484414\n",
      "3845     11        NaN       NaN        NaN\n",
      "1145      9  11.002569  9.913507  13.283312\n",
      "3117      8  11.944520  9.913507  12.871463\n",
      "525       4  10.934428  9.913507  12.017857\n",
      "1976     13   7.025950  9.913507   7.319366\n",
      "--------------------------\n",
      "Epoch 1/1000\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 38.3344 - val_loss: 9.1560\n",
      "Epoch 2/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 16.4722 - val_loss: 7.4273\n",
      "Epoch 3/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 15.3193 - val_loss: 6.3294\n",
      "Epoch 4/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 13.7042 - val_loss: 5.7532\n",
      "Epoch 5/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 12.8574 - val_loss: 5.4913\n",
      "Epoch 6/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 12.3504 - val_loss: 5.3500\n",
      "Epoch 7/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 12.3547 - val_loss: 5.0292\n",
      "Epoch 8/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 11.7574 - val_loss: 5.3268\n",
      "Epoch 9/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 11.5909 - val_loss: 6.0188\n",
      "Epoch 10/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 11.1270 - val_loss: 6.0071\n",
      "Epoch 11/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.8892 - val_loss: 5.5427\n",
      "Epoch 12/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.1886 - val_loss: 4.8881\n",
      "Epoch 13/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 9.6986 - val_loss: 5.2380\n",
      "Epoch 14/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 9.8951 - val_loss: 5.5170\n",
      "Epoch 15/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 9.0647 - val_loss: 4.7788\n",
      "Epoch 16/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 9.1630 - val_loss: 5.4099\n",
      "Epoch 17/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 9.1996 - val_loss: 5.5549\n",
      "Epoch 18/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 8.8621 - val_loss: 5.0130\n",
      "Epoch 19/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 8.2283 - val_loss: 5.1549\n",
      "Epoch 20/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 8.5371 - val_loss: 5.3902\n",
      "Epoch 1/1000\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 35.8339 - val_loss: 7.8886\n",
      "Epoch 2/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 6.1375 - val_loss: 5.2858\n",
      "Epoch 3/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.0982 - val_loss: 5.1079\n",
      "Epoch 4/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.8946 - val_loss: 5.1814\n",
      "Epoch 5/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.7976 - val_loss: 5.0874\n",
      "Epoch 6/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.7088 - val_loss: 4.8525\n",
      "Epoch 7/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6794 - val_loss: 4.8022\n",
      "Epoch 8/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6431 - val_loss: 4.7566\n",
      "Epoch 9/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5883 - val_loss: 4.8009\n",
      "Epoch 10/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5842 - val_loss: 4.6909\n",
      "Epoch 11/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5177 - val_loss: 4.8938\n",
      "Epoch 12/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5276 - val_loss: 4.8623\n",
      "Epoch 13/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 4.5000 - val_loss: 4.7644\n",
      "Epoch 14/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4736 - val_loss: 4.6835\n",
      "Epoch 15/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4784 - val_loss: 4.8138\n",
      "Epoch 16/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4446 - val_loss: 4.7034\n",
      "Epoch 17/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4303 - val_loss: 4.6349\n",
      "Epoch 18/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4251 - val_loss: 5.0609\n",
      "Epoch 19/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4095 - val_loss: 4.6545\n",
      "Epoch 20/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4182 - val_loss: 4.6122\n",
      "Epoch 21/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4260 - val_loss: 4.5447\n",
      "Epoch 22/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3815 - val_loss: 4.6942\n",
      "Epoch 23/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3937 - val_loss: 4.6458\n",
      "Epoch 24/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3636 - val_loss: 4.7396\n",
      "Epoch 25/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3376 - val_loss: 4.6015\n",
      "Epoch 26/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3605 - val_loss: 4.6793\n",
      "Epoch 1/1000\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 84.0655 - val_loss: 74.2866\n",
      "Epoch 2/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 67.0231 - val_loss: 59.3733\n",
      "Epoch 3/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 53.3231 - val_loss: 47.3749\n",
      "Epoch 4/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 42.4812 - val_loss: 37.7887\n",
      "Epoch 5/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 33.7289 - val_loss: 29.8461\n",
      "Epoch 6/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 26.3125 - val_loss: 23.0149\n",
      "Epoch 7/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 19.5414 - val_loss: 16.5939\n",
      "Epoch 8/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 13.8636 - val_loss: 12.4192\n",
      "Epoch 9/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 11.0722 - val_loss: 11.0174\n",
      "Epoch 10/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.3753 - val_loss: 10.8337\n",
      "Epoch 11/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.2924 - val_loss: 10.8135\n",
      "Epoch 12/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.2669 - val_loss: 10.7843\n",
      "Epoch 13/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.2022 - val_loss: 10.6663\n",
      "Epoch 14/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 9.9835 - val_loss: 10.2564\n",
      "Epoch 15/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 9.4930 - val_loss: 9.6731\n",
      "Epoch 16/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 8.9988 - val_loss: 9.1482\n",
      "Epoch 17/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 8.4973 - val_loss: 8.5949\n",
      "Epoch 18/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 8.0221 - val_loss: 8.0884\n",
      "Epoch 19/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 7.6103 - val_loss: 7.6847\n",
      "Epoch 20/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 7.2819 - val_loss: 7.3535\n",
      "Epoch 21/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 7.0324 - val_loss: 7.1340\n",
      "Epoch 22/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 6.8181 - val_loss: 6.9381\n",
      "Epoch 23/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 6.5309 - val_loss: 6.5763\n",
      "Epoch 24/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 6.1665 - val_loss: 6.2958\n",
      "Epoch 25/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.8489 - val_loss: 6.0363\n",
      "Epoch 26/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.6096 - val_loss: 5.8988\n",
      "Epoch 27/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.4355 - val_loss: 5.7615\n",
      "Epoch 28/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.2921 - val_loss: 5.6466\n",
      "Epoch 29/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.1674 - val_loss: 5.6438\n",
      "Epoch 30/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.0767 - val_loss: 5.4458\n",
      "Epoch 31/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.9901 - val_loss: 5.3807\n",
      "Epoch 32/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.9126 - val_loss: 5.2969\n",
      "Epoch 33/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.8599 - val_loss: 5.3269\n",
      "Epoch 34/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.8113 - val_loss: 5.2886\n",
      "Epoch 35/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.7752 - val_loss: 5.2324\n",
      "Epoch 36/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.7415 - val_loss: 5.1592\n",
      "Epoch 37/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.7091 - val_loss: 5.0898\n",
      "Epoch 38/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6873 - val_loss: 5.0839\n",
      "Epoch 39/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6673 - val_loss: 5.0498\n",
      "Epoch 40/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6524 - val_loss: 5.0362\n",
      "Epoch 41/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6349 - val_loss: 4.9962\n",
      "Epoch 42/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6042 - val_loss: 5.0397\n",
      "Epoch 43/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6015 - val_loss: 4.9830\n",
      "Epoch 44/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5867 - val_loss: 4.9340\n",
      "Epoch 45/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5639 - val_loss: 4.9895\n",
      "Epoch 46/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5552 - val_loss: 4.9571\n",
      "Epoch 47/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5447 - val_loss: 4.8637\n",
      "Epoch 48/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5289 - val_loss: 4.8750\n",
      "Epoch 49/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5149 - val_loss: 4.8451\n",
      "Epoch 50/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5179 - val_loss: 4.8331\n",
      "Epoch 51/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5179 - val_loss: 4.8671\n",
      "Epoch 52/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4906 - val_loss: 4.7895\n",
      "Epoch 53/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4815 - val_loss: 4.8217\n",
      "Epoch 54/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4872 - val_loss: 4.7732\n",
      "Epoch 55/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4840 - val_loss: 4.7735\n",
      "Epoch 56/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4658 - val_loss: 4.7831\n",
      "Epoch 57/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4644 - val_loss: 4.7884\n",
      "Epoch 58/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4485 - val_loss: 4.7552\n",
      "Epoch 59/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 4.4437 - val_loss: 4.7230\n",
      "Epoch 60/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4391 - val_loss: 4.7584\n",
      "Epoch 61/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4322 - val_loss: 4.7971\n",
      "Epoch 62/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4309 - val_loss: 4.7130\n",
      "Epoch 63/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4262 - val_loss: 4.7141\n",
      "Epoch 64/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4232 - val_loss: 4.7733\n",
      "Epoch 65/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 4.4202 - val_loss: 4.6874\n",
      "Epoch 66/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4108 - val_loss: 4.7730\n",
      "Epoch 67/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4133 - val_loss: 4.6931\n",
      "Epoch 68/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3968 - val_loss: 4.8591\n",
      "Epoch 69/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4015 - val_loss: 4.6627\n",
      "Epoch 70/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3945 - val_loss: 4.6778\n",
      "Epoch 71/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3725 - val_loss: 4.8159\n",
      "Epoch 72/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 4.3796 - val_loss: 4.6483\n",
      "Epoch 73/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3796 - val_loss: 4.6342\n",
      "Epoch 74/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3465 - val_loss: 4.6932\n",
      "Epoch 75/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3570 - val_loss: 4.6882\n",
      "Epoch 76/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3650 - val_loss: 4.6110\n",
      "Epoch 77/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3683 - val_loss: 4.6246\n",
      "Epoch 78/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 4.3622 - val_loss: 4.6494\n",
      "Epoch 79/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 4.3529 - val_loss: 4.6206\n",
      "Epoch 80/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3455 - val_loss: 4.6445\n",
      "Epoch 81/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3280 - val_loss: 4.6274\n",
      "105/105 [==============================] - 0s 731us/step\n",
      "105/105 [==============================] - 0s 731us/step\n",
      "105/105 [==============================] - 0s 760us/step\n",
      "27/27 [==============================] - 0s 769us/step\n",
      "27/27 [==============================] - 0s 731us/step\n",
      "27/27 [==============================] - 0s 731us/step\n",
      "--------------------------\n",
      "rmsprop 16\n",
      "131/131 - 0s - loss: 5.1733 - 104ms/epoch - 794us/step\n",
      "131/131 - 0s - loss: 4.4265 - 105ms/epoch - 802us/step\n",
      "131/131 - 0s - loss: 4.4094 - 108ms/epoch - 824us/step\n",
      "131/131 - 0s - loss: 5.1733 - 106ms/epoch - 809us/step\n",
      "5.173309803009033\n",
      "131/131 - 0s - loss: 4.4265 - 107ms/epoch - 817us/step\n",
      "4.426450729370117\n",
      "131/131 - 0s - loss: 4.4094 - 110ms/epoch - 840us/step\n",
      "4.409439563751221\n",
      "      Rings       pred      pred2      pred3\n",
      "503      13  10.487042  13.164642  11.175706\n",
      "4086      8        NaN        NaN        NaN\n",
      "2531     10   6.679858   7.085255   6.610790\n",
      "3091     10   9.265427  10.471107   9.996311\n",
      "9        19   6.951379   7.534055   7.131285\n",
      "2149      7   9.619367  10.771528  10.328691\n",
      "1019     11   6.861219   8.169477   7.282355\n",
      "848       8   9.893398  11.127359  10.644266\n",
      "3418      8        NaN        NaN        NaN\n",
      "304      10   6.660740   7.299023   6.800359\n",
      "1998      7  10.744884  12.549749  11.189054\n",
      "46        9   9.015266  10.177487   9.541601\n",
      "2199     21   7.927430   8.081470   8.410504\n",
      "3736     10        NaN        NaN        NaN\n",
      "3977      7        NaN        NaN        NaN\n",
      "3451      8        NaN        NaN        NaN\n",
      "2798     10   9.863104  11.272192  10.775669\n",
      "2409      8   9.749408  10.837820  10.013659\n",
      "3617      9        NaN        NaN        NaN\n",
      "342      12   8.382577   9.318425   8.584885\n",
      "--------------------------\n",
      "Epoch 1/1000\n",
      "105/105 [==============================] - 1s 3ms/step - loss: 87.0534 - val_loss: 51.2607\n",
      "Epoch 2/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 38.1022 - val_loss: 17.9509\n",
      "Epoch 3/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 25.1396 - val_loss: 9.2096\n",
      "Epoch 4/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 21.2030 - val_loss: 8.6868\n",
      "Epoch 5/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 19.0036 - val_loss: 7.2293\n",
      "Epoch 6/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 18.6246 - val_loss: 7.7279\n",
      "Epoch 7/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 18.3505 - val_loss: 6.4803\n",
      "Epoch 8/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 17.6555 - val_loss: 6.8895\n",
      "Epoch 9/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 16.8693 - val_loss: 7.4317\n",
      "Epoch 10/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 16.7175 - val_loss: 6.3425\n",
      "Epoch 11/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 16.3986 - val_loss: 6.9918\n",
      "Epoch 12/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 15.6759 - val_loss: 5.9363\n",
      "Epoch 13/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 16.3795 - val_loss: 6.9833\n",
      "Epoch 14/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 15.4635 - val_loss: 5.4377\n",
      "Epoch 15/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 15.4147 - val_loss: 6.6708\n",
      "Epoch 16/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 14.6165 - val_loss: 5.6649\n",
      "Epoch 17/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 14.6815 - val_loss: 6.3085\n",
      "Epoch 18/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 13.9270 - val_loss: 5.8501\n",
      "Epoch 19/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 14.6418 - val_loss: 5.9694\n",
      "Epoch 1/1000\n",
      "105/105 [==============================] - 1s 2ms/step - loss: 88.6937 - val_loss: 50.2862\n",
      "Epoch 2/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 27.0222 - val_loss: 17.0832\n",
      "Epoch 3/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 10.8071 - val_loss: 6.3772\n",
      "Epoch 4/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.6686 - val_loss: 5.1918\n",
      "Epoch 5/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.1155 - val_loss: 5.0423\n",
      "Epoch 6/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.9747 - val_loss: 4.9678\n",
      "Epoch 7/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.9114 - val_loss: 4.9303\n",
      "Epoch 8/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.8438 - val_loss: 4.8650\n",
      "Epoch 9/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.7946 - val_loss: 4.8686\n",
      "Epoch 10/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.7652 - val_loss: 4.8049\n",
      "Epoch 11/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.7183 - val_loss: 4.7496\n",
      "Epoch 12/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.7140 - val_loss: 4.7927\n",
      "Epoch 13/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.6979 - val_loss: 4.8398\n",
      "Epoch 14/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.6776 - val_loss: 4.8337\n",
      "Epoch 15/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.6702 - val_loss: 4.7839\n",
      "Epoch 16/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.6300 - val_loss: 4.8210\n",
      "Epoch 1/1000\n",
      "105/105 [==============================] - 1s 2ms/step - loss: 113.1300 - val_loss: 106.6971\n",
      "Epoch 2/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 103.1856 - val_loss: 98.9773\n",
      "Epoch 3/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 96.3416 - val_loss: 92.8071\n",
      "Epoch 4/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 90.4840 - val_loss: 87.2467\n",
      "Epoch 5/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 84.7734 - val_loss: 81.2333\n",
      "Epoch 6/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 78.2194 - val_loss: 74.2348\n",
      "Epoch 7/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 70.5986 - val_loss: 65.9772\n",
      "Epoch 8/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 61.8244 - val_loss: 57.1662\n",
      "Epoch 9/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 53.3542 - val_loss: 49.2174\n",
      "Epoch 10/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 45.8120 - val_loss: 42.2292\n",
      "Epoch 11/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 39.2193 - val_loss: 36.1513\n",
      "Epoch 12/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 33.4206 - val_loss: 30.8470\n",
      "Epoch 13/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 28.4767 - val_loss: 26.3132\n",
      "Epoch 14/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 24.2090 - val_loss: 22.4253\n",
      "Epoch 15/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 20.5643 - val_loss: 19.1270\n",
      "Epoch 16/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 17.5101 - val_loss: 16.4402\n",
      "Epoch 17/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 15.0314 - val_loss: 14.3056\n",
      "Epoch 18/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 13.1223 - val_loss: 12.7264\n",
      "Epoch 19/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 11.7583 - val_loss: 11.6738\n",
      "Epoch 20/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 10.8896 - val_loss: 11.0990\n",
      "Epoch 21/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 10.4659 - val_loss: 10.8775\n",
      "Epoch 22/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 10.3250 - val_loss: 10.8298\n",
      "Epoch 23/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 10.2942 - val_loss: 10.8254\n",
      "Epoch 24/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 10.2871 - val_loss: 10.8267\n",
      "Epoch 25/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 10.2871 - val_loss: 10.8262\n",
      "Epoch 26/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 10.2870 - val_loss: 10.8268\n",
      "Epoch 27/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 10.2858 - val_loss: 10.8259\n",
      "Epoch 28/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 10.2868 - val_loss: 10.8259\n",
      "105/105 [==============================] - 0s 760us/step\n",
      "105/105 [==============================] - 0s 1ms/step\n",
      "105/105 [==============================] - 0s 769us/step\n",
      "27/27 [==============================] - 0s 731us/step\n",
      "27/27 [==============================] - 0s 808us/step\n",
      "27/27 [==============================] - 0s 769us/step\n",
      "--------------------------\n",
      "rmsprop 32\n",
      "131/131 - 0s - loss: 5.7793 - 106ms/epoch - 809us/step\n",
      "131/131 - 0s - loss: 4.7257 - 103ms/epoch - 786us/step\n",
      "131/131 - 0s - loss: 10.3931 - 105ms/epoch - 802us/step\n",
      "131/131 - 0s - loss: 5.7793 - 104ms/epoch - 794us/step\n",
      "5.779306888580322\n",
      "131/131 - 0s - loss: 4.7257 - 106ms/epoch - 809us/step\n",
      "4.725710868835449\n",
      "131/131 - 0s - loss: 10.3931 - 106ms/epoch - 809us/step\n",
      "10.393108367919922\n",
      "      Rings       pred      pred2     pred3\n",
      "1940     11  10.269854  12.127687  9.915517\n",
      "2228     12   7.372285   8.292878  9.915517\n",
      "3502     10        NaN        NaN       NaN\n",
      "2898      8  10.132347  11.550388  9.915517\n",
      "3370     12        NaN        NaN       NaN\n",
      "1917     10   9.866053  11.499454  9.915517\n",
      "2284      9   8.931365  10.646590  9.915517\n",
      "55        8   9.220232  10.837459  9.915517\n",
      "1637      8   9.102660  10.290614  9.915517\n",
      "796      11  10.712497  12.473284  9.915517\n",
      "666      11   9.739804  11.252625  9.915517\n",
      "1140      9   7.240821   8.709542  9.915517\n",
      "1160      9  10.535874  11.886163  9.915517\n",
      "2601     10   7.992517   9.609602  9.915517\n",
      "1301      9   7.938460   9.322555  9.915517\n",
      "1137     10   8.449254   9.865022  9.915517\n",
      "3495      8        NaN        NaN       NaN\n",
      "3092     11   9.638026  11.437793  9.915517\n",
      "191      12  10.047786  11.494739  9.915517\n",
      "2231      9   9.372126  11.205875  9.915517\n",
      "--------------------------\n",
      "Epoch 1/1000\n",
      "53/53 [==============================] - 1s 4ms/step - loss: 93.7813 - val_loss: 72.5244\n",
      "Epoch 2/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 54.4668 - val_loss: 31.5896\n",
      "Epoch 3/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 32.9537 - val_loss: 20.0144\n",
      "Epoch 4/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 25.4434 - val_loss: 11.8386\n",
      "Epoch 5/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 19.6462 - val_loss: 7.9205\n",
      "Epoch 6/1000\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 17.5144 - val_loss: 6.4434\n",
      "Epoch 7/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 16.3865 - val_loss: 6.4041\n",
      "Epoch 8/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 15.0330 - val_loss: 5.8269\n",
      "Epoch 9/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.8090 - val_loss: 6.0952\n",
      "Epoch 10/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.1987 - val_loss: 5.6713\n",
      "Epoch 11/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.2076 - val_loss: 6.3236\n",
      "Epoch 12/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.0766 - val_loss: 5.5813\n",
      "Epoch 13/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.1353 - val_loss: 5.6603\n",
      "Epoch 14/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.9486 - val_loss: 6.4669\n",
      "Epoch 15/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.0110 - val_loss: 6.4398\n",
      "Epoch 16/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.9370 - val_loss: 6.0215\n",
      "Epoch 17/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.2216 - val_loss: 5.2516\n",
      "Epoch 18/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.8949 - val_loss: 6.0254\n",
      "Epoch 19/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.9225 - val_loss: 5.6836\n",
      "Epoch 20/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.2654 - val_loss: 5.9922\n",
      "Epoch 21/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.9225 - val_loss: 6.0125\n",
      "Epoch 22/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.4313 - val_loss: 6.4192\n",
      "Epoch 1/1000\n",
      "53/53 [==============================] - 1s 3ms/step - loss: 108.6241 - val_loss: 107.3987\n",
      "Epoch 2/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 104.0563 - val_loss: 92.9473\n",
      "Epoch 3/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 73.3473 - val_loss: 49.8148\n",
      "Epoch 4/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 32.7530 - val_loss: 21.9511\n",
      "Epoch 5/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 17.1420 - val_loss: 12.1017\n",
      "Epoch 6/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9.4086 - val_loss: 6.7427\n",
      "Epoch 7/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.1741 - val_loss: 5.6385\n",
      "Epoch 8/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.4392 - val_loss: 5.5048\n",
      "Epoch 9/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.2098 - val_loss: 5.3143\n",
      "Epoch 10/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.0643 - val_loss: 5.1990\n",
      "Epoch 11/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.9695 - val_loss: 5.2757\n",
      "Epoch 12/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 4.8769 - val_loss: 5.1810\n",
      "Epoch 13/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8228 - val_loss: 5.0368\n",
      "Epoch 14/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7947 - val_loss: 5.0429\n",
      "Epoch 15/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7860 - val_loss: 5.1292\n",
      "Epoch 16/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7255 - val_loss: 4.9533\n",
      "Epoch 17/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7066 - val_loss: 4.8725\n",
      "Epoch 18/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6891 - val_loss: 5.1851\n",
      "Epoch 19/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 4.6797 - val_loss: 4.8646\n",
      "Epoch 20/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6639 - val_loss: 4.8973\n",
      "Epoch 21/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6421 - val_loss: 5.0683\n",
      "Epoch 22/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6563 - val_loss: 4.8617\n",
      "Epoch 23/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6120 - val_loss: 4.8329\n",
      "Epoch 24/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5980 - val_loss: 5.0540\n",
      "Epoch 25/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6008 - val_loss: 4.8705\n",
      "Epoch 26/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5898 - val_loss: 4.8018\n",
      "Epoch 27/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5598 - val_loss: 4.9087\n",
      "Epoch 28/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5646 - val_loss: 4.9085\n",
      "Epoch 29/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5564 - val_loss: 4.7495\n",
      "Epoch 30/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5479 - val_loss: 4.8197\n",
      "Epoch 31/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5319 - val_loss: 4.8330\n",
      "Epoch 32/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5125 - val_loss: 4.8499\n",
      "Epoch 33/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5083 - val_loss: 4.7192\n",
      "Epoch 34/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4877 - val_loss: 4.7158\n",
      "Epoch 35/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4963 - val_loss: 4.8656\n",
      "Epoch 36/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4755 - val_loss: 4.7059\n",
      "Epoch 37/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4773 - val_loss: 4.7220\n",
      "Epoch 38/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4668 - val_loss: 4.6837\n",
      "Epoch 39/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4688 - val_loss: 4.6824\n",
      "Epoch 40/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4424 - val_loss: 4.6941\n",
      "Epoch 41/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4265 - val_loss: 4.8355\n",
      "Epoch 42/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4278 - val_loss: 4.6360\n",
      "Epoch 43/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4238 - val_loss: 4.6459\n",
      "Epoch 44/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4267 - val_loss: 4.7294\n",
      "Epoch 45/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4293 - val_loss: 4.6801\n",
      "Epoch 46/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4095 - val_loss: 4.6127\n",
      "Epoch 47/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3787 - val_loss: 4.6870\n",
      "Epoch 48/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3890 - val_loss: 4.6703\n",
      "Epoch 49/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3925 - val_loss: 4.6272\n",
      "Epoch 50/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3852 - val_loss: 4.8511\n",
      "Epoch 51/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4078 - val_loss: 4.6040\n",
      "Epoch 52/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3838 - val_loss: 4.7619\n",
      "Epoch 53/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3776 - val_loss: 4.6238\n",
      "Epoch 54/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3664 - val_loss: 4.7034\n",
      "Epoch 55/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3514 - val_loss: 4.8677\n",
      "Epoch 56/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.3765 - val_loss: 4.6507\n",
      "Epoch 1/1000\n",
      "53/53 [==============================] - 1s 3ms/step - loss: 83.3959 - val_loss: 77.9988\n",
      "Epoch 2/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 74.2718 - val_loss: 69.8550\n",
      "Epoch 3/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 66.7888 - val_loss: 63.0593\n",
      "Epoch 4/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 60.3128 - val_loss: 57.0580\n",
      "Epoch 5/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 54.7007 - val_loss: 51.9574\n",
      "Epoch 6/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 49.9668 - val_loss: 47.6699\n",
      "Epoch 7/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 45.9700 - val_loss: 44.0265\n",
      "Epoch 8/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 42.5562 - val_loss: 40.9034\n",
      "Epoch 9/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 39.5847 - val_loss: 38.1352\n",
      "Epoch 10/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 36.9290 - val_loss: 35.6545\n",
      "Epoch 11/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 34.5333 - val_loss: 33.3889\n",
      "Epoch 12/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 32.3303 - val_loss: 31.2893\n",
      "Epoch 13/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 30.2698 - val_loss: 29.3134\n",
      "Epoch 14/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 28.3536 - val_loss: 27.4993\n",
      "Epoch 15/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 26.5778 - val_loss: 25.8022\n",
      "Epoch 16/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 24.8973 - val_loss: 24.2104\n",
      "Epoch 17/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 23.3215 - val_loss: 22.7133\n",
      "Epoch 18/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 21.8507 - val_loss: 21.3013\n",
      "Epoch 19/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 20.4641 - val_loss: 19.9835\n",
      "Epoch 20/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 19.1751 - val_loss: 18.7759\n",
      "Epoch 21/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 17.9819 - val_loss: 17.6553\n",
      "Epoch 22/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 16.8816 - val_loss: 16.6090\n",
      "Epoch 23/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 15.8559 - val_loss: 15.6644\n",
      "Epoch 24/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.9314 - val_loss: 14.8037\n",
      "Epoch 25/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 14.0930 - val_loss: 14.0300\n",
      "Epoch 26/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 13.3507 - val_loss: 13.3600\n",
      "Epoch 27/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.6807 - val_loss: 12.7478\n",
      "Epoch 28/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 12.0983 - val_loss: 12.2467\n",
      "Epoch 29/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.6169 - val_loss: 11.8224\n",
      "Epoch 30/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 11.2137 - val_loss: 11.4769\n",
      "Epoch 31/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.8883 - val_loss: 11.2175\n",
      "Epoch 32/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.6422 - val_loss: 11.0344\n",
      "Epoch 33/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.4829 - val_loss: 10.9208\n",
      "Epoch 34/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3800 - val_loss: 10.8629\n",
      "Epoch 35/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3267 - val_loss: 10.8364\n",
      "Epoch 36/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.3031 - val_loss: 10.8279\n",
      "Epoch 37/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2925 - val_loss: 10.8252\n",
      "Epoch 38/1000\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 10.2871 - val_loss: 10.8258\n",
      "Epoch 39/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2859 - val_loss: 10.8270\n",
      "Epoch 40/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2852 - val_loss: 10.8289\n",
      "Epoch 41/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2852 - val_loss: 10.8284\n",
      "Epoch 42/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2851 - val_loss: 10.8271\n",
      "105/105 [==============================] - 0s 740us/step\n",
      "105/105 [==============================] - 0s 731us/step\n",
      "105/105 [==============================] - 0s 740us/step\n",
      "27/27 [==============================] - 0s 768us/step\n",
      "27/27 [==============================] - 0s 731us/step\n",
      "27/27 [==============================] - 0s 769us/step\n",
      "--------------------------\n",
      "rmsprop 64\n",
      "131/131 - 0s - loss: 6.2008 - 103ms/epoch - 786us/step\n",
      "131/131 - 0s - loss: 4.4254 - 103ms/epoch - 786us/step\n",
      "131/131 - 0s - loss: 10.3928 - 107ms/epoch - 817us/step\n",
      "131/131 - 0s - loss: 6.2008 - 102ms/epoch - 779us/step\n",
      "6.200751304626465\n",
      "131/131 - 0s - loss: 4.4254 - 103ms/epoch - 786us/step\n",
      "4.4253997802734375\n",
      "131/131 - 0s - loss: 10.3928 - 105ms/epoch - 802us/step\n",
      "10.392776489257812\n",
      "      Rings       pred      pred2     pred3\n",
      "1868      8   7.702749   9.027129  9.933427\n",
      "1140      9   7.048832   7.867436  9.933427\n",
      "1918     11   8.019729   8.343038  9.933427\n",
      "75       15   8.277917   9.011138  9.933427\n",
      "2558      6  12.484006  13.282038  9.933427\n",
      "3836      9        NaN        NaN       NaN\n",
      "641      13   9.041801  10.676736  9.933427\n",
      "1028     11  10.994712  12.742743  9.933427\n",
      "2312      8   9.062306   9.546908  9.933427\n",
      "3762      8        NaN        NaN       NaN\n",
      "3491      8        NaN        NaN       NaN\n",
      "3873      5        NaN        NaN       NaN\n",
      "1646      9  12.987494  13.844182  9.933427\n",
      "78       11   9.762723  10.826966  9.933427\n",
      "4005      9        NaN        NaN       NaN\n",
      "3060     11   6.469553   7.005144  9.933427\n",
      "491      13   9.392023  10.241794  9.933427\n",
      "395      11   8.458566   9.173086  9.933427\n",
      "2216      7   9.926292  10.583373  9.933427\n",
      "1131      8   6.423667   7.063356  9.933427\n",
      "--------------------------\n",
      "Epoch 1/1000\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 18.5780 - val_loss: 7.1942\n",
      "Epoch 2/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 7.1105 - val_loss: 5.7700\n",
      "Epoch 3/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 6.1549 - val_loss: 5.4979\n",
      "Epoch 4/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.9846 - val_loss: 5.3293\n",
      "Epoch 5/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.5885 - val_loss: 5.0140\n",
      "Epoch 6/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.5029 - val_loss: 5.0282\n",
      "Epoch 7/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 5.4170 - val_loss: 4.9710\n",
      "Epoch 8/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.4230 - val_loss: 5.1066\n",
      "Epoch 9/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.3193 - val_loss: 4.8937\n",
      "Epoch 10/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.2140 - val_loss: 4.8720\n",
      "Epoch 11/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.2099 - val_loss: 4.7302\n",
      "Epoch 12/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 5.3120 - val_loss: 5.0808\n",
      "Epoch 13/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.2052 - val_loss: 5.0201\n",
      "Epoch 14/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.1604 - val_loss: 4.6357\n",
      "Epoch 15/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.1582 - val_loss: 4.8089\n",
      "Epoch 16/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 5.0859 - val_loss: 4.9818\n",
      "Epoch 17/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.1232 - val_loss: 4.8510\n",
      "Epoch 18/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 5.0264 - val_loss: 5.0010\n",
      "Epoch 19/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 4.9808 - val_loss: 4.7564\n",
      "Epoch 1/1000\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 9.6486 - val_loss: 5.5855\n",
      "Epoch 2/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.4766 - val_loss: 5.4961\n",
      "Epoch 3/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.1380 - val_loss: 5.6846\n",
      "Epoch 4/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.8896 - val_loss: 5.1161\n",
      "Epoch 5/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 4.8843 - val_loss: 5.1148\n",
      "Epoch 6/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.8165 - val_loss: 5.5625\n",
      "Epoch 7/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.7227 - val_loss: 5.1063\n",
      "Epoch 8/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.7646 - val_loss: 5.6073\n",
      "Epoch 9/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6140 - val_loss: 5.0057\n",
      "Epoch 10/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6183 - val_loss: 4.9070\n",
      "Epoch 11/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 4.5938 - val_loss: 4.8643\n",
      "Epoch 12/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5868 - val_loss: 5.0840\n",
      "Epoch 13/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4727 - val_loss: 4.6244\n",
      "Epoch 14/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5733 - val_loss: 5.3620\n",
      "Epoch 15/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5164 - val_loss: 4.8589\n",
      "Epoch 16/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4895 - val_loss: 4.7231\n",
      "Epoch 17/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4753 - val_loss: 4.5694\n",
      "Epoch 18/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5044 - val_loss: 4.7538\n",
      "Epoch 19/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4337 - val_loss: 4.8964\n",
      "Epoch 20/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4288 - val_loss: 4.6579\n",
      "Epoch 21/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3642 - val_loss: 6.2434\n",
      "Epoch 22/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.3659 - val_loss: 4.6745\n",
      "Epoch 1/1000\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 14.3093 - val_loss: 10.7607\n",
      "Epoch 2/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.1625 - val_loss: 10.5440\n",
      "Epoch 3/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 9.5901 - val_loss: 9.1881\n",
      "Epoch 4/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 7.5305 - val_loss: 7.0244\n",
      "Epoch 5/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 6.6714 - val_loss: 6.6995\n",
      "Epoch 6/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 6.4372 - val_loss: 6.4788\n",
      "Epoch 7/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.9996 - val_loss: 5.8794\n",
      "Epoch 8/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.5240 - val_loss: 5.5895\n",
      "Epoch 9/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.2901 - val_loss: 5.8116\n",
      "Epoch 10/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.2267 - val_loss: 6.5011\n",
      "Epoch 11/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.2173 - val_loss: 5.5463\n",
      "Epoch 12/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 5.1289 - val_loss: 5.2433\n",
      "Epoch 13/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.0403 - val_loss: 5.2093\n",
      "Epoch 14/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 5.0169 - val_loss: 5.3765\n",
      "Epoch 15/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.9396 - val_loss: 5.6708\n",
      "Epoch 16/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.9085 - val_loss: 5.8539\n",
      "Epoch 17/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.8178 - val_loss: 4.8888\n",
      "Epoch 18/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.7412 - val_loss: 5.0698\n",
      "Epoch 19/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6988 - val_loss: 4.8174\n",
      "Epoch 20/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6942 - val_loss: 4.8967\n",
      "Epoch 21/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6402 - val_loss: 4.7571\n",
      "Epoch 22/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.6049 - val_loss: 4.7468\n",
      "Epoch 23/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 4.5644 - val_loss: 4.8273\n",
      "Epoch 24/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5494 - val_loss: 4.7667\n",
      "Epoch 25/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5504 - val_loss: 4.6584\n",
      "Epoch 26/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4993 - val_loss: 4.8828\n",
      "Epoch 27/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5386 - val_loss: 4.7532\n",
      "Epoch 28/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.5137 - val_loss: 4.6334\n",
      "Epoch 29/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 4.4731 - val_loss: 4.7255\n",
      "Epoch 30/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4751 - val_loss: 4.6862\n",
      "Epoch 31/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4696 - val_loss: 4.6955\n",
      "Epoch 32/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4264 - val_loss: 4.6568\n",
      "Epoch 33/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 4.4172 - val_loss: 4.8197\n",
      "105/105 [==============================] - 0s 760us/step\n",
      "105/105 [==============================] - 0s 1ms/step\n",
      "105/105 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 731us/step\n",
      "27/27 [==============================] - 0s 769us/step\n",
      "27/27 [==============================] - 0s 808us/step\n",
      "--------------------------\n",
      "sgd 16\n",
      "131/131 - 0s - loss: 4.5147 - 104ms/epoch - 794us/step\n",
      "131/131 - 0s - loss: 4.2319 - 102ms/epoch - 779us/step\n",
      "131/131 - 0s - loss: 4.5601 - 105ms/epoch - 802us/step\n",
      "131/131 - 0s - loss: 4.5147 - 102ms/epoch - 779us/step\n",
      "4.514708995819092\n",
      "131/131 - 0s - loss: 4.2319 - 107ms/epoch - 817us/step\n",
      "4.23193883895874\n",
      "131/131 - 0s - loss: 4.5601 - 107ms/epoch - 817us/step\n",
      "4.560120105743408\n",
      "      Rings       pred      pred2      pred3\n",
      "307      13  10.305576  10.658498  10.840102\n",
      "3926     14        NaN        NaN        NaN\n",
      "1288      7   5.694260   5.397082   5.854520\n",
      "3000      9  11.586113  10.492535  10.617258\n",
      "3765      9        NaN        NaN        NaN\n",
      "4107      7        NaN        NaN        NaN\n",
      "2288      6  10.920049  10.301242   9.957867\n",
      "692      11  10.773604  10.978500  10.395631\n",
      "2782      8   6.433401   6.402768   6.390929\n",
      "797       9  13.758363  12.534039  13.346678\n",
      "3410      6        NaN        NaN        NaN\n",
      "1958     10   8.494802   8.776704   8.082992\n",
      "244       8  11.475476  10.327982  10.067675\n",
      "1363     10   6.276504   6.317657   6.296540\n",
      "2839     10   9.247608   9.320325   9.591723\n",
      "846      11  12.512829  11.987186  11.188240\n",
      "1366      9  16.112236  13.953478  14.478030\n",
      "2942      9   5.645914   5.450733   5.778750\n",
      "2042      8   9.857306   9.865594   9.714228\n",
      "3803      7        NaN        NaN        NaN\n",
      "--------------------------\n",
      "Epoch 1/1000\n",
      "105/105 [==============================] - 1s 3ms/step - loss: 20.5701 - val_loss: 6.0151\n",
      "Epoch 2/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 8.2127 - val_loss: 6.3103\n",
      "Epoch 3/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 7.1858 - val_loss: 6.1588\n",
      "Epoch 4/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 6.7177 - val_loss: 5.1519\n",
      "Epoch 5/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 6.3483 - val_loss: 5.6117\n",
      "Epoch 6/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.9685 - val_loss: 5.9582\n",
      "Epoch 7/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 6.0309 - val_loss: 4.9307\n",
      "Epoch 8/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.7244 - val_loss: 5.1347\n",
      "Epoch 9/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.7136 - val_loss: 4.8752\n",
      "Epoch 10/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.5190 - val_loss: 5.4852\n",
      "Epoch 11/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.4624 - val_loss: 4.9170\n",
      "Epoch 12/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.1864 - val_loss: 4.9707\n",
      "Epoch 13/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.2617 - val_loss: 5.0687\n",
      "Epoch 14/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.1306 - val_loss: 5.1585\n",
      "Epoch 1/1000\n",
      "105/105 [==============================] - 1s 2ms/step - loss: 11.8090 - val_loss: 7.0678\n",
      "Epoch 2/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.5582 - val_loss: 7.1849\n",
      "Epoch 3/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.1337 - val_loss: 5.5428\n",
      "Epoch 4/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.9544 - val_loss: 4.8902\n",
      "Epoch 5/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.6720 - val_loss: 6.0873\n",
      "Epoch 6/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.8121 - val_loss: 6.9385\n",
      "Epoch 7/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.8539 - val_loss: 5.6081\n",
      "Epoch 8/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.1433 - val_loss: 4.7361\n",
      "Epoch 9/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.7827 - val_loss: 4.6489\n",
      "Epoch 10/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.9049 - val_loss: 5.7521\n",
      "Epoch 11/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.7222 - val_loss: 4.6312\n",
      "Epoch 12/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.7506 - val_loss: 4.7161\n",
      "Epoch 13/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.6807 - val_loss: 5.1985\n",
      "Epoch 14/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.6369 - val_loss: 7.0033\n",
      "Epoch 15/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5957 - val_loss: 4.6487\n",
      "Epoch 16/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.5333 - val_loss: 4.6606\n",
      "Epoch 1/1000\n",
      "105/105 [==============================] - 1s 3ms/step - loss: 17.7880 - val_loss: 10.8191\n",
      "Epoch 2/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 10.2344 - val_loss: 10.7394\n",
      "Epoch 3/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 10.1915 - val_loss: 10.6598\n",
      "Epoch 4/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 10.0792 - val_loss: 10.4979\n",
      "Epoch 5/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 9.8552 - val_loss: 10.1642\n",
      "Epoch 6/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 9.3499 - val_loss: 9.3695\n",
      "Epoch 7/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 8.3914 - val_loss: 8.2239\n",
      "Epoch 8/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 7.4737 - val_loss: 7.4312\n",
      "Epoch 9/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 7.0220 - val_loss: 7.1124\n",
      "Epoch 10/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 6.8079 - val_loss: 6.9123\n",
      "Epoch 11/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 6.6911 - val_loss: 6.7787\n",
      "Epoch 12/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 6.5947 - val_loss: 6.8556\n",
      "Epoch 13/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 6.4997 - val_loss: 6.6219\n",
      "Epoch 14/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 6.3749 - val_loss: 6.4498\n",
      "Epoch 15/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 6.1955 - val_loss: 6.2128\n",
      "Epoch 16/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.9001 - val_loss: 5.9079\n",
      "Epoch 17/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.5832 - val_loss: 5.7545\n",
      "Epoch 18/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.3464 - val_loss: 5.6239\n",
      "Epoch 19/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.2609 - val_loss: 5.5927\n",
      "Epoch 20/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.1521 - val_loss: 5.4221\n",
      "Epoch 21/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.1196 - val_loss: 5.4799\n",
      "Epoch 22/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 5.0764 - val_loss: 5.4339\n",
      "Epoch 23/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.1009 - val_loss: 5.2899\n",
      "Epoch 24/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 5.0074 - val_loss: 5.2989\n",
      "Epoch 25/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.9772 - val_loss: 5.6095\n",
      "Epoch 26/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.9397 - val_loss: 5.2271\n",
      "Epoch 27/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.9823 - val_loss: 5.1365\n",
      "Epoch 28/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.9036 - val_loss: 5.1756\n",
      "Epoch 29/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.9325 - val_loss: 5.1516\n",
      "Epoch 30/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.8000 - val_loss: 4.9251\n",
      "Epoch 31/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.8510 - val_loss: 5.2633\n",
      "Epoch 32/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.7893 - val_loss: 5.1800\n",
      "Epoch 33/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.7469 - val_loss: 4.9478\n",
      "Epoch 34/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.7494 - val_loss: 4.8080\n",
      "Epoch 35/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.6975 - val_loss: 4.7984\n",
      "Epoch 36/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.6482 - val_loss: 4.7800\n",
      "Epoch 37/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.6507 - val_loss: 4.7231\n",
      "Epoch 38/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.6446 - val_loss: 5.2768\n",
      "Epoch 39/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.6095 - val_loss: 4.8779\n",
      "Epoch 40/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.5656 - val_loss: 4.6957\n",
      "Epoch 41/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.6276 - val_loss: 4.7042\n",
      "Epoch 42/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.6380 - val_loss: 4.8204\n",
      "Epoch 43/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5707 - val_loss: 4.6645\n",
      "Epoch 44/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5280 - val_loss: 5.0212\n",
      "Epoch 45/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.5156 - val_loss: 4.7036\n",
      "Epoch 46/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.5868 - val_loss: 4.8120\n",
      "Epoch 47/1000\n",
      "105/105 [==============================] - 0s 1ms/step - loss: 4.5138 - val_loss: 5.2685\n",
      "Epoch 48/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 4.4707 - val_loss: 4.7145\n",
      "105/105 [==============================] - 0s 750us/step\n",
      "105/105 [==============================] - 0s 788us/step\n",
      "105/105 [==============================] - 0s 750us/step\n",
      "27/27 [==============================] - 0s 808us/step\n",
      "27/27 [==============================] - 0s 885us/step\n",
      "27/27 [==============================] - 0s 962us/step\n",
      "--------------------------\n",
      "sgd 32\n",
      "131/131 - 0s - loss: 4.8817 - 105ms/epoch - 802us/step\n",
      "131/131 - 0s - loss: 4.4052 - 114ms/epoch - 870us/step\n",
      "131/131 - 0s - loss: 4.5350 - 105ms/epoch - 802us/step\n",
      "131/131 - 0s - loss: 4.8817 - 105ms/epoch - 802us/step\n",
      "4.881711006164551\n",
      "131/131 - 0s - loss: 4.4052 - 106ms/epoch - 809us/step\n",
      "4.405202388763428\n",
      "131/131 - 0s - loss: 4.5350 - 106ms/epoch - 809us/step\n",
      "4.5349812507629395\n",
      "      Rings       pred      pred2      pred3\n",
      "3646      9        NaN        NaN        NaN\n",
      "1582      9  11.258108   9.752343  10.468109\n",
      "1990      7  10.506385  10.440677  10.285925\n",
      "2704     10  11.299982  10.197020  10.593252\n",
      "1256      7  10.711625  10.851219   9.975833\n",
      "2426     14   8.506534   8.968335   8.701845\n",
      "3776     13        NaN        NaN        NaN\n",
      "3190      5  10.472771  10.105727  10.630110\n",
      "516       9   9.934898  10.483360  10.507805\n",
      "394       6   8.308661   7.831228   7.713036\n",
      "930       7   7.640184   7.728089   7.157944\n",
      "1960     10  13.532320  13.316627  13.010820\n",
      "3173      6  11.323887  10.103666  10.612745\n",
      "2790     10  10.351671   9.404275   9.513748\n",
      "3706     11        NaN        NaN        NaN\n",
      "4146     10        NaN        NaN        NaN\n",
      "948       6   8.821015   8.585499   8.759195\n",
      "3620      9        NaN        NaN        NaN\n",
      "694       4  14.636775  14.181738  12.682396\n",
      "1457      7   9.462145   9.844847   9.684177\n",
      "--------------------------\n",
      "Epoch 1/1000\n",
      "53/53 [==============================] - 1s 4ms/step - loss: 22.2799 - val_loss: 13.2995\n",
      "Epoch 2/1000\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 8.9218 - val_loss: 5.7446\n",
      "Epoch 3/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.8843 - val_loss: 7.3616\n",
      "Epoch 4/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.6706 - val_loss: 7.7280\n",
      "Epoch 5/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.2014 - val_loss: 7.3304\n",
      "Epoch 6/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.2575 - val_loss: 9.8307\n",
      "Epoch 7/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.8427 - val_loss: 5.0451\n",
      "Epoch 8/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.7253 - val_loss: 5.5684\n",
      "Epoch 9/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.6667 - val_loss: 4.9458\n",
      "Epoch 10/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.4864 - val_loss: 5.1191\n",
      "Epoch 11/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.3593 - val_loss: 5.8019\n",
      "Epoch 12/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.1894 - val_loss: 4.9437\n",
      "Epoch 13/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.4682 - val_loss: 5.5096\n",
      "Epoch 14/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.1550 - val_loss: 6.5312\n",
      "Epoch 15/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.1501 - val_loss: 4.7729\n",
      "Epoch 16/1000\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.7786 - val_loss: 4.7495\n",
      "Epoch 17/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.0102 - val_loss: 4.7834\n",
      "Epoch 18/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.6479 - val_loss: 4.7997\n",
      "Epoch 19/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.6446 - val_loss: 4.6394\n",
      "Epoch 20/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.5499 - val_loss: 4.9371\n",
      "Epoch 21/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.8442 - val_loss: 4.6272\n",
      "Epoch 22/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.6648 - val_loss: 4.8816\n",
      "Epoch 23/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.5555 - val_loss: 4.9119\n",
      "Epoch 24/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.5585 - val_loss: 4.7245\n",
      "Epoch 25/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.5597 - val_loss: 6.0905\n",
      "Epoch 26/1000\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.6267 - val_loss: 4.6893\n",
      "Epoch 1/1000\n",
      "53/53 [==============================] - 1s 4ms/step - loss: 21.7255 - val_loss: 6.3379\n",
      "Epoch 2/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.4589 - val_loss: 5.2443\n",
      "Epoch 3/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 5.4993 - val_loss: 4.9452\n",
      "Epoch 4/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.4990 - val_loss: 5.1228\n",
      "Epoch 5/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.0164 - val_loss: 12.8109\n",
      "Epoch 6/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.1441 - val_loss: 5.1287\n",
      "Epoch 7/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6878 - val_loss: 7.5538\n",
      "Epoch 8/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8634 - val_loss: 4.7505\n",
      "Epoch 9/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8337 - val_loss: 4.9674\n",
      "Epoch 10/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 4.6104 - val_loss: 5.5346\n",
      "Epoch 11/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.9182 - val_loss: 4.8345\n",
      "Epoch 12/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6226 - val_loss: 4.6821\n",
      "Epoch 13/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.5225 - val_loss: 5.6151\n",
      "Epoch 14/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 4.6893 - val_loss: 4.6110\n",
      "Epoch 15/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 4.5084 - val_loss: 4.6686\n",
      "Epoch 16/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4213 - val_loss: 4.8280\n",
      "Epoch 17/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.4687 - val_loss: 8.4255\n",
      "Epoch 18/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7434 - val_loss: 8.1758\n",
      "Epoch 19/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 4.6384 - val_loss: 14.0147\n",
      "Epoch 1/1000\n",
      "53/53 [==============================] - 1s 3ms/step - loss: 28.3077 - val_loss: 10.7761\n",
      "Epoch 2/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2463 - val_loss: 10.7759\n",
      "Epoch 3/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2189 - val_loss: 10.7411\n",
      "Epoch 4/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.2002 - val_loss: 10.7131\n",
      "Epoch 5/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.1729 - val_loss: 10.6763\n",
      "Epoch 6/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.1359 - val_loss: 10.6255\n",
      "Epoch 7/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 10.0753 - val_loss: 10.5610\n",
      "Epoch 8/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9.9845 - val_loss: 10.4269\n",
      "Epoch 9/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9.8247 - val_loss: 10.1938\n",
      "Epoch 10/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 9.5275 - val_loss: 9.7463\n",
      "Epoch 11/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8.9546 - val_loss: 8.9468\n",
      "Epoch 12/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 8.1097 - val_loss: 8.0329\n",
      "Epoch 13/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.3521 - val_loss: 7.3845\n",
      "Epoch 14/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 7.0073 - val_loss: 7.1090\n",
      "Epoch 15/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.8660 - val_loss: 7.1540\n",
      "Epoch 16/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.7881 - val_loss: 6.9147\n",
      "Epoch 17/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.7180 - val_loss: 6.8818\n",
      "Epoch 18/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.6692 - val_loss: 6.8142\n",
      "Epoch 19/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.6209 - val_loss: 6.7493\n",
      "Epoch 20/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.5718 - val_loss: 6.7087\n",
      "Epoch 21/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.5262 - val_loss: 6.6646\n",
      "Epoch 22/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.4891 - val_loss: 6.6246\n",
      "Epoch 23/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.4198 - val_loss: 6.5456\n",
      "Epoch 24/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.3555 - val_loss: 6.4768\n",
      "Epoch 25/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.2728 - val_loss: 6.4451\n",
      "Epoch 26/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.1762 - val_loss: 6.2668\n",
      "Epoch 27/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 6.0187 - val_loss: 6.2685\n",
      "Epoch 28/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.8588 - val_loss: 6.0099\n",
      "Epoch 29/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.6634 - val_loss: 5.8483\n",
      "Epoch 30/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.4827 - val_loss: 5.8483\n",
      "Epoch 31/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 5.3048 - val_loss: 5.5479\n",
      "Epoch 32/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 5.2226 - val_loss: 5.5196\n",
      "Epoch 33/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.1661 - val_loss: 5.5749\n",
      "Epoch 34/1000\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 5.1380 - val_loss: 5.9997\n",
      "Epoch 35/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.1359 - val_loss: 5.5780\n",
      "Epoch 36/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.0904 - val_loss: 5.3154\n",
      "Epoch 37/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.0536 - val_loss: 5.7827\n",
      "Epoch 38/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.0426 - val_loss: 5.8504\n",
      "Epoch 39/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.9711 - val_loss: 5.2579\n",
      "Epoch 40/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 5.0120 - val_loss: 5.2650\n",
      "Epoch 41/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 4.9731 - val_loss: 5.3976\n",
      "Epoch 42/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.9595 - val_loss: 5.2121\n",
      "Epoch 43/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.9297 - val_loss: 5.9866\n",
      "Epoch 44/1000\n",
      "53/53 [==============================] - 0s 1ms/step - loss: 4.9821 - val_loss: 5.3777\n",
      "Epoch 45/1000\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 4.9136 - val_loss: 5.1073\n",
      "Epoch 46/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8897 - val_loss: 5.1337\n",
      "Epoch 47/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.9124 - val_loss: 5.2796\n",
      "Epoch 48/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8296 - val_loss: 5.0896\n",
      "Epoch 49/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7917 - val_loss: 5.7068\n",
      "Epoch 50/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8189 - val_loss: 4.9853\n",
      "Epoch 51/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7861 - val_loss: 4.9558\n",
      "Epoch 52/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8088 - val_loss: 5.0720\n",
      "Epoch 53/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.8082 - val_loss: 5.1916\n",
      "Epoch 54/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7686 - val_loss: 4.9486\n",
      "Epoch 55/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7451 - val_loss: 5.1806\n",
      "Epoch 56/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7693 - val_loss: 5.5707\n",
      "Epoch 57/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.6919 - val_loss: 7.1655\n",
      "Epoch 58/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7083 - val_loss: 5.5452\n",
      "Epoch 59/1000\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 4.7091 - val_loss: 5.1812\n",
      "105/105 [==============================] - 0s 769us/step\n",
      "105/105 [==============================] - 0s 721us/step\n",
      "105/105 [==============================] - 0s 740us/step\n",
      "27/27 [==============================] - 0s 731us/step\n",
      "27/27 [==============================] - 0s 1ms/step\n",
      "27/27 [==============================] - 0s 962us/step\n",
      "--------------------------\n",
      "sgd 64\n",
      "131/131 - 0s - loss: 4.4363 - 105ms/epoch - 801us/step\n",
      "131/131 - 0s - loss: 13.2423 - 103ms/epoch - 786us/step\n",
      "131/131 - 0s - loss: 5.0244 - 102ms/epoch - 779us/step\n",
      "131/131 - 0s - loss: 4.4363 - 110ms/epoch - 840us/step\n",
      "4.436325550079346\n",
      "131/131 - 0s - loss: 13.2423 - 107ms/epoch - 817us/step\n",
      "13.2422513961792\n",
      "131/131 - 0s - loss: 5.0244 - 115ms/epoch - 878us/step\n",
      "5.024422645568848\n",
      "      Rings       pred      pred2      pred3\n",
      "2653      9  10.896444  11.592472   8.298260\n",
      "1033     10   7.501407  11.934385   6.926607\n",
      "1179     11   7.454587  10.465014   6.914073\n",
      "1135      8   9.190505  14.114367   8.714706\n",
      "499      10  10.986535  11.698536   9.507847\n",
      "378      15   7.794747  12.786016   7.208931\n",
      "406       8  10.213909  13.873516   9.151394\n",
      "2973     12   6.856715  12.013386   6.554403\n",
      "2980      7  10.996729  12.150576  10.380088\n",
      "3313     11   9.709507  13.231112   8.855537\n",
      "774      10  11.024107  12.781002  10.268922\n",
      "1956     11  10.073221  11.077623   9.235318\n",
      "1514     11  10.498689  14.236572   9.379380\n",
      "2030     10  11.084892  11.968729  10.323673\n",
      "1849      7  11.808630  13.718088  10.718685\n",
      "3868      8        NaN        NaN        NaN\n",
      "1866     10   9.656096  10.642946   9.013120\n",
      "3592     11        NaN        NaN        NaN\n",
      "2863     11  10.684515  11.556936   9.463570\n",
      "953       7  12.678484  13.320303  11.955645\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "lst = ['adam', 'rmsprop', 'sgd']\n",
    "batch_lst = [16, 32, 64]\n",
    "adam_result = []\n",
    "rmsprop_result = []\n",
    "sgd_result = []\n",
    "result_lst = []\n",
    "test_adam = []\n",
    "test_rmsprop =[]\n",
    "test_sgd = []\n",
    "for i in lst:\n",
    "    for j in batch_lst:\n",
    "\n",
    "        # 모델 구현\n",
    "        model = Sequential()\n",
    "        model.add(Dense(32, activation='relu', input_dim=x.shape[1]))\n",
    "        model.add(Dropout(0.1))  # Dropout 추가\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(Dropout(0.1))  # Dropout 추가\n",
    "        model.add(Dense(8, activation='relu'))\n",
    "        model.add(Dropout(0.1))  # Dropout 추가\n",
    "        model.add(Dense(4, activation='relu'))\n",
    "        model.add(Dropout(0.1))  # Dropout 추가\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        # Rings가 정수이기 때문에 linear로 마무리\n",
    "\n",
    "\n",
    "\n",
    "        model.compile(loss='mse', optimizer=i)\n",
    "\n",
    "        # Define the early stopping callback\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "        # Fit the model with early stopping\n",
    "        model.fit(X_train, y_train, epochs=1000, batch_size=j, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "        # 기존모델\n",
    "        model2 = Sequential()\n",
    "        model2.add(Dense(32, activation='relu', input_dim=x.shape[1]))\n",
    "        model2.add(Dense(16, activation='relu'))\n",
    "        model2.add(Dense(8, activation='relu'))\n",
    "        model2.add(Dense(4, activation='relu'))\n",
    "        model2.add(Dense(1, activation='linear'))\n",
    "\n",
    "        model2.compile(loss='mse', optimizer=i)\n",
    "\n",
    "        # Define the early stopping callback\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "        # Fit the model2 with early stopping\n",
    "        model2.fit(X_train, y_train, epochs=1000, batch_size=j, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "        # 활성화 함수 변경\n",
    "        model3 = Sequential()\n",
    "        model3.add(Dense(32, activation='sigmoid', input_dim=x.shape[1]))  # Change activation function\n",
    "        model3.add(Dense(16, activation='sigmoid'))  # Change activation function\n",
    "        model3.add(Dense(8, activation='sigmoid'))  # Change activation function\n",
    "        model3.add(Dense(4, activation='sigmoid'))  # Change activation function\n",
    "        model3.add(Dense(1, activation='linear'))\n",
    "\n",
    "        model3.compile(loss='mse', optimizer=i)\n",
    "\n",
    "        # Define the early stopping callback\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "        # Fit the model3 with early stopping\n",
    "        model3.fit(X_train, y_train, epochs=1000, batch_size=j, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "\n",
    "        pred_y = model.predict(X_train)\n",
    "        pred_y2 = model2.predict(X_train)\n",
    "        pred_y3 = model3.predict(X_train)\n",
    "        pred_y4 = model.predict(X_test)\n",
    "        pred_y5 = model2.predict(X_test)\n",
    "        pred_y6 = model3.predict(X_test)\n",
    "        y_pred = pred_y.flatten()\n",
    "        y_pred2 = pred_y2.flatten()\n",
    "        y_pred3 = pred_y3.flatten()\n",
    "        y_pred4 = pred_y4.flatten()\n",
    "        y_pred5 = pred_y5.flatten()\n",
    "        y_pred6 = pred_y6.flatten()        \n",
    "        tmp = eval_accuracy(y_train, y_pred)\n",
    "        tmp2 = eval_accuracy(y_train, y_pred2)\n",
    "        tmp3 = eval_accuracy(y_train, y_pred3)\n",
    "        tmp4 = eval_accuracy(y_test, y_pred4)\n",
    "        tmp5 = eval_accuracy(y_test, y_pred5)\n",
    "        tmp6 = eval_accuracy(y_test, y_pred6)\n",
    "\n",
    "        if i == 'adam':\n",
    "            adam_result.append(j)\n",
    "            adam_result.append((tmp, tmp2, tmp3))\n",
    "            test_adam.append((tmp4,tmp5,tmp6))\n",
    "        elif i == 'rmsprop':\n",
    "            rmsprop_result.append(j)\n",
    "            rmsprop_result.append((tmp, tmp2, tmp3))\n",
    "            test_rmsprop.append((tmp4,tmp5,tmp6))\n",
    "        else:\n",
    "            sgd_result.append(j)\n",
    "            sgd_result.append((tmp, tmp2, tmp3))\n",
    "            test_sgd.append((tmp4,tmp5,tmp6))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        new_y = y_train\n",
    "        stacked_array = np.vstack((pred_y))\n",
    "        stacked_array2 = np.vstack((pred_y2))\n",
    "        stacked_array3 = np.vstack((pred_y3))\n",
    "        new_df = pd.DataFrame(stacked_array)\n",
    "        new_df2 = pd.DataFrame(stacked_array2)\n",
    "        new_df3 = pd.DataFrame(stacked_array3)\n",
    "        new_y = pd.DataFrame(new_y)\n",
    "        new_y['pred'] = new_df[0] \n",
    "        new_y['pred2'] = new_df2[0]\n",
    "        new_y['pred3'] = new_df3[0]\n",
    "        print('--------------------------')\n",
    "        print(i,j)\n",
    "        result_lst.append((i,j))\n",
    "        result_lst.append(model.evaluate(x, y, verbose=2))\n",
    "        result_lst.append(model2.evaluate(x, y, verbose=2))\n",
    "        result_lst.append(model3.evaluate(x, y, verbose=2))\n",
    "        print(model.evaluate(x, y, verbose=2))\n",
    "        print(model2.evaluate(x, y, verbose=2))\n",
    "        print(model3.evaluate(x, y, verbose=2))\n",
    "        print(new_y.sample(20))\n",
    "        print('--------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ adam  ------\n",
      "------ train accuracy ------\n",
      "[16, (0.8479260499333018, 0.8404577856999101, 0.8520785569459047), 32, (0.8507504215516729, 0.8452034799973911, 0.8548420166356817), 64, (0.8575394565574471, 0.7332055784405824, 0.8536006252162873)]\n",
      "------ test accuracy ------\n",
      "[(0.8491143181342201, 0.8360599215572839, 0.8493522356329728), (0.8512785770599439, 0.8426021472094265, 0.8525714037360235), (0.8559459276586463, 0.7275011183774313, 0.851562558506652)] \n",
      "\n",
      "------ rmsprop  ------\n",
      "------ train accuracy ------\n",
      "[16, (0.8593355401103872, 0.8394640808331874, 0.8550167083684154), 32, (0.8537047692246935, 0.8330568338300487, 0.7331509293181376), 64, (0.8456741576519586, 0.8585035423445398, 0.7326640624963785)]\n",
      "------ test accuracy ------\n",
      "[(0.8591202899154792, 0.8349046788963603, 0.8534604252043843), (0.8548113302687984, 0.8320152616808477, 0.727441274944091), (0.8459079261660782, 0.8550992808592336, 0.7269359951869134)] \n",
      "\n",
      "------ sgd  ------\n",
      "------ train accuracy ------\n",
      "[16, (0.8413935045610897, 0.8525083678675123, 0.8501682371316917), 32, (0.8325682365646121, 0.8534769431969867, 0.8549837064339139), 64, (0.8403918912676356, 0.6165716974148872, 0.8521315557874481)]\n",
      "------ test accuracy ------\n",
      "[(0.8370794227000364, 0.847589835085288, 0.8478386088429446), (0.8281472610017262, 0.8502156885675968, 0.8512479842486128), (0.8348097115058718, 0.6019021168766095, 0.8478431446907331)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"------ adam  ------\\n------ train accuracy ------\")\n",
    "print(adam_result)\n",
    "print(\"------ test accuracy ------\")\n",
    "print(test_adam,\"\\n\")\n",
    "print(\"------ rmsprop  ------\\n------ train accuracy ------\")\n",
    "print(rmsprop_result)\n",
    "print(\"------ test accuracy ------\")\n",
    "print(test_rmsprop,\"\\n\")\n",
    "print(\"------ sgd  ------\\n------ train accuracy ------\")\n",
    "print(sgd_result)\n",
    "print(\"------ test accuracy ------\")\n",
    "print(test_sgd,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('adam', 16), 4.6697187423706055, 4.568828582763672, 4.291143417358398, ('adam', 32), 5.335988998413086, 4.356584548950195, 4.4404168128967285, ('adam', 64), 7.03688907623291, 4.2998504638671875, 4.4536967277526855, ('rmsprop', 16), 5.0542097091674805, 4.52847146987915, 10.392776489257812, ('rmsprop', 32), 5.901552200317383, 4.3465704917907715, 10.392814636230469, ('rmsprop', 64), 5.714547634124756, 4.445828437805176, 4.570673942565918, ('sgd', 16), 4.427296161651611, 4.267493724822998, 4.422511100769043, ('sgd', 32), 4.627623558044434, 4.547616958618164, 4.510445594787598, ('sgd', 64), 5.1090407371521, 10.196386337280273, 6.51143217086792]\n"
     ]
    }
   ],
   "source": [
    "print(result_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adam 64 3\n",
    "\n",
    "\n",
    "# 기존 모델을 변경해야하는지. (numpy 모델 / 딥러닝모델 / 머신러닝모델)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method_custom_metric 구현\n",
    "def accuracy(y_true, y_pred):\n",
    "    return 1 - tf.abs((y_true - y_pred) / y_true) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "105/105 [==============================] - 1s 3ms/step - loss: 88.9432 - accuracy: 0.1339 - val_loss: 38.3902 - val_accuracy: 0.4849\n",
      "Epoch 2/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 25.6303 - accuracy: 0.5950 - val_loss: 9.6143 - val_accuracy: 0.7918\n",
      "Epoch 3/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 15.6878 - accuracy: 0.6942 - val_loss: 6.9343 - val_accuracy: 0.8492\n",
      "Epoch 4/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 14.2235 - accuracy: 0.7181 - val_loss: 5.9024 - val_accuracy: 0.8517\n",
      "Epoch 5/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 13.6284 - accuracy: 0.7211 - val_loss: 6.2575 - val_accuracy: 0.8507\n",
      "Epoch 6/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 13.1672 - accuracy: 0.7242 - val_loss: 5.6919 - val_accuracy: 0.8526\n",
      "Epoch 7/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 12.5372 - accuracy: 0.7334 - val_loss: 5.5919 - val_accuracy: 0.8496\n",
      "Epoch 8/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 11.9957 - accuracy: 0.7400 - val_loss: 5.5113 - val_accuracy: 0.8521\n",
      "Epoch 9/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 11.3358 - accuracy: 0.7444 - val_loss: 6.0059 - val_accuracy: 0.8515\n",
      "Epoch 10/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 11.0560 - accuracy: 0.7436 - val_loss: 5.6090 - val_accuracy: 0.8520\n",
      "Epoch 11/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 10.8849 - accuracy: 0.7549 - val_loss: 5.0708 - val_accuracy: 0.8497\n",
      "Epoch 12/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 10.4401 - accuracy: 0.7553 - val_loss: 5.9475 - val_accuracy: 0.8518\n",
      "Epoch 13/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 10.4557 - accuracy: 0.7574 - val_loss: 5.1593 - val_accuracy: 0.8526\n",
      "Epoch 14/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 10.1117 - accuracy: 0.7638 - val_loss: 5.3506 - val_accuracy: 0.8540\n",
      "Epoch 15/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 9.7642 - accuracy: 0.7624 - val_loss: 5.2880 - val_accuracy: 0.8540\n",
      "Epoch 16/1000\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 9.9990 - accuracy: 0.7640 - val_loss: 5.4766 - val_accuracy: 0.8548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18c9c1a8ac0>"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 좋은 모델 구현 /  batch : 32, optimizer : adam \n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=x.shape[1]))\n",
    "model.add(Dropout(0.1))  # Dropout 추가\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.1))  # Dropout 추가\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.1))  # Dropout 추가\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dropout(0.1))  # Dropout 추가\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# Rings가 정수이기 때문에 linear로 마무리\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam', metrics=[accuracy])\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Fit the model with early stopping\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 0s 838us/step\n",
      "131/131 - 0s - loss: 5.2345 - 106ms/epoch - 809us/step\n",
      "5.234450340270996\n",
      "      Rings       pred\n",
      "3515     12  10.380693\n",
      "3757     10   8.554095\n",
      "4020     11   9.316498\n",
      "1760     11  11.940524\n",
      "1484      9   8.073967\n",
      "3028      7   8.482928\n",
      "3940     11   8.706378\n",
      "1449      6   8.035497\n",
      "3008     12  14.365173\n",
      "820       7   8.147294\n",
      "2455      6   4.836672\n",
      "3233     12  10.798211\n",
      "3330     10   8.705069\n",
      "1612      9   9.652300\n",
      "3944     20  13.350824\n",
      "270      22  16.558657\n",
      "2736      7   7.247585\n",
      "401       9  10.463152\n",
      "2478      8  12.098919\n",
      "2639      6   7.510412\n",
      "2940     10   8.620636\n",
      "831       6   7.059111\n",
      "1922      8   9.338520\n",
      "2687     10   9.615753\n",
      "1076      7   7.082126\n",
      "3208      9   5.859188\n",
      "3424     10  10.922712\n",
      "1403     10   9.311589\n",
      "2422     12   9.778882\n",
      "1676     10   9.264255\n",
      "2084     12  10.044170\n",
      "365      19  11.387296\n",
      "3781     10   8.994729\n",
      "2416     18  10.051412\n",
      "2185      9   8.653342\n",
      "1527     12  10.097306\n",
      "3844     14  12.905582\n",
      "851       9   8.567462\n",
      "1010     11  10.139963\n",
      "2716      6   5.755735\n",
      "2114      4   4.407065\n",
      "3261     12  10.112998\n",
      "3026      8   7.731662\n",
      "801      10   9.357557\n",
      "3973      7   6.749156\n",
      "2125      8   9.365726\n",
      "3564      9   7.906901\n",
      "3911     10   6.741200\n",
      "615      13  10.628775\n",
      "1310      8   8.652893\n",
      "815       7   5.792955\n",
      "540      15  10.119689\n",
      "4164      7   6.614946\n",
      "43        5   4.604518\n",
      "2237     17  13.907301\n",
      "802       9  10.161263\n",
      "223      11  10.882957\n",
      "1389     10  10.261092\n",
      "3464     10   9.086419\n",
      "620      10   7.169953\n",
      "4029      7   7.280515\n",
      "304      10   9.922338\n",
      "435      11  10.601238\n",
      "1386     12  10.318904\n",
      "1691     12   9.216003\n",
      "1861      8   7.930148\n",
      "3472      3   4.538245\n",
      "1107      8   8.257118\n",
      "81       12  18.120285\n",
      "637       6   7.137810\n",
      "395      11   7.156042\n",
      "3819      8   8.879051\n",
      "4142     13   9.804681\n",
      "1201      8   8.356256\n",
      "2909      9   9.966181\n",
      "2798     10   9.541854\n",
      "1196     11  10.112451\n",
      "2730      6   6.410885\n",
      "1445      6   6.810294\n",
      "2672      8   8.315762\n",
      "3338     16  14.078003\n",
      "1075      6   6.996830\n",
      "2299      8   8.152348\n",
      "4073      9   8.799120\n",
      "3135      9  10.119778\n",
      "3934      5   5.220088\n",
      "29       11   8.747316\n",
      "1233      6   6.221078\n",
      "2302      9   9.803009\n",
      "2217     13  10.664829\n",
      "4126     11  10.616279\n",
      "3656     11   8.325033\n",
      "1684      9  10.044536\n",
      "2856     11   8.293018\n",
      "3280     24  14.618754\n",
      "1237      6   6.449961\n",
      "1450      7   7.369225\n",
      "2618     13  10.787153\n",
      "4141     11  10.507271\n",
      "908       7   5.700831\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x)\n",
    "new_y = y\n",
    "stacked_array = np.vstack((y_pred))\n",
    "new_df = pd.DataFrame(stacked_array)\n",
    "new_y = pd.DataFrame(new_y)\n",
    "new_y['pred'] = new_df[0] \n",
    "\n",
    "print(model.evaluate(x, y, verbose=2))\n",
    "\n",
    "print(new_y.sample(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[안내] 모델이 실행됩니다.\n",
      "Epoch 1/1000\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 93.9857 - accuracy: 0.1001 - val_loss: 48.1326 - val_accuracy: 0.4178\n",
      "Epoch 2/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 34.6026 - accuracy: 0.5260 - val_loss: 13.7941 - val_accuracy: 0.7433\n",
      "Epoch 3/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 19.8636 - accuracy: 0.6655 - val_loss: 8.0992 - val_accuracy: 0.8384\n",
      "Epoch 4/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 16.8584 - accuracy: 0.6967 - val_loss: 6.5740 - val_accuracy: 0.8490\n",
      "Epoch 5/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 15.3717 - accuracy: 0.7078 - val_loss: 7.2797 - val_accuracy: 0.8441\n",
      "Epoch 6/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 14.6586 - accuracy: 0.7216 - val_loss: 6.5676 - val_accuracy: 0.8498\n",
      "Epoch 7/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 13.0768 - accuracy: 0.7341 - val_loss: 6.8427 - val_accuracy: 0.8488\n",
      "Epoch 8/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 13.2930 - accuracy: 0.7319 - val_loss: 7.1045 - val_accuracy: 0.8435\n",
      "Epoch 9/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 12.0106 - accuracy: 0.7486 - val_loss: 6.1561 - val_accuracy: 0.8543\n",
      "Epoch 10/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 12.0461 - accuracy: 0.7492 - val_loss: 6.8490 - val_accuracy: 0.8468\n",
      "Epoch 11/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 12.2071 - accuracy: 0.7546 - val_loss: 6.1982 - val_accuracy: 0.8520\n",
      "Epoch 12/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.5292 - accuracy: 0.7657 - val_loss: 6.8838 - val_accuracy: 0.8468\n",
      "Epoch 13/1000\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 11.0251 - accuracy: 0.7634 - val_loss: 6.5717 - val_accuracy: 0.8484\n",
      "Epoch 14/1000\n",
      "209/209 [==============================] - 0s 1ms/step - loss: 10.9204 - accuracy: 0.7639 - val_loss: 6.4609 - val_accuracy: 0.8519\n",
      "131/131 [==============================] - 0s 738us/step\n",
      "[안내] 최종 모델\n",
      "131/131 - 0s - loss: 6.2317 - accuracy: 0.8507 - 119ms/epoch - 909us/step\n",
      "[안내] 실행 시간 : 5.448 seconds\n",
      "[안내] 샘플 10개의 결과\n",
      "      Rings       pred\n",
      "3859      9   9.827324\n",
      "2670      9   8.393818\n",
      "3256     12   9.821785\n",
      "3623     10   9.598242\n",
      "3225      9   7.722341\n",
      "2844     10   8.626442\n",
      "2248      6   7.301757\n",
      "2477     13  11.029272\n",
      "585      11   8.162371\n",
      "2906      8  10.114727\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 좋은 모델 구현 / Batch : 16 , optimizer : rmsprop\n",
    "start_time = time.time()\n",
    "print(\"[안내] 모델이 실행됩니다.\")\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=x.shape[1]))\n",
    "model.add(Dropout(0.1))  # Dropout 추가\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(0.1))  # Dropout 추가\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(0.1))  # Dropout 추가\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dropout(0.1))  # Dropout 추가\n",
    "model.add(Dense(1, activation='linear'))\n",
    "# Rings가 정수이기 때문에 linear로 마무리\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='mse', optimizer='rmsprop', metrics=[accuracy])\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Fit the model with early stopping\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=16, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "y_pred = model.predict(x)\n",
    "print(\"[안내] 최종 모델\")\n",
    "loss, accuracy = model.evaluate(x, y, verbose=2)\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(\"[안내] 실행 시간 : {:.3f} seconds\".format(execution_time))\n",
    "\n",
    "x = input(\"예측 샘플 확인 : (y or n)\")\n",
    "if x == 'y':\n",
    "    print(\"[안내] 샘플 10개의 결과\")\n",
    "    new_y = y\n",
    "    stacked_array = np.vstack((y_pred))\n",
    "    new_df = pd.DataFrame(stacked_array)\n",
    "    new_y = pd.DataFrame(new_y)\n",
    "    new_y['pred'] = new_df[0]\n",
    "    print(new_y.sample(10))\n",
    "else:\n",
    "    print(\"[안내] 실행을 종료합니다.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131/131 [==============================] - 0s 815us/step\n",
      "131/131 - 0s - loss: 4.5929 - absolute_percentage_error: 0.8564 - 109ms/epoch - 832us/step\n",
      "      Rings       pred\n",
      "406       8   8.975977\n",
      "3024      9   8.400672\n",
      "3120      9  10.050747\n",
      "870      12  12.022715\n",
      "940       7   7.391232\n",
      "3758      9   9.934937\n",
      "707      10   7.437623\n",
      "1108      8   7.587103\n",
      "2359     13  12.987674\n",
      "2829      7   8.299488\n",
      "704       9   8.428637\n",
      "1384      9   9.856888\n",
      "1924      8   9.772204\n",
      "3162     15  11.957195\n",
      "2603      9   9.973102\n",
      "1966     10  11.361793\n",
      "3341     12  10.974815\n",
      "1798     11   9.758018\n",
      "1461      6   9.032294\n",
      "3210     12  10.490632\n",
      "2311      7   7.391108\n",
      "1255      7   7.336595\n",
      "3170     14  11.554789\n",
      "3036     11  10.816627\n",
      "3784     11   9.913762\n",
      "2146     11  10.175589\n",
      "2705     13  11.094278\n",
      "2361     12  10.668835\n",
      "1319      9   9.711514\n",
      "1318      9   9.634491\n",
      "1013     10  11.222443\n",
      "1529     10  10.065878\n",
      "2986     13  10.657429\n",
      "2199     21  15.633911\n",
      "2345     17  13.856924\n",
      "2443     11  10.345102\n",
      "1985     13  14.928577\n",
      "729      15   9.658818\n",
      "2935     10  11.074046\n",
      "809      12  10.813038\n",
      "80        9  10.494384\n",
      "4130     10  10.479259\n",
      "607      10   8.788544\n",
      "3992     10  13.974656\n",
      "655       7   6.441743\n",
      "40        9   8.601140\n",
      "16        7   6.552785\n",
      "3936     13  11.419101\n",
      "3617      9   8.599871\n",
      "2033     10  11.039689\n",
      "4162      8   7.672261\n",
      "116      11  11.028392\n",
      "427      19  11.935294\n",
      "3879     15   9.332044\n",
      "1971     11  10.975008\n",
      "2417     11   6.621875\n",
      "3027      8  10.034325\n",
      "4028      8   7.198888\n",
      "2086     10   9.062325\n",
      "3949     13  12.785294\n",
      "3515     12  10.624971\n",
      "1492     11   9.808113\n",
      "3793     14  11.363334\n",
      "1750     12  10.465566\n",
      "2400     13   8.325828\n",
      "432      18  13.056939\n",
      "3228      9   7.986627\n",
      "1583      7   8.310489\n",
      "3959     11  10.886706\n",
      "1763     12  12.273696\n",
      "388      14  10.949487\n",
      "84       14  12.440004\n",
      "3403      9   7.089034\n",
      "3364     11   7.191887\n",
      "2438     16  13.444811\n",
      "3554      8   9.004084\n",
      "4025      7   6.131871\n",
      "2870      7   6.656870\n",
      "1308      9   9.689354\n",
      "4094     13   9.899497\n",
      "2490      7   8.738259\n",
      "3695     10  10.511957\n",
      "3461     11  11.112631\n",
      "2808      9  10.476227\n",
      "3717      7   6.234248\n",
      "1765      6   6.487714\n",
      "2551      6   5.721099\n",
      "384      10   9.510427\n",
      "1903     10  10.414969\n",
      "1826      7   5.844929\n",
      "2165      9  11.081565\n",
      "3775      8  10.851984\n",
      "2164      5   7.809160\n",
      "3688     11  10.751084\n",
      "181      21  15.850532\n",
      "822       7   6.832922\n",
      "480      29  11.764102\n",
      "3115      6   6.930207\n",
      "458      10   7.442220\n",
      "1872      9   9.001380\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x)\n",
    "new_y = y\n",
    "stacked_array = np.vstack((y_pred))\n",
    "new_df = pd.DataFrame(stacked_array)\n",
    "new_y = pd.DataFrame(new_y)\n",
    "new_y['pred'] = new_df[0] \n",
    "\n",
    "loss, accuracy = model.evaluate(x, y, verbose=2)\n",
    "\n",
    "print(new_y.sample(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
