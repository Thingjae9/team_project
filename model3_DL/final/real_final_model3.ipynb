{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import keras.utils\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, recall_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 불러오기\n",
    "\n",
    "- EDA 데이터 모델 적용 결과 feature importance를 통해 칼럼을 제거한 데이터를 사용하면 모델의 복잡도 감소 + val_accuracy는 유지 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 새로운 데이터 (EDA)과정 진행 후 데이터\n",
    "X_train = pd.read_csv('csv/Multi_to_share/Multi_to_share/fi로 drop 진행/X_fi_train.csv')\n",
    "X_test = pd.read_csv('csv/Multi_to_share/Multi_to_share/fi로 drop 진행/X_fi_test.csv')\n",
    "X_val = pd.read_csv('csv/Multi_to_share/Multi_to_share/fi로 drop 진행/X_fi_val.csv')\n",
    "y_train = pd.read_csv('csv/Multi_to_share/Multi_to_share/fi로 drop 진행/y_fi_train.csv')\n",
    "y_test = pd.read_csv('csv/Multi_to_share/Multi_to_share/fi로 drop 진행/y_fi_test.csv')\n",
    "y_val = pd.read_csv('csv/Multi_to_share/Multi_to_share/fi로 drop 진행/y_fi_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 표준화\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train['class']\n",
    "y_test = y_test['class']\n",
    "y_val = y_val['class']\n",
    "\n",
    "# 타겟 맵핑\t\t\t\t\n",
    "label_map = {'Pastry': 0, 'Z_Scratch': 1, 'K_Scatch': 2, 'Stains': 3, 'Dirtiness': 4, 'Bumps': 5, 'Other_Faults': 6}\n",
    "y_train = [label_map[label] for label in y_train]\n",
    "y_test = [label_map[label] for label in y_test]\n",
    "y_val = [label_map[label] for label in y_val]\n",
    "\n",
    "# y_train을 one-hot encoding으로 변환합니다\n",
    "num_classes = len(set(y_train))\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 구현\n",
    "- 큰 노드부터 감소시키는 은닉층을 갖는 구조 + 출력층 활성화함수 'softmax'고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 - 0s - loss: 0.8046 - accuracy: 0.7203 - 45ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8345 - accuracy: 0.7363 - 47ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.9288 - accuracy: 0.7428 - 55ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 1.0047 - accuracy: 0.7524 - 53ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 1.1451 - accuracy: 0.7235 - 53ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7729 - accuracy: 0.7492 - 43ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.8225 - accuracy: 0.7331 - 46ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8564 - accuracy: 0.7460 - 58ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.8371 - accuracy: 0.7395 - 50ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 1.0017 - accuracy: 0.7267 - 75ms/epoch - 8ms/step\n",
      "10/10 - 0s - loss: 0.7727 - accuracy: 0.7556 - 43ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.7849 - accuracy: 0.7588 - 55ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.7824 - accuracy: 0.7717 - 46ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8511 - accuracy: 0.7556 - 51ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 1.0224 - accuracy: 0.6399 - 44ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.8163 - accuracy: 0.7331 - 62ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.8280 - accuracy: 0.7717 - 51ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8380 - accuracy: 0.7363 - 40ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.7974 - accuracy: 0.7299 - 51ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 1.1498 - accuracy: 0.5884 - 45ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7869 - accuracy: 0.7363 - 47ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8302 - accuracy: 0.7170 - 45ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.7759 - accuracy: 0.7814 - 55ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.8848 - accuracy: 0.7395 - 51ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8379 - accuracy: 0.7395 - 34ms/epoch - 3ms/step\n",
      "10/10 - 0s - loss: 1.1692 - accuracy: 0.5595 - 71ms/epoch - 7ms/step\n",
      "10/10 - 0s - loss: 1.0503 - accuracy: 0.6077 - 50ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8249 - accuracy: 0.6720 - 55ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.8730 - accuracy: 0.6720 - 52ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7977 - accuracy: 0.7074 - 55ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 1.0002 - accuracy: 0.5916 - 59ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.8242 - accuracy: 0.7074 - 56ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.8003 - accuracy: 0.7203 - 35ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.7787 - accuracy: 0.7299 - 46ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8132 - accuracy: 0.7395 - 51ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8501 - accuracy: 0.6752 - 50ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7143 - accuracy: 0.7588 - 39ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.7514 - accuracy: 0.7299 - 51ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7746 - accuracy: 0.7524 - 49ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7510 - accuracy: 0.7588 - 59ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.7039 - accuracy: 0.7556 - 50ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7261 - accuracy: 0.7749 - 50ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7233 - accuracy: 0.7621 - 43ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.7964 - accuracy: 0.7363 - 49ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7918 - accuracy: 0.7428 - 36ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.6702 - accuracy: 0.7814 - 48ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7428 - accuracy: 0.7395 - 45ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7466 - accuracy: 0.7524 - 53ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7921 - accuracy: 0.7556 - 51ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8012 - accuracy: 0.7621 - 45ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.9908 - accuracy: 0.6302 - 49ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 1.0599 - accuracy: 0.5209 - 51ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.9536 - accuracy: 0.6592 - 46ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.9808 - accuracy: 0.6527 - 51ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 2.1986 - accuracy: 0.3473 - 46ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 1.0072 - accuracy: 0.6077 - 35ms/epoch - 3ms/step\n",
      "10/10 - 0s - loss: 0.7780 - accuracy: 0.7460 - 80ms/epoch - 8ms/step\n",
      "10/10 - 0s - loss: 0.7344 - accuracy: 0.7331 - 53ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7523 - accuracy: 0.7428 - 63ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 1.8476 - accuracy: 0.3473 - 52ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8265 - accuracy: 0.7042 - 58ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.7476 - accuracy: 0.7428 - 63ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.7049 - accuracy: 0.7621 - 64ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.9104 - accuracy: 0.7106 - 56ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 1.6577 - accuracy: 0.3473 - 49ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8114 - accuracy: 0.6945 - 41ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.8905 - accuracy: 0.6881 - 47ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.9101 - accuracy: 0.6785 - 44ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.9303 - accuracy: 0.6881 - 61ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 1.6830 - accuracy: 0.3473 - 50ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.9811 - accuracy: 0.6559 - 64ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.8446 - accuracy: 0.6752 - 55ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.9686 - accuracy: 0.6559 - 45ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 1.7166 - accuracy: 0.3473 - 50ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 1.9618 - accuracy: 0.2026 - 51ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7869 - accuracy: 0.7556 - 43ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.9263 - accuracy: 0.7395 - 72ms/epoch - 7ms/step\n",
      "10/10 - 0s - loss: 0.9063 - accuracy: 0.7395 - 63ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.8404 - accuracy: 0.7653 - 52ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.9991 - accuracy: 0.7556 - 55ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.7578 - accuracy: 0.7428 - 73ms/epoch - 7ms/step\n",
      "10/10 - 0s - loss: 0.7719 - accuracy: 0.7460 - 53ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7643 - accuracy: 0.7588 - 47ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8554 - accuracy: 0.7653 - 53ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8589 - accuracy: 0.7492 - 47ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7647 - accuracy: 0.7556 - 66ms/epoch - 7ms/step\n",
      "10/10 - 0s - loss: 0.8406 - accuracy: 0.7106 - 47ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8115 - accuracy: 0.7653 - 57ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.8075 - accuracy: 0.7331 - 54ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.8930 - accuracy: 0.7331 - 53ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7267 - accuracy: 0.7267 - 46ms/epoch - 5ms/step\n",
      "10/10 - 0s - loss: 0.7293 - accuracy: 0.7363 - 44ms/epoch - 4ms/step\n",
      "10/10 - 0s - loss: 0.7163 - accuracy: 0.7556 - 60ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.7829 - accuracy: 0.7492 - 66ms/epoch - 7ms/step\n",
      "10/10 - 0s - loss: 0.8578 - accuracy: 0.6849 - 82ms/epoch - 8ms/step\n",
      "10/10 - 0s - loss: 0.7154 - accuracy: 0.7460 - 82ms/epoch - 8ms/step\n",
      "10/10 - 0s - loss: 0.7260 - accuracy: 0.7460 - 66ms/epoch - 7ms/step\n",
      "10/10 - 0s - loss: 0.7338 - accuracy: 0.7685 - 64ms/epoch - 6ms/step\n",
      "10/10 - 0s - loss: 0.7035 - accuracy: 0.7781 - 79ms/epoch - 8ms/step\n",
      "10/10 - 0s - loss: 0.7074 - accuracy: 0.7363 - 62ms/epoch - 6ms/step\n",
      "Best hyperparameters: {'activation': 'relu', 'learning_rate': 0.0005, 'batch': 64}\n",
      "Best validation accuracy: 0.7813504934310913\n",
      "Best time: 6.03266978263855\n",
      "Best time hyperparameters: {'activation': 'sigmoid', 'learning_rate': 0.009, 'batch': 256}\n"
     ]
    }
   ],
   "source": [
    "# 복습한 내용으로 추가적인 진행 - 다이아몬드형\n",
    "input_dim = X_train.shape[1]\n",
    "output_dim = y_train.shape[1]\n",
    "\n",
    "def custom_opt(n):\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=n)\n",
    "    return opt\n",
    "\n",
    "# dropout, 배치 정규화 추가\n",
    "# 변수 리스트 생성\n",
    "act_func = ['relu', 'tanh', 'sigmoid', 'elu']\n",
    "best_accuracy = 0.0\n",
    "best_hyperparams = {}\n",
    "lr_lst = [0.009, 0.006, 0.003, 0.001, 0.0005]\n",
    "best_time = 11111.0\n",
    "time_hyper = {}\n",
    "batch_lst = [16, 32, 64, 128, 256]\n",
    "\n",
    "# 모델 구현\n",
    "for func in act_func:\n",
    "    for i in lr_lst:\n",
    "        for batch in batch_lst:\n",
    "            model = Sequential()\n",
    "            model.add(Dense(512, activation=func, input_dim=input_dim))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가\n",
    "            model.add(Dense(256, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가              \n",
    "            model.add(Dense(128, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가   \n",
    "            model.add(Dense(96, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가\n",
    "            model.add(Dense(64, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가\n",
    "            model.add(Dense(32, activation=func))\n",
    "            model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "            model.add(Dropout(0.2))  # Dropout 추가\n",
    "            model.add(Dense(16, activation=func))\n",
    "            model.add(Dense(12, activation=func)) \n",
    "            model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "            # 모델 컴파일\n",
    "            model.compile(loss='categorical_crossentropy', optimizer=custom_opt(i), metrics=['accuracy'])\n",
    "\n",
    "            # Early stopping 기능 추가\n",
    "            early_stopping = EarlyStopping(patience=10, monitor='val_accuracy')\n",
    "            start_time = time.time()\n",
    "\n",
    "            # 모델 학습\n",
    "            model.fit(X_train, y_train, epochs=1000, batch_size=batch, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose = 0)\n",
    "            end_time = time.time()\n",
    "            long_time = end_time - start_time\n",
    "            if long_time < best_time:\n",
    "                best_time = long_time\n",
    "                time_hyper = {'activation': func, 'learning_rate': i, 'batch': batch}\n",
    "\n",
    "\n",
    "            \n",
    "            loss, acc = model.evaluate(X_val, y_val, verbose = 2)\n",
    "\n",
    "            if acc > best_accuracy:\n",
    "                best_accuracy = acc\n",
    "                best_hyperparams = {'activation': func, 'learning_rate': i, 'batch': batch}\n",
    "\n",
    "print('Best hyperparameters:', best_hyperparams)\n",
    "print('Best validation accuracy:', best_accuracy)\n",
    "print('Best time:',best_time)\n",
    "print('Best time hyperparameters:',time_hyper)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최종 모델\n",
    "- Best hyperparameters: {'activation': 'relu', 'learning_rate': 0.0005, 'batch': 64}\n",
    "- Best validation accuracy: 0.7813504934310913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "20/20 [==============================] - 4s 20ms/step - loss: 2.3253 - accuracy: 0.1942 - val_loss: 1.9021 - val_accuracy: 0.2862\n",
      "Epoch 2/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.9570 - accuracy: 0.2812 - val_loss: 1.8496 - val_accuracy: 0.3537\n",
      "Epoch 3/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.7700 - accuracy: 0.3175 - val_loss: 1.8024 - val_accuracy: 0.4180\n",
      "Epoch 4/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.6759 - accuracy: 0.3650 - val_loss: 1.7440 - val_accuracy: 0.4920\n",
      "Epoch 5/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.5679 - accuracy: 0.3997 - val_loss: 1.6706 - val_accuracy: 0.5016\n",
      "Epoch 6/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.4558 - accuracy: 0.4553 - val_loss: 1.5832 - val_accuracy: 0.5305\n",
      "Epoch 7/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.4047 - accuracy: 0.4867 - val_loss: 1.4861 - val_accuracy: 0.5498\n",
      "Epoch 8/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.3415 - accuracy: 0.4956 - val_loss: 1.4054 - val_accuracy: 0.5659\n",
      "Epoch 9/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.2755 - accuracy: 0.5254 - val_loss: 1.3211 - val_accuracy: 0.5981\n",
      "Epoch 10/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.2204 - accuracy: 0.5528 - val_loss: 1.2248 - val_accuracy: 0.6302\n",
      "Epoch 11/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.2180 - accuracy: 0.5673 - val_loss: 1.1491 - val_accuracy: 0.6270\n",
      "Epoch 12/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.1428 - accuracy: 0.5915 - val_loss: 1.0745 - val_accuracy: 0.6624\n",
      "Epoch 13/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.1107 - accuracy: 0.5939 - val_loss: 1.0128 - val_accuracy: 0.6817\n",
      "Epoch 14/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.0489 - accuracy: 0.6350 - val_loss: 0.9677 - val_accuracy: 0.6913\n",
      "Epoch 15/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.0386 - accuracy: 0.6406 - val_loss: 0.9324 - val_accuracy: 0.6913\n",
      "Epoch 16/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 1.0268 - accuracy: 0.6342 - val_loss: 0.8831 - val_accuracy: 0.6977\n",
      "Epoch 17/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.9650 - accuracy: 0.6503 - val_loss: 0.8482 - val_accuracy: 0.7010\n",
      "Epoch 18/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.9543 - accuracy: 0.6591 - val_loss: 0.8236 - val_accuracy: 0.7138\n",
      "Epoch 19/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.9416 - accuracy: 0.6479 - val_loss: 0.8100 - val_accuracy: 0.7235\n",
      "Epoch 20/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.9209 - accuracy: 0.6688 - val_loss: 0.7944 - val_accuracy: 0.7299\n",
      "Epoch 21/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.8899 - accuracy: 0.6890 - val_loss: 0.7991 - val_accuracy: 0.7267\n",
      "Epoch 22/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.8547 - accuracy: 0.6898 - val_loss: 0.7757 - val_accuracy: 0.7395\n",
      "Epoch 23/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.8717 - accuracy: 0.6809 - val_loss: 0.7679 - val_accuracy: 0.7395\n",
      "Epoch 24/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.8157 - accuracy: 0.6946 - val_loss: 0.7526 - val_accuracy: 0.7363\n",
      "Epoch 25/1000\n",
      "20/20 [==============================] - 0s 12ms/step - loss: 0.8198 - accuracy: 0.6890 - val_loss: 0.7416 - val_accuracy: 0.7524\n",
      "Epoch 26/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.7994 - accuracy: 0.7220 - val_loss: 0.7545 - val_accuracy: 0.7588\n",
      "Epoch 27/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.7770 - accuracy: 0.7220 - val_loss: 0.7549 - val_accuracy: 0.7492\n",
      "Epoch 28/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.7877 - accuracy: 0.7147 - val_loss: 0.7413 - val_accuracy: 0.7460\n",
      "Epoch 29/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.7773 - accuracy: 0.7156 - val_loss: 0.7402 - val_accuracy: 0.7299\n",
      "Epoch 30/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.7721 - accuracy: 0.7260 - val_loss: 0.7477 - val_accuracy: 0.7395\n",
      "Epoch 31/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.7436 - accuracy: 0.7381 - val_loss: 0.7367 - val_accuracy: 0.7363\n",
      "Epoch 32/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.7243 - accuracy: 0.7325 - val_loss: 0.7371 - val_accuracy: 0.7395\n",
      "Epoch 33/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.7387 - accuracy: 0.7446 - val_loss: 0.7373 - val_accuracy: 0.7524\n",
      "Epoch 34/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.7103 - accuracy: 0.7534 - val_loss: 0.7244 - val_accuracy: 0.7428\n",
      "Epoch 35/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.6898 - accuracy: 0.7478 - val_loss: 0.7329 - val_accuracy: 0.7492\n",
      "Epoch 36/1000\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.7235 - accuracy: 0.7454 - val_loss: 0.7281 - val_accuracy: 0.7556\n",
      "==== Final Model ====\n",
      "걸린시간 : 9.290071964263916\n",
      "==== train los acc ====\n",
      "39/39 - 0s - loss: 0.4406 - accuracy: 0.8259 - 75ms/epoch - 2ms/step\n",
      "==== val los acc ====\n",
      "10/10 - 0s - loss: 0.7281 - accuracy: 0.7556 - 32ms/epoch - 3ms/step\n"
     ]
    }
   ],
   "source": [
    "func = 'relu'\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation=func, input_dim=input_dim))\n",
    "model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "model.add(Dropout(0.2))  # Dropout 추가\n",
    "model.add(Dense(256, activation=func))\n",
    "model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "model.add(Dropout(0.2))  # Dropout 추가              \n",
    "model.add(Dense(128, activation=func))\n",
    "model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "model.add(Dropout(0.2))  # Dropout 추가   \n",
    "model.add(Dense(96, activation=func))\n",
    "model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "model.add(Dropout(0.2))  # Dropout 추가\n",
    "model.add(Dense(64, activation=func))\n",
    "model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "model.add(Dropout(0.2))  # Dropout 추가\n",
    "model.add(Dense(28, activation=func))\n",
    "model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "model.add(Dropout(0.2))  # Dropout 추가\n",
    "model.add(Dense(28, activation=func))\n",
    "model.add(BatchNormalization()) # 배치 정규화 추가\n",
    "model.add(Dropout(0.2))  # Dropout 추가\n",
    "model.add(Dense(output_dim, activation='softmax'))\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='categorical_crossentropy', optimizer=custom_opt(0.0005), metrics=['accuracy'])\n",
    "\n",
    "# Early stopping 기능 추가\n",
    "early_stopping = EarlyStopping(patience=10, monitor='val_accuracy')\n",
    "start_time = time.time()\n",
    "\n",
    "# 모델 학습\n",
    "model.fit(X_train, y_train, epochs=1000, batch_size=64, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
    "end_time = time.time()\n",
    "long_time = end_time - start_time\n",
    "\n",
    "print(\"==== Final Model ====\")\n",
    "print(\"걸린시간 :\", long_time)\n",
    "print(\"==== train los acc ====\")\n",
    "train_loss, train_acc = model.evaluate(X_train, y_train, verbose = 2)\n",
    "print(\"==== val los acc ====\")\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
